#!/usr/bin/env python3
"""
üöÄ –°–ö–†–ò–ü–¢ –î–õ–Ø –ú–ê–°–°–û–í–û–ì–û –°–ö–†–ê–ü–ò–†–û–í–ê–ù–ò–Ø –í RAILWAY –ë–î

–°–æ—Å–∫—Ä–∞–ø—ã–≤–∞–µ—Ç 20 —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞:
- Casa.it
- Subito.it  
- Idealista.it
- Immobiliare.it

–ò —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –Ω–∞ Railway

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/bulk_scrape_railway.py --pages 20

–ò–ª–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –ë–î:
    python scripts/bulk_scrape_railway.py --pages 20 --local
"""
import sys
import asyncio
import logging
import argparse
import os
import time
from pathlib import Path
from datetime import datetime, timedelta

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
from dotenv import load_dotenv
load_dotenv()

from src.parsers import CasaScraper, SubitoScraper, IdealistaScraper, ImmobiliareScraper
from src.services.scraping_service import ScrapingService
from src.db.database import SessionLocal
from src.core.config import settings

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("bulk_scrape.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class BulkScraper:
    """–ö–ª–∞—Å—Å –¥–ª—è –º–∞—Å—Å–æ–≤–æ–≥–æ —Å–∫—Ä–∞–ø–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self, max_pages: int = 20):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä–∞–ø–µ—Ä–∞
        
        Args:
            max_pages: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Å–∫—Ä–∞–ø–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        """
        self.max_pages = max_pages
        self.scraping_service = ScrapingService()
        
        logger.info(f"üìä BulkScraper –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Å–∫—Ä–∞–ø–∏—Ä–æ–≤–∞–Ω–∏—è {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
        logger.info(f"üîå –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ë–î: {settings.DATABASE_URL[:50]}...")
    
    async def scrape_all_sources(self) -> dict:
        """
        –°–∫—Ä–∞–ø–∏—Ç –≤—Å–µ 4 –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        
        Returns:
            dict: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        """
        logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥ –≤—Å–µ—Ö 4 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        
        results = {
            "casa_it": [],
            "subito": [],
            "idealista": [],
            "immobiliare": []
        }
        
        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            raw_results = await asyncio.gather(
                self.scraping_service.scrape_casa_async({}, self.max_pages),
                self.scraping_service.scrape_subito_async({}, self.max_pages),
                self.scraping_service.scrape_idealista_async({}, self.max_pages),
                self.scraping_service.scrape_immobiliare_async({}, self.max_pages),
                return_exceptions=True
            )
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            source_names = ["casa_it", "subito", "idealista", "immobiliare"]
            
            for i, result in enumerate(raw_results):
                source_name = source_names[i]
                
                if isinstance(result, Exception):
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source_name}: {result}")
                    results[source_name] = []
                elif isinstance(result, list):
                    results[source_name] = result
                    logger.info(f"‚úÖ {source_name}: –ø–æ–ª—É—á–µ–Ω–æ {len(result)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º —Å–∫—Ä–∞–ø–∏–Ω–≥–µ: {e}")
            return results
    
    def save_to_database(self, results: dict) -> dict:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            results: –°–ª–æ–≤–∞—Ä—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            
        Returns:
            dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        db = SessionLocal()
        try:
            logger.info("üíæ –ù–∞—á–∏–Ω–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            all_listings = []
            for source, listings in results.items():
                # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ source —É–∫–∞–∑–∞–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ
                for listing in listings:
                    if 'source' not in listing or not listing.get('source'):
                        listing['source'] = source
                all_listings.extend(listings)
            
            if not all_listings:
                logger.warning("‚ö†Ô∏è –ù–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
                return {
                    "total_listings": 0,
                    "created": 0,
                    "updated": 0,
                    "errors": 0,
                    "skipped_duplicates": 0
                }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            stats = self.scraping_service.save_listings_to_db(all_listings, db)
            
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            logger.info(f"   üìä –ù–æ–≤—ã—Ö: {stats.get('created', 0)}")
            logger.info(f"   üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {stats.get('updated', 0)}")
            logger.info(f"   ‚è≠Ô∏è –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø—Ä–æ–ø—É—â–µ–Ω–æ: {stats.get('skipped_duplicates', 0)}")
            logger.info(f"   ‚ùå –û—à–∏–±–æ–∫: {stats.get('errors', 0)}")
            
            return stats
            
        finally:
            db.close()
    
    async def run(self) -> dict:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥: —Å–∫—Ä–∞–ø–∏—Ä–æ–≤–∞–Ω–∏–µ + —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        
        Returns:
            dict: –ü–æ–ª–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏
        """
        start_time = datetime.now()
        
        try:
            logger.info("=" * 80)
            logger.info(f"üéØ –ú–ê–°–°–û–í–û–ï –°–ö–†–ê–ü–ò–†–û–í–ê–ù–ò–ï –ù–ê–ß–ê–¢–û")
            logger.info(f"üìÖ –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {start_time.strftime('%d.%m.%Y %H:%M:%S')}")
            logger.info(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫: {self.max_pages}")
            logger.info(f"üåê –ò—Å—Ç–æ—á–Ω–∏–∫–∏: Casa.it, Subito.it, Idealista.it, Immobiliare.it")
            logger.info("=" * 80)
            
            # –®–∞–≥ 1: –°–∫—Ä–∞–ø–∏—Ä–æ–≤–∞–Ω–∏–µ
            logger.info("\nüìç –®–ê–ì 1: –°–∫—Ä–∞–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
            scrape_results = await self.scrape_all_sources()
            
            total_scraped = sum(len(listings) for listings in scrape_results.values())
            logger.info(f"\n‚úÖ –°–∫—Ä–∞–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {total_scraped} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤—Å–µ–≥–æ")
            
            # –®–∞–≥ 2: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            logger.info("\nüìç –®–ê–ì 2: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
            save_stats = self.save_to_database(scrape_results)
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            end_time = datetime.now()
            elapsed = (end_time - start_time).total_seconds()
            
            final_stats = {
                "success": True,
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "elapsed_seconds": elapsed,
                "elapsed_formatted": f"{int(elapsed // 60)} –º–∏–Ω {int(elapsed % 60)} —Å–µ–∫",
                "scraped_by_source": {
                    source: len(listings) 
                    for source, listings in scrape_results.items()
                },
                "total_scraped": total_scraped,
                "database_stats": save_stats
            }
            
            # –í—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            logger.info("\n" + "=" * 80)
            logger.info("üéâ –û–ü–ï–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
            logger.info("=" * 80)
            
            logger.info(f"\n‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {final_stats['elapsed_formatted']}")
            
            logger.info(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ò–°–¢–û–ß–ù–ò–ö–ê–ú:")
            for source, count in final_stats['scraped_by_source'].items():
                logger.info(f"   {source.upper():15} ‚Üí {count} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            
            logger.info(f"\nüíæ –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–û–•–†–ê–ù–ï–ù–ò–Ø:")
            logger.info(f"   –í—Å–µ–≥–æ —Å–∫—Ä–∞–ø–µ–Ω–æ   ‚Üí {total_scraped}")
            logger.info(f"   –°–æ–∑–¥–∞–Ω–æ –Ω–æ–≤—ã—Ö    ‚Üí {save_stats.get('created', 0)}")
            logger.info(f"   –û–±–Ω–æ–≤–ª–µ–Ω–æ        ‚Üí {save_stats.get('updated', 0)}")
            logger.info(f"   –î—É–±–ª–∏–∫–∞—Ç—ã        ‚Üí {save_stats.get('skipped_duplicates', 0)}")
            logger.info(f"   –û—à–∏–±–∫–∏           ‚Üí {save_stats.get('errors', 0)}")
            
            if 'by_source' in save_stats:
                logger.info(f"\nüìà –ü–û–î–†–û–ë–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ò–°–¢–û–ß–ù–ò–ö–ê–ú:")
                for source, stats in save_stats['by_source'].items():
                    logger.info(f"   {source.upper()}:")
                    logger.info(f"      –í—Å–µ–≥–æ     ‚Üí {stats.get('total', 0)}")
                    logger.info(f"      –ù–æ–≤—ã–µ     ‚Üí {stats.get('created', 0)}")
                    logger.info(f"      –û–±–Ω–æ–≤–ª–µ–Ω–æ ‚Üí {stats.get('updated', 0)}")
                    logger.info(f"      –ü—Ä–æ–ø—É—â–µ–Ω–æ ‚Üí {stats.get('skipped', 0)}")
                    logger.info(f"      –û—à–∏–±–∫–∏    ‚Üí {stats.get('errors', 0)}")
            
            logger.info("\n" + "=" * 80)
            
            return final_stats
            
        except Exception as e:
            logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            return {
                "success": False,
                "error": str(e),
                "elapsed_seconds": (datetime.now() - start_time).total_seconds()
            }


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(
        description='–ú–∞—Å—Å–æ–≤–æ–µ —Å–∫—Ä–∞–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –≤ Railway –ë–î'
    )
    parser.add_argument(
        '--pages', 
        type=int, 
        default=20, 
        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (default: 20)'
    )
    parser.add_argument(
        '--local', 
        action='store_true',
        help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –ë–î –≤–º–µ—Å—Ç–æ Railway (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)'
    )
    
    args = parser.parse_args()
    
    logger.info(f"\nüîß –ü–ê–†–ê–ú–ï–¢–†–´ –ó–ê–ü–£–°–ö–ê:")
    logger.info(f"   –°—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫: {args.pages}")
    logger.info(f"   –ë–î: {'–õ–æ–∫–∞–ª—å–Ω–∞—è' if args.local else 'Railway'}")
    logger.info(f"   DATABASE_URL: {settings.DATABASE_URL[:50]}...")
    
    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Å–∫—Ä–∞–ø–µ—Ä
    scraper = BulkScraper(max_pages=args.pages)
    stats = await scraper.run()
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–¥ –≤—ã—Ö–æ–¥–∞
    return 0 if stats.get('success', False) else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
