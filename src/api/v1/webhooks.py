"""
Webhook endpoints –¥–ª—è –ø—Ä–∏–µ–º–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç –≤–Ω–µ—à–Ω–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
"""
from fastapi import APIRouter, HTTPException, Request, BackgroundTasks
from pydantic import BaseModel
from typing import Dict, Any, Optional, List
import logging

from src.parsers.idealista_crawler import IdealistaCrawler
from src.crud.crud_listing import create_listing
from src.db.database import get_db
from src.db.models import Listing

logger = logging.getLogger(__name__)

router = APIRouter()


class ScraperWebhookPayload(BaseModel):
    """Payload –æ—Ç ScraperAPI Crawler webhook"""
    job_id: str
    status: str
    url: str
    html: Optional[str] = None
    status_code: Optional[int] = None
    error: Optional[str] = None


@router.post("/scraper")
async def scraper_webhook(
    payload: ScraperWebhookPayload,
    background_tasks: BackgroundTasks,
    request: Request
):
    """
    Webhook –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç ScraperAPI Crawler
    
    –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ–≥–¥–∞ Crawler –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É
    """
    try:
        logger.info(f"üì¨ –ü–æ–ª—É—á–µ–Ω webhook –æ—Ç Crawler")
        logger.info(f"   Job ID: {payload.job_id}")
        logger.info(f"   Status: {payload.status}")
        logger.info(f"   URL: {payload.url}")
        logger.info(f"   Status Code: {payload.status_code}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å
        if payload.status != "finished" or not payload.html:
            logger.warning(f"‚ö†Ô∏è –ù–µ—É—Å–ø–µ—à–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {payload.error or 'No HTML'}")
            return {"status": "received", "processed": False}
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º HTML –≤ —Ñ–æ–Ω–µ
        background_tasks.add_task(
            process_crawler_result,
            payload.job_id,
            payload.url,
            payload.html
        )
        
        return {
            "status": "received",
            "processed": True,
            "job_id": payload.job_id
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ webhook: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def process_crawler_result(job_id: str, page_url: str, html_content: str):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç crawler –≤ —Ñ–æ–Ω–µ
    
    Args:
        job_id: ID –∑–∞–¥–∞—á–∏ crawler
        page_url: URL –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        html_content: HTML –∫–æ–Ω—Ç–µ–Ω—Ç
    """
    try:
        logger.info(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç crawler")
        logger.info(f"   Job ID: {job_id}")
        logger.info(f"   URL: {page_url}")
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä
        crawler = IdealistaCrawler()
        
        # –ü–∞—Ä—Å–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        listings = crawler.parse_listing_from_html(html_content, page_url)
        
        if not listings:
            logger.info(f"üìÑ –û–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_url}")
            return
        
        logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        saved_count = 0
        for db in get_db():
            for listing_data in listings:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ
                    existing = db.query(Listing).filter(
                        Listing.external_id == listing_data.get('external_id')
                    ).first()
                    
                    if not existing:
                        create_listing(db, listing_data)
                        saved_count += 1
                        logger.debug(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –Ω–æ–≤–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: {listing_data.get('external_id')}")
                    else:
                        logger.debug(f"‚è≠Ô∏è  –û–±—ä—è–≤–ª–µ–Ω–∏–µ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {listing_data.get('external_id')}")
                        
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
            
            break  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é —Å–µ—Å—Å–∏—é
        
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count}/{len(listings)} –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ crawler: {e}")


@router.get("/scraper/test")
async def test_scraper_webhook():
    """–¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã webhook"""
    return {
        "status": "ok",
        "message": "Scraper webhook is working",
        "endpoint": "/api/v1/webhooks/scraper"
    }

