"""
–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
"""
import logging
import time
import asyncio
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Union
import aiohttp
import requests
from bs4 import BeautifulSoup

from src.core.config import settings

logger = logging.getLogger(__name__)


class BaseParser(ABC):
    """
    –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–±—â–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∏ –±–∞–∑–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
    """
    
    def __init__(self, name: str, base_url: str):
        self.name = name
        self.base_url = base_url
        self.scraperapi_key = getattr(settings, 'SCRAPERAPI_KEY', None) or getattr(settings, 'SCRAPER_API_KEY', None)
        self.user_agent = settings.USER_AGENT if hasattr(settings, 'USER_AGENT') else "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        
    @abstractmethod
    def build_search_url(self, filters: Dict[str, Any], page: int = 1) -> str:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å URL –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
        
        Args:
            filters: –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ–∏—Å–∫–∞
            page: –ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            
        Returns:
            URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        """
        pass
    
    @abstractmethod
    def parse_listings_from_page(self, html_content: str) -> List[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ—á—å –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        Args:
            html_content: HTML –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        """
        pass
    
    @abstractmethod
    def normalize_listing_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
        
        Args:
            raw_data: –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            
        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        pass
    
    def get_html_content(self, url: str, retries: int = 3, use_scraperapi: bool = True) -> Optional[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å HTML –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            retries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
            use_scraperapi: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ScraperAPI –∏–ª–∏ –æ–±—ã—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            HTML –∫–æ–Ω—Ç–µ–Ω—Ç –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if use_scraperapi and self.scraperapi_key:
            return self._get_html_with_scraperapi(url, retries)
        else:
            return self._get_html_direct(url, retries)
    
    def _get_html_with_scraperapi(self, url: str, retries: int = 3) -> Optional[str]:
        """–ü–æ–ª—É—á–∏—Ç—å HTML —á–µ—Ä–µ–∑ ScraperAPI"""
        params = {
            'api_key': self.scraperapi_key,
            'url': url,
            'render': 'true',
            'premium': 'true',
            'country_code': 'it'
        }
        
        for attempt in range(retries):
            try:
                logger.info(f"üåê [{self.name}] –ó–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ ScraperAPI: {url[:100]}...")
                response = requests.get(
                    'https://api.scraperapi.com/', 
                    params=params, 
                    timeout=180
                )
                
                if response.status_code >= 500:
                    logger.warning(f"‚ö†Ô∏è [{self.name}] ScraperAPI –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É {response.status_code}. –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries}")
                    time.sleep(5 * (attempt + 1))
                    continue
                    
                response.raise_for_status()
                logger.info(f"‚úÖ [{self.name}] –ü–æ–ª—É—á–µ–Ω HTML –∫–æ–Ω—Ç–µ–Ω—Ç ({len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤)")
                return response.text
                
            except requests.exceptions.RequestException as e:
                logger.error(f"‚ùå [{self.name}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —á–µ—Ä–µ–∑ ScraperAPI: {e}")
                if attempt < retries - 1:
                    time.sleep(5 * (attempt + 1))
                    
        return None
    
    def _get_html_direct(self, url: str, retries: int = 3) -> Optional[str]:
        """–ü–æ–ª—É—á–∏—Ç—å HTML –Ω–∞–ø—Ä—è–º—É—é"""
        headers = {
            'User-Agent': self.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'it-IT,it;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        
        for attempt in range(retries):
            try:
                logger.info(f"üåê [{self.name}] –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å: {url[:100]}...")
                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                logger.info(f"‚úÖ [{self.name}] –ü–æ–ª—É—á–µ–Ω HTML –∫–æ–Ω—Ç–µ–Ω—Ç ({len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤)")
                return response.text
                
            except requests.exceptions.RequestException as e:
                logger.error(f"‚ùå [{self.name}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä—è–º–æ–º –∑–∞–ø—Ä–æ—Å–µ: {e}")
                if attempt < retries - 1:
                    time.sleep(2 * (attempt + 1))
                    
        return None
    
    async def get_html_content_async(self, url: str, retries: int = 3) -> Optional[str]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            retries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
            
        Returns:
            HTML –∫–æ–Ω—Ç–µ–Ω—Ç –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        headers = {
            'User-Agent': self.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'it-IT,it;q=0.8,en-US;q=0.5,en;q=0.3',
        }
        
        for attempt in range(retries):
            try:
                async with aiohttp.ClientSession() as session:
                    logger.info(f"üåê [{self.name}] –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {url[:100]}...")
                    async with session.get(url, headers=headers, timeout=30) as response:
                        response.raise_for_status()
                        content = await response.text()
                        logger.info(f"‚úÖ [{self.name}] –ü–æ–ª—É—á–µ–Ω HTML –∫–æ–Ω—Ç–µ–Ω—Ç ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")
                        return content
                        
            except Exception as e:
                logger.error(f"‚ùå [{self.name}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ: {e}")
                if attempt < retries - 1:
                    await asyncio.sleep(2 * (attempt + 1))
                    
        return None
    
    def scrape_listings(
        self, 
        filters: Dict[str, Any], 
        max_pages: int = 5,
        use_scraperapi: bool = True
    ) -> List[Dict[str, Any]]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        
        Args:
            filters: –§–∏–ª—å—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            use_scraperapi: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ScraperAPI
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        """
        all_listings = []
        
        logger.info(f"üöÄ [{self.name}] –ù–∞—á–∏–Ω–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏: {filters}")
        logger.info(f"üìÑ [{self.name}] –ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü: {max_pages}")
        
        for page_num in range(1, max_pages + 1):
            try:
                # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å URL –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                search_url = self.build_search_url(filters, page_num)
                logger.info(f"üìÑ [{self.name}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num}: {search_url[:100]}...")
                
                # –ü–æ–ª—É—á–∏—Ç—å HTML –∫–æ–Ω—Ç–µ–Ω—Ç
                html_content = self.get_html_content(search_url, use_scraperapi=use_scraperapi)
                if not html_content:
                    logger.error(f"‚ùå [{self.name}] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                    continue
                
                # –ò–∑–≤–ª–µ—á—å –æ–±—ä—è–≤–ª–µ–Ω–∏—è
                page_listings = self.parse_listings_from_page(html_content)
                if not page_listings:
                    logger.info(f"üìÑ [{self.name}] –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –ó–∞–≤–µ—Ä—à–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥.")
                    break
                
                # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ
                normalized_listings = []
                for raw_listing in page_listings:
                    try:
                        normalized = self.normalize_listing_data(raw_listing)
                        if normalized:
                            normalized_listings.append(normalized)
                    except Exception as e:
                        logger.error(f"‚ùå [{self.name}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
                        continue
                
                all_listings.extend(normalized_listings)
                logger.info(f"‚úÖ [{self.name}] –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –Ω–∞–π–¥–µ–Ω–æ {len(page_listings)}, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(normalized_listings)}")
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
                if page_num < max_pages:
                    time.sleep(2)
                    
            except Exception as e:
                logger.error(f"‚ùå [{self.name}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
                continue
        
        logger.info(f"üéâ [{self.name}] –°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω! –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        return all_listings
    
    def extract_number_from_string(self, text: str) -> Optional[int]:
        """–ò–∑–≤–ª–µ—á—å —á–∏—Å–ª–æ –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        import re
        if not text:
            return None
        match = re.search(r'\d+', str(text))
        return int(match.group(0)) if match else None
    
    def clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∏—Ç—å —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not text:
            return ""
        return text.strip().replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
    
    def validate_listing_data(self, data: Dict[str, Any]) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        
        Args:
            data: –î–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            
        Returns:
            True –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –≤–∞–ª–∏–¥–Ω—ã
        """
        required_fields = ['title', 'url', 'source', 'external_id']
        
        for field in required_fields:
            if not data.get(field):
                logger.warning(f"‚ö†Ô∏è [{self.name}] –ü—Ä–æ–ø—É—â–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ '{field}'")
                return False
        
        return True 