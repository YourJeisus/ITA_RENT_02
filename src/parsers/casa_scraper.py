#!/usr/bin/env python3
"""
üöÄ –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–ê–†–°–ï–† CASA.IT
–í—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ JSON - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±—ã—Å—Ç—Ä–æ!
"""
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from src.core.config import settings
import json
import re
from datetime import datetime
from typing import List, Dict, Any, Optional
from src.parsers.description_analyzer import DescriptionAnalyzer

class CasaScraper:
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä Casa.it"""
    
    def __init__(self, max_concurrent: int = 10, enable_geocoding: bool = False):
        self.base_url = "https://www.casa.it"
        self.api_url = "https://api.scraperapi.com/"
        self.api_key = settings.SCRAPERAPI_KEY
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.image_base_url = "https://images-1.casa.it/"
        self.enable_geocoding = enable_geocoding  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'success': 0,
            'failed': 0,
            'pages_scraped': 0
        }
    
    async def fetch_html(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —á–µ—Ä–µ–∑ ScraperAPI"""
        async with self.semaphore:
            params = {
                'api_key': self.api_key,
                'url': url,
                'render': 'true',
                'ultra_premium': 'true'
            }
            
            try:
                async with session.get(self.api_url, params=params, timeout=aiohttp.ClientTimeout(total=90)) as response:
                    if response.status == 200:
                        return await response.text()
                    return None
            except Exception as e:
                return None
    
    def extract_initial_state(self, html: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –¥–∞–Ω–Ω—ã–µ –∏–∑ window.__INITIAL_STATE__"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            for script in soup.find_all('script'):
                if script.string and 'window.__INITIAL_STATE__' in script.string:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON —Å—Ç—Ä–æ–∫—É
                    match = re.search(r'JSON\.parse\("(.+?)"\);', script.string, re.DOTALL)
                    
                    if match:
                        json_str = match.group(1)
                        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º escape –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                        json_str = json_str.replace('\\"', '"')
                        json_str = json_str.replace('\\/', '/')
                        json_str = json_str.encode().decode('unicode_escape')
                        
                        data = json.loads(json_str)
                        return data
            
            return None
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON: {e}")
            return None
    
    def extract_advertiser_type(self, listing_data: Dict[str, Any]) -> Optional[bool]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–∏–ø —Ä–µ–∫–ª–∞–º–æ–¥–∞—Ç–µ–ª—è (—á–∞—Å—Ç–Ω–æ–µ –ª–∏—Ü–æ / –∞–≥–µ–Ω—Ç—Å—Ç–≤–æ).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: False = —á–∞—Å—Ç–Ω–æ–µ (–±–µ–∑ –∫–æ–º–∏—Å—Å–∏–∏), True = –∞–≥–µ–Ω—Ç—Å—Ç–≤–æ (—Å –∫–æ–º–∏—Å—Å–∏–µ–π), None = –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ
        """
        advertiser = listing_data.get('advertiser', {})
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º isPrivate
        is_private = advertiser.get('isPrivate')
        if is_private is True:
            return False  # –ß–∞—Å—Ç–Ω–æ–µ –ª–∏—Ü–æ - –±–µ–∑ –∫–æ–º–∏—Å—Å–∏–∏
        elif is_private is False:
            return True  # –ê–≥–µ–Ω—Ç—Å—Ç–≤–æ - —Å –∫–æ–º–∏—Å—Å–∏–µ–π
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º type
        adv_type = advertiser.get('type', '').lower()
        if adv_type == 'private' or adv_type == 'privati':
            return False
        elif adv_type == 'agency' or adv_type == 'agenzie':
            return True
        
        return None
    
    def parse_listing(self, listing_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –∏–∑ JSON"""
        try:
            data = {
                'scraped_at': datetime.utcnow().isoformat(),
                'source': 'casa_it',
                'external_id': f"casa_it_{listing_data['id']}",
                'url': self.base_url + listing_data['uri']
            }
            
            # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            data['title'] = listing_data.get('highlight') or listing_data.get('title', {}).get('main', '')
            data['description'] = listing_data.get('description', '')
            data['property_type'] = listing_data.get('propertyType', '')
            
            # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–∑ features
            features = listing_data.get('features', {})
            
            # –ü–ª–æ—â–∞–¥—å
            if features.get('mq'):
                data['area'] = int(features['mq'])
            
            # –ö–æ–º–Ω–∞—Ç—ã
            if features.get('rooms'):
                data['rooms'] = int(features['rooms'])
            
            # –í–∞–Ω–Ω—ã–µ
            if features.get('bathrooms'):
                data['bathrooms'] = int(features['bathrooms'])
            
            # –≠—Ç–∞–∂
            if features.get('level'):
                data['floor'] = features['level']
            
            # –¶–µ–Ω–∞
            price_data = features.get('price', {})
            if price_data.get('value'):
                # –£–±–∏—Ä–∞–µ–º —Ç–æ—á–∫–∏ (—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç—ã—Å—è—á)
                price_str = str(price_data['value']).replace('.', '')
                data['price'] = float(price_str)
            elif price_data.get('marker', {}).get('price'):
                # –¶–µ–Ω–∞ –¥–æ–≥–æ–≤–æ—Ä–Ω–∞—è
                data['price_text'] = price_data['marker']['price']
            
            # –ì–µ–æ–ª–æ–∫–∞—Ü–∏—è
            geo_info = listing_data.get('geoInfos', {})
            
            # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (100% –µ—Å—Ç—å!)
            if geo_info.get('lat') and geo_info.get('lon'):
                data['latitude'] = float(geo_info['lat'])
                data['longitude'] = float(geo_info['lon'])
            
            # –ê–¥—Ä–µ—Å
            if geo_info.get('street'):
                data['address'] = geo_info['street']
            
            data['city'] = geo_info.get('city', 'Roma')
            data['district'] = geo_info.get('district_name', '')
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            media = listing_data.get('media', {})
            items = media.get('items', [])
            
            images = []
            for item in items:
                if item.get('uri'):
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    uri = item['uri']
                    if uri.startswith('http'):
                        # –£–∂–µ –ø–æ–ª–Ω—ã–π URL
                        img_url = uri
                    else:
                        # Casa.it —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–∑–º–µ—Ä –≤ URL: /800x600/ –∏–ª–∏ /360x265/
                        # –£–±–∏—Ä–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π —Å–ª–µ—à –∏–∑ uri –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                        uri = uri.lstrip('/')
                        img_url = self.image_base_url + '800x600/' + uri
                    images.append(img_url)
            
            data['images'] = images
            
            # –ò–∑–¥–∞—Ç–µ–ª—å (–∏—Å–ø–æ–ª—å–∑—É–µ–º agency_name –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è)
            publisher = listing_data.get('publisher', {})
            if publisher:
                data['agency_name'] = publisher.get('publisherName', '')
                data['contact_phone'] = publisher.get('publisherPhone', '')
                data['contact_website'] = publisher.get('publisherWebsite', '')
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            features_list = []
            if features.get('mq'):
                features_list.append(f"{features['mq']} m¬≤")
            if features.get('rooms'):
                features_list.append(f"{features['rooms']} locali")
            if features.get('bathrooms'):
                features_list.append(f"{features['bathrooms']} bagni")
            if features.get('level'):
                features_list.append(features['level'])
            if features.get('energyClass'):
                features_list.append(f"Classe energetica {features['energyClass']}")
            
            data['features'] = features_list
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø —Ä–µ–∫–ª–∞–º–æ–¥–∞—Ç–µ–ª—è –Ω–∞–ø—Ä—è–º—É—é –∏–∑ JSON
            agency_commission_from_json = self.extract_advertiser_type(listing_data)
            
            # –ê–Ω–∞–ª–∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä–æ–≤
            description = data.get('description', '')
            analysis = DescriptionAnalyzer.analyze(description, floor=data.get('floor'))
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Casa.it JSON –Ω–∞–¥ DescriptionAnalyzer
            data['agency_commission'] = agency_commission_from_json if agency_commission_from_json is not None else analysis.get('agency_commission')
            
            # –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª—è –∏–∑ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
            data['pets_allowed'] = analysis.get('pets_allowed')
            data['children_friendly'] = analysis.get('children_friendly')
            data['renovation_type'] = analysis.get('renovation_type')
            data['building_type'] = analysis.get('building_type')
            data['year_built'] = analysis.get('year_built')
            data['total_floors'] = analysis.get('total_floors')
            data['floor_number'] = analysis.get('floor_number')
            data['is_first_floor'] = analysis.get('is_first_floor')
            data['is_top_floor'] = analysis.get('is_top_floor')
            data['park_nearby'] = analysis.get('park_nearby')
            data['noisy_roads_nearby'] = analysis.get('noisy_roads_nearby')
            
            return data
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
            return None
    
    async def scrape_page(self, session: aiohttp.ClientSession, page_num: int) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å–æ —Å–ø–∏—Å–∫–æ–º –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
        
        if page_num == 1:
            url = f"{self.base_url}/affitto/residenziale/roma/"
        else:
            url = f"{self.base_url}/affitto/residenziale/roma/?p={page_num}"
        
        print(f"\nüîÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: {url}")
        
        html = await self.fetch_html(session, url)
        
        if not html:
            print(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML")
            return []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –¥–∞–Ω–Ω—ã–µ
        initial_state = self.extract_initial_state(html)
        
        if not initial_state:
            print(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å JSON –¥–∞–Ω–Ω—ã–µ")
            return []
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        search_data = initial_state.get('search', {})
        listings_data = search_data.get('list', [])
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(listings_data)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        
        # –ü–∞—Ä—Å–∏–º –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        parsed_listings = []
        for listing_data in listings_data:
            parsed = self.parse_listing(listing_data)
            if parsed:
                parsed_listings.append(parsed)
                self.stats['success'] += 1
            else:
                self.stats['failed'] += 1
        
        self.stats['pages_scraped'] += 1
        
        return parsed_listings
    
    async def scrape_multiple_pages(self, max_pages: int = 5):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º (—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å)"""
        return await self.scrape_parallel(num_pages=max_pages)
    
    async def scrape_parallel(self, num_pages: int = 5):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º"""
        print("=" * 80)
        print(f"üöÄ –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì CASA.IT (–¥–æ {self.max_concurrent} –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)")
        print("=" * 80)
        print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü: {num_pages}")
        print("=" * 80)
        
        start_time = datetime.utcnow()
        
        async with aiohttp.ClientSession() as session:
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            tasks = [
                self.scrape_page(session, page_num)
                for page_num in range(1, num_pages + 1)
            ]
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            results = await asyncio.gather(*tasks)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            all_listings = []
            for page_listings in results:
                all_listings.extend(page_listings)
        
        end_time = datetime.utcnow()
        elapsed = (end_time - start_time).total_seconds()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n" + "=" * 80)
        print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("=" * 80)
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {self.stats['success']}")
        print(f"‚ùå –û—à–∏–±–∫–∏: {self.stats['failed']}")
        print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü: {self.stats['pages_scraped']}")
        print()
        print(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {elapsed:.1f} —Å–µ–∫—É–Ω–¥")
        if all_listings:
            print(f"‚ö° –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: {elapsed/len(all_listings):.2f} —Å–µ–∫")
            print(f"üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {len(all_listings)/elapsed*60:.1f} –æ–±—ä—è–≤–ª–µ–Ω–∏–π/–º–∏–Ω—É—Ç—É")
        
        return all_listings


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description='–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä Casa.it')
    parser.add_argument('--pages', type=int, default=5, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5)')
    parser.add_argument('--concurrent', type=int, default=5, help='–û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5)')
    
    args = parser.parse_args()
    
    scraper = CasaScraper(max_concurrent=args.concurrent)
    
    # –ü–∞—Ä—Å–∏–º
    listings = await scraper.scrape_parallel(num_pages=args.pages)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    if listings:
        output_file = f'/tmp/casa_it_results.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(listings, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª–µ–π
        print("\nüìã –ó–ê–ü–û–õ–ù–ï–ù–ù–û–°–¢–¨ –ü–û–õ–ï–ô:")
        fields_count = {}
        for listing in listings:
            for key, value in listing.items():
                if key not in fields_count:
                    fields_count[key] = 0
                if value and value != [] and value != 'N/A':
                    fields_count[key] += 1
        
        for key in sorted(fields_count.keys()):
            count = fields_count[key]
            percentage = (count / len(listings)) * 100
            print(f"   ‚Ä¢ {key:20s}: {count:3d}/{len(listings)} ({percentage:5.1f}%)")


if __name__ == "__main__":
    asyncio.run(main())

