#!/usr/bin/env python3
"""
üöÄ –ê–°–ò–ù–•–†–û–ù–ù–´–ô –°–ö–†–ê–ü–ï–† –î–õ–Ø IDEALISTA.IT V1
–°–æ–∑–¥–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã ImmobiliareScraper –∏ SubitoScraper

–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:
‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è ScraperAPI Async API
‚úÖ Job submission -> Status polling -> Result extraction
‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ external_id
‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å —Ä–∞–∑–Ω—ã–º–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏
‚úÖ Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π API –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö
‚úÖ –ì–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ OpenStreetMap
"""
import asyncio
import aiohttp
import logging
import json
import re
import time
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
from ..core.config import settings

logger = logging.getLogger(__name__)


class IdealistaScraper:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–µ—Ä –¥–ª—è Idealista.it
    –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ ImmobiliareScraper –∏ SubitoScraper
    """
    
    def __init__(self, enable_geocoding: bool = True):
        self.name = "Idealista.it Async Scraper V1"
        self.base_url = "https://www.idealista.it"
        self.search_url = "https://www.idealista.it/affitto-case/roma-roma/"
        self.enable_geocoding = enable_geocoding
        
        # ScraperAPI endpoints
        self.async_jobs_url = "https://async.scraperapi.com/jobs"
        self.sync_api_url = "https://api.scraperapi.com"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–∞–π–º–∞—É—Ç–æ–≤
        self.job_submit_timeout = 30
        self.job_poll_timeout = 300  # 5 –º–∏–Ω—É—Ç –º–∞–∫—Å–∏–º—É–º
        self.sync_request_timeout = 70  # –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        
        # –ö–µ—à –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        self.seen_listing_ids: Set[str] = set()
    
    def build_page_url(self, page: int) -> str:
        """–°—Ç—Ä–æ–∏—Ç URL –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        if page <= 1:
            return f"{self.search_url}?ordine=pubblicazione-desc"
        return f"{self.search_url}lista-{page}.htm?ordine=pubblicazione-desc"
    
    async def submit_async_job(self, url: str, page_num: int) -> Optional[Dict[str, Any]]:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–¥–∞—á—É –≤ ScraperAPI Async API"""
        try:
            payload = {
                "apiKey": settings.SCRAPERAPI_KEY,
                "url": url,
                "device": "desktop",
                "render": "true",  # –í–∫–ª—é—á–∞–µ–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ JS
                "wait": 5000,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è
                "premium": "true",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–º–∏—É–º
                "session_number": 1  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ—Å—Å–∏—é
            }
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.job_submit_timeout)) as session:
                logger.info(f"üîÑ [{self.name}] –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {url[:80]}...")
                
                async with session.post(self.async_jobs_url, json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        job_id = result.get("id")
                        if job_id:
                            logger.info(f"‚úÖ [{self.name}] –ó–∞–¥–∞—á–∞ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞, ID: {job_id}")
                            return {"job_id": job_id, "page_num": page_num, "url": url}
                        else:
                            logger.error(f"‚ùå [{self.name}] –ù–µ –ø–æ–ª—É—á–µ–Ω job_id –≤ –æ—Ç–≤–µ—Ç–µ: {result}")
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå [{self.name}] –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –∑–∞–¥–∞—á–∏ (status {response.status}): {error_text}")
                        
        except Exception as e:
            logger.error(f"‚ùå [{self.name}] –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –∑–∞–¥–∞—á–∏ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
        
        return None
    
    async def poll_job_result(self, job_info: Dict[str, Any]) -> Optional[str]:
        """–û–ø—Ä–∞—à–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–¥–∞—á–∏ ScraperAPI"""
        job_id = job_info["job_id"]
        page_num = job_info["page_num"]
        
        try:
            result_url = f"{self.async_jobs_url}/{job_id}"
            params = {"apiKey": settings.SCRAPERAPI_KEY}
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.job_poll_timeout)) as session:
                start_time = time.time()
                
                while time.time() - start_time < self.job_poll_timeout:
                    async with session.get(result_url, params=params) as response:
                        if response.status == 200:
                            try:
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º content-type
                                content_type = response.headers.get('content-type', '').lower()
                                if 'application/json' not in content_type:
                                    logger.warning(f"‚ö†Ô∏è [{self.name}] –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π content-type: {content_type} –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                                    text_response = await response.text()
                                    # –ï—Å–ª–∏ —ç—Ç–æ HTML —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º, –ø–æ–ø—Ä–æ–±—É–µ–º –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
                                    if text_response and '<html' in text_response.lower():
                                        logger.info(f"‚úÖ [{self.name}] –ü–æ–ª—É—á–µ–Ω HTML —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                                        return text_response
                                    await asyncio.sleep(5)
                                    continue
                                
                                result = await response.json()
                                status = result.get("statusCode")
                                
                                if status == "finished":
                                    response_body = result.get("response", {}).get("body")
                                    if response_body:
                                        logger.info(f"‚úÖ [{self.name}] –ü–æ–ª—É—á–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                                        return response_body
                                    else:
                                        logger.error(f"‚ùå [{self.name}] –ü—É—Å—Ç–æ–π response body –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                                        return None
                                elif status == "failed":
                                    logger.error(f"‚ùå [{self.name}] –ó–∞–¥–∞—á–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–æ–π –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                                    return None
                                else:
                                    # –ó–∞–¥–∞—á–∞ –µ—â–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è
                                    await asyncio.sleep(5)
                            except Exception as json_error:
                                logger.warning(f"‚ö†Ô∏è [{self.name}] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {json_error}")
                                await asyncio.sleep(5)
                        else:
                            logger.error(f"‚ùå [{self.name}] –û—à–∏–±–∫–∞ –æ–ø—Ä–æ—Å–∞ –∑–∞–¥–∞—á–∏ (status {response.status})")
                            await asyncio.sleep(5)
                
                logger.error(f"‚è∞ [{self.name}] –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–∂–∏–¥–∞–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                
        except Exception as e:
            logger.error(f"‚ùå [{self.name}] –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –æ–ø—Ä–æ—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
        
        return None
    
    async def fallback_sync_request(self, url: str, page_num: int) -> Optional[str]:
        """Fallback –Ω–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π API ScraperAPI –∏–ª–∏ –ø—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º ScraperAPI –±–µ–∑ –≥–µ–æ—Ç–∞—Ä–≥–µ—Ç–∏–Ω–≥–∞
        try:
            params = {
                "api_key": settings.SCRAPERAPI_KEY,
                "url": url,
                "device_type": "desktop",
                "render": "true",
                "wait": 5000,
                "premium": "true",
                "session_number": 1
            }
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.sync_request_timeout)) as session:
                logger.info(f"üîÑ [{self.name}] Fallback ScraperAPI –∑–∞–ø—Ä–æ—Å –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                
                async with session.get(self.sync_api_url, params=params) as response:
                    if response.status == 200:
                        html_content = await response.text()
                        logger.info(f"‚úÖ [{self.name}] Fallback ScraperAPI —É—Å–ø–µ—à–µ–Ω –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                        return html_content
                    else:
                        error_text = await response.text()
                        logger.warning(f"‚ö†Ô∏è [{self.name}] ScraperAPI fallback –æ—à–∏–±–∫–∞ (status {response.status}): {error_text[:200]}...")
                        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{self.name}] ScraperAPI fallback –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
        
        # –ï—Å–ª–∏ ScraperAPI –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—Ä–æ–±—É–µ–º –ø—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            }
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                logger.info(f"üîÑ [{self.name}] –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} (—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)")
                
                async with session.get(url, headers=headers) as response:
                    if response.status == 200:
                        html_content = await response.text()
                        logger.info(f"‚úÖ [{self.name}] –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–µ–Ω –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                        return html_content
                    else:
                        logger.error(f"‚ùå [{self.name}] –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –æ—à–∏–±–∫–∞ (status {response.status})")
                        
        except Exception as e:
            logger.error(f"‚ùå [{self.name}] –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –≤ –ø—Ä—è–º–æ–º –∑–∞–ø—Ä–æ—Å–µ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
        
        return None
    
    def parse_listings_from_html(self, html_content: str, page_num: int) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ HTML"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è Idealista)
            listing_containers = (
                soup.find_all('article', class_='item') or
                soup.find_all('div', class_='item') or
                soup.find_all('article', class_='item-multimedia-container') or
                soup.find_all('div', class_='item-multimedia-container') or
                soup.find_all('div', class_='item-info-container') or
                soup.find_all('article', {'data-element-id': True}) or
                soup.find_all('div', {'data-adid': True}) or
                soup.find_all('div', {'data-id': True}) or
                soup.find_all('article', {'itemtype': re.compile(r'.*Product.*')}) or
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                soup.find_all('div', class_=re.compile(r'.*item.*')) or
                soup.find_all('article', class_=re.compile(r'.*item.*'))
            )
            
            if not listing_containers:
                # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                logger.warning(f"‚ö†Ô∏è [{self.name}] –ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–æ–æ–±—â–µ –µ—Å—Ç—å –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                title_tag = soup.find('title')
                page_title = title_tag.get_text(strip=True) if title_tag else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                logger.info(f"üîç [{self.name}] –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_title}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—à–∏–±–æ–∫ –∏–ª–∏ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
                if any(keyword in page_title.lower() for keyword in ['error', 'not found', '404', 'captcha', 'blocked']):
                    logger.error(f"‚ùå [{self.name}] –°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—à–∏–±–∫—É: {page_title}")
                elif 'idealista' not in page_title.lower():
                    logger.warning(f"‚ö†Ô∏è [{self.name}] –í–æ–∑–º–æ–∂–Ω–æ –ø–æ–ª—É—á–µ–Ω–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {page_title}")
                
                # –ò—â–µ–º –ª—é–±—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
                all_links = soup.find_all('a', href=True)
                listing_links = [link for link in all_links if '/immobile/' in link.get('href', '')]
                if listing_links:
                    logger.info(f"üîó [{self.name}] –ù–∞–π–¥–µ–Ω–æ {len(listing_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã")
                
                return []
            
            listings = []
            
            for container in listing_containers:
                try:
                    listing = self.parse_single_listing(container)
                    if listing and listing.get('external_id'):
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
                        external_id = listing['external_id']
                        if external_id not in self.seen_listing_ids:
                            self.seen_listing_ids.add(external_id)
                            listings.append(listing)
                        else:
                            logger.debug(f"üîÑ –î—É–±–ª–∏–∫–∞—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –ø—Ä–æ–ø—É—â–µ–Ω: {external_id}")
                except Exception as e:
                    logger.error(f"‚ùå [{self.name}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
                    continue
            
            logger.info(f"üìä [{self.name}] –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –Ω–∞–π–¥–µ–Ω–æ {len(listing_containers)} –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤, –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(listings)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            return listings
            
        except Exception as e:
            logger.error(f"‚ùå [{self.name}] –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
            return []
    
    def parse_single_listing(self, container) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞"""
        try:
            # ID –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            external_id = None
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è ID
            if container.get('data-element-id'):
                external_id = container.get('data-element-id')
            elif container.get('data-adid'):
                external_id = container.get('data-adid')
            elif container.get('id'):
                external_id = container.get('id')
            else:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ —Å—Å—ã–ª–∫–µ
                link = container.find('a', href=True)
                if link:
                    href = link.get('href', '')
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL —Ç–∏–ø–∞ /immobile/123456/
                    id_match = re.search(r'/immobile/(\d+)/', href)
                    if id_match:
                        external_id = id_match.group(1)
            
            if not external_id:
                return None
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ - –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            title_elem = (
                container.find('a', class_='item-link') or
                container.find('h2') or
                container.find('h3') or
                container.find('h4') or
                container.find('span', class_='item-title') or
                container.find('div', class_='item-title') or
                container.find('a', class_='item-title') or
                container.find('a', href=re.compile(r'/immobile/')) or
                container.find('a', href=True)
            )
            
            title = title_elem.get_text(strip=True) if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            
            # –¶–µ–Ω–∞ - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è Idealista
            price = 0
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –ø–æ–∏—Å–∫—É —Ü–µ–Ω—ã
            price_elem = None
            
            # –ü–æ–¥—Ö–æ–¥ 1: CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            price_candidates = container.select('span.item-price, .price-row, [class*=\"price\"]')
            for candidate in price_candidates:
                text = candidate.get_text(strip=True)
                if '‚Ç¨' in text and any(c.isdigit() for c in text):
                    price_elem = candidate
                    break
            
            # –ü–æ–¥—Ö–æ–¥ 2: –ø–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º –±–µ–∑ BeautifulSoup find
            if not price_elem:
                for elem in container.find_all(['span', 'div']):
                    classes = elem.get('class', [])
                    classes_str = ' '.join(classes) if classes else ''
                    if 'price' in classes_str and '‚Ç¨' in elem.get_text():
                        price_elem = elem
                        break
            
            # Debug –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –ª–æ–≥–æ–≤
            if price_elem:
                logger.debug(f"Price element found: {price_elem.name} with classes {price_elem.get('class', [])}")
            
            if price_elem:
                if hasattr(price_elem, 'get_text'):
                    price_text = price_elem.get_text(strip=True)
                else:
                    price_text = str(price_elem).strip()
                
                # –ü–∞—Ä—Å–∏–º —Ü–µ–Ω—ã –≤ –∏—Ç–∞–ª—å—è–Ω—Å–∫–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (1.500‚Ç¨, 4.000‚Ç¨/mese)
                # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä, —Ç–æ—á–µ–∫ –∏ –∑–∞–ø—è—Ç—ã—Ö
                price_clean = re.sub(r'[^\d,.]', '', price_text)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Ç–∞–ª—å—è–Ω—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç —á–∏—Å–µ–ª (—Ç–æ—á–∫–∞ = —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Ç—ã—Å—è—á, –∑–∞–ø—è—Ç–∞—è = –¥–µ—Å—è—Ç–∏—á–Ω—ã–µ)
                if ',' in price_clean and '.' in price_clean:
                    # –ï—Å—Ç—å –∏ —Ç–æ—á–∫–∏ –∏ –∑–∞–ø—è—Ç—ã–µ: 1.234,56
                    parts = price_clean.split(',')
                    if len(parts) == 2:
                        integer_part = parts[0].replace('.', '')  # —É–±–∏—Ä–∞–µ–º —Ç–æ—á–∫–∏-—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç—ã—Å—è—á
                        decimal_part = parts[1]
                        price_clean = f"{integer_part}.{decimal_part}"
                elif '.' in price_clean and len(price_clean.split('.')[-1]) != 2:
                    # –¢–æ–ª—å–∫–æ —Ç–æ—á–∫–∏, –Ω–æ –Ω–µ –¥–µ—Å—è—Ç–∏—á–Ω—ã–µ: 4.000 (—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Ç—ã—Å—è—á)
                    price_clean = price_clean.replace('.', '')
                elif ',' in price_clean:
                    # –¢–æ–ª—å–∫–æ –∑–∞–ø—è—Ç–∞—è: 1234,56
                    price_clean = price_clean.replace(',', '.')
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ
                try:
                    if price_clean and price_clean.replace('.', '').isdigit():
                        price = float(price_clean)
                        logger.debug(f"Price parsed successfully: '{price_text}' -> {price}‚Ç¨")
                    else:
                        logger.warning(f"Price clean is not a number: '{price_clean}' from '{price_text}'")
                except ValueError as e:
                    logger.error(f"Error parsing price: '{price_text}' -> {e}")
            
            # –ü–ª–æ—â–∞–¥—å
            area = None
            area_elem = container.find('span', string=re.compile(r'm¬≤|mq')) or \
                       container.find('div', string=re.compile(r'm¬≤|mq'))
            
            if area_elem:
                area_text = area_elem.get_text(strip=True)
                area_match = re.search(r'(\d+)', area_text)
                if area_match:
                    area = int(area_match.group(1))
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç - —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
            rooms = None
            
            # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É —Ç–µ–∫—Å—Ç–∞
            rooms_elem = container.find('span', string=re.compile(r'locale|stanz|camer')) or \
                        container.find('div', string=re.compile(r'locale|stanz|camer'))
            
            if not rooms_elem:
                # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ (bilocale, trilocale, etc.)
                title_lower = title.lower()
                if 'monolocale' in title_lower or 'mono' in title_lower:
                    rooms = 1
                elif 'bilocale' in title_lower or 'bilo' in title_lower:
                    rooms = 2
                elif 'trilocale' in title_lower or 'trilo' in title_lower:
                    rooms = 3
                elif 'quadrilocale' in title_lower:
                    rooms = 4
                else:
                    # –ú–µ—Ç–æ–¥ 3: –ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ "X –ª–æ–∫–∞–ª–∏" –∏–ª–∏ "X –∫–æ–º–Ω–∞—Ç"
                    rooms_match = re.search(r'(\d+)\s*(?:local|stanz|camer|room)', title_lower)
                    if rooms_match:
                        rooms = int(rooms_match.group(1))
            
            if rooms_elem and not rooms:
                # –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                rooms_text = rooms_elem.get_text(strip=True)
                rooms_match = re.search(r'(\d+)', rooms_text)
                if rooms_match:
                    rooms = int(rooms_match.group(1))
            
            # –ê–¥—Ä–µ—Å/–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ
            address = "Roma, Italia"
            address_elem = container.find('span', class_='item-zone') or \
                          container.find('div', class_='location') or \
                          container.find('span', class_='zone')
            
            if address_elem:
                address_text = address_elem.get_text(strip=True)
                if address_text:
                    address = f"{address_text}, Roma, Italia"
            
            # –°—Å—ã–ª–∫–∞ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
            url = ""
            link_elem = container.find('a', href=True)
            if link_elem:
                href = link_elem.get('href')
                if href.startswith('/'):
                    url = urljoin(self.base_url, href)
                else:
                    url = href
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = []
            img_elem = container.find('img', src=True)
            if img_elem:
                img_src = img_elem.get('src')
                if img_src and not img_src.startswith('data:'):
                    if img_src.startswith('/'):
                        img_src = urljoin(self.base_url, img_src)
                    images.append(img_src)
            
            listing = {
                'external_id': f"idealista_{external_id}",
                'source': 'idealista',
                'title': title,
                'price': price,
                'property_type': 'apartment',  # –î–æ–±–∞–≤–ª—è–µ–º property_type –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                'area': area,
                'rooms': rooms,
                'address': address,
                'city': 'Roma',
                'url': url,
                'images': images,
                'scraped_at': datetime.utcnow()
            }
            
            return listing
            
        except Exception as e:
            logger.error(f"‚ùå [{self.name}] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
            return None
    
    async def geocode_listing(self, listing: Dict[str, Any]) -> Dict[str, Any]:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∫ –æ–±—ä—è–≤–ª–µ–Ω–∏—é"""
        if not self.enable_geocoding:
            return listing
        
        try:
            address = listing.get('address', '')
            if not address or address == "Roma, Italia":
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ü–µ–Ω—Ç—Ä–∞ –†–∏–º–∞
                listing['latitude'] = 41.9028
                listing['longitude'] = 12.4964
                return listing
            
            # –ì–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Nominatim
            geocode_url = "https://nominatim.openstreetmap.org/search"
            params = {
                'q': address,
                'format': 'json',
                'limit': 1,
                'countrycodes': 'it'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(geocode_url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        if data:
                            listing['latitude'] = float(data[0]['lat'])
                            listing['longitude'] = float(data[0]['lon'])
                        else:
                            # Fallback –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –†–∏–º–∞
                            listing['latitude'] = 41.9028
                            listing['longitude'] = 12.4964
                    else:
                        listing['latitude'] = 41.9028
                        listing['longitude'] = 12.4964
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —É–≤–∞–∂–µ–Ω–∏—è –∫ API
            await asyncio.sleep(0.1)
            
        except Exception as e:
            logger.error(f"‚ùå [{self.name}] –û—à–∏–±–∫–∞ –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            # Fallback –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –†–∏–º–∞
            listing['latitude'] = 41.9028
            listing['longitude'] = 12.4964
        
        return listing
    
    async def scrape_single_page(self, page_num: int) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        url = self.build_page_url(page_num)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º async API
        job_info = await self.submit_async_job(url, page_num)
        html_content = None
        
        if job_info:
            html_content = await self.poll_job_result(job_info)
        
        # Fallback –Ω–∞ sync API
        if not html_content:
            logger.info(f"üîÑ [{self.name}] –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ fallback –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
            html_content = await self.fallback_sync_request(url, page_num)
        
        if not html_content:
            logger.error(f"‚ùå [{self.name}] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
            return []
        
        # –ü–∞—Ä—Å–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        listings = self.parse_listings_from_html(html_content, page_num)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if self.enable_geocoding and listings:
            geocoded_listings = []
            for listing in listings:
                geocoded_listing = await self.geocode_listing(listing)
                geocoded_listings.append(geocoded_listing)
            return geocoded_listings
        
        return listings
    
    async def scrape_multiple_pages(self, max_pages: int = 10) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        logger.info(f"üöÄ [{self.name}] –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
        
        # –û—á–∏—â–∞–µ–º –∫–µ—à –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        self.seen_listing_ids.clear()
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        tasks = []
        for page_num in range(1, max_pages + 1):
            task = self.scrape_single_page(page_num)
            tasks.append(task)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        all_listings = []
        successful_pages = 0
        
        for page_num, result in enumerate(results, 1):
            if isinstance(result, Exception):
                logger.error(f"‚ùå [{self.name}] –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}: {result}")
            elif isinstance(result, list):
                all_listings.extend(result)
                successful_pages += 1
                logger.info(f"‚úÖ [{self.name}] –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: {len(result)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        
        logger.info(f"üéâ [{self.name}] –ó–∞–≤–µ—Ä—à–µ–Ω–æ! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {successful_pages}/{max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü, –ø–æ–ª—É—á–µ–Ω–æ {len(all_listings)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        return all_listings


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
async def test_idealista_scraper():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Å–∫—Ä–∞–ø–µ—Ä Idealista"""
    scraper = IdealistaScraper(enable_geocoding=False)
    listings = await scraper.scrape_multiple_pages(max_pages=2)
    
    print(f"\nüéâ –ü–æ–ª—É—á–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å Idealista.it")
    
    if listings:
        print("\nüìã –ü–µ—Ä–≤—ã–µ 3 –æ–±—ä—è–≤–ª–µ–Ω–∏—è:")
        for i, listing in enumerate(listings[:3], 1):
            print(f"\n{i}. {listing.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")
            print(f"   üí∞ –¶–µ–Ω–∞: {listing.get('price', 0)}‚Ç¨")
            print(f"   üìê –ü–ª–æ—â–∞–¥—å: {listing.get('area', 'N/A')} –º¬≤")
            print(f"   üö™ –ö–æ–º–Ω–∞—Ç—ã: {listing.get('rooms', 'N/A')}")
            print(f"   üìç –ê–¥—Ä–µ—Å: {listing.get('address', 'N/A')}")
            print(f"   üîó URL: {listing.get('url', 'N/A')}")


if __name__ == "__main__":
    # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
    asyncio.run(test_idealista_scraper()) 