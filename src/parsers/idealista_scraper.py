#!/usr/bin/env python3
"""
üöÄ –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–ê–†–°–ï–† IDEALISTA
–°–∫–æ—Ä–æ—Å—Ç—å: –≤ 5-10 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
"""
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from src.core.config import settings
import json
import re
from datetime import datetime
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin

class IdealistaScraper:
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(self, max_concurrent: int = 10, enable_geocoding: bool = False):
        self.base_url = "https://www.idealista.it"
        self.api_url = "https://api.scraperapi.com/"
        self.api_key = settings.SCRAPERAPI_KEY
        self.max_concurrent = max_concurrent  # –ú–∞–∫—Å–∏–º—É–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.enable_geocoding = enable_geocoding  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'success': 0,
            'failed': 0,
            'coords_from_html': 0,
            'coords_from_geocoding': 0,
            'coords_not_found': 0
        }
    
    def extract_area_from_features(self, features: List[str]) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–ª–æ—â–∞–¥—å –∏–∑ features"""
        for feat in features:
            match = re.search(r'(\d+)\s*m[¬≤q]', feat, re.IGNORECASE)
            if match:
                return int(match.group(1))
        return None
    
    def extract_rooms_from_features(self, features: List[str]) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç –∏–∑ features"""
        for feat in features:
            match = re.search(r'(\d+)\s*(?:local|stanz)', feat, re.IGNORECASE)
            if match:
                return int(match.group(1))
        return None
    
    def extract_bathrooms_from_features(self, features: List[str]) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–Ω–Ω—ã—Ö –∏–∑ features"""
        for feat in features:
            match = re.search(r'(\d+)\s*bagn', feat, re.IGNORECASE)
            if match:
                return int(match.group(1))
        return None
    
    def extract_floor_from_features(self, features: List[str]) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —ç—Ç–∞–∂ –∏–∑ features"""
        for feat in features:
            if 'piano' in feat.lower():
                match = re.search(r'(\d+)[¬∫¬∞]?\s*piano', feat, re.IGNORECASE)
                if match:
                    return match.group(1)
                if 'terra' in feat.lower():
                    return '0'
                return feat.strip()
        return None

    async def fetch_html(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —á–µ—Ä–µ–∑ ScraperAPI —Å —Å–µ–º–∞—Ñ–æ—Ä–æ–º –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞"""
        async with self.semaphore:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            params = {
                'api_key': self.api_key,
                'url': url,
                'render': 'true',
                'ultra_premium': 'true'
            }
            
            try:
                async with session.get(self.api_url, params=params, timeout=aiohttp.ClientTimeout(total=90)) as response:
                    if response.status == 200:
                        return await response.text()
                    else:
                        return None
            except Exception as e:
                return None
    
    def parse_listing_card(self, container) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –∫–∞—Ä—Ç–æ—á–∫—É –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ —Å–ø–∏—Å–∫–∞"""
        try:
            external_id = container.get('data-adid') or container.get('data-element-id')
            if not external_id:
                return None
            
            title_elem = container.find('a', class_='item-link')
            if not title_elem or not title_elem.get('href'):
                return None
            
            detail_url = self.base_url + title_elem['href']
            
            return {
                'external_id': f"idealista_{external_id}",
                'url': detail_url
            }
        except Exception as e:
            return None
    
    def parse_detail_page(self, html: str, url: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            data = {'url': url, 'scraped_at': datetime.utcnow().isoformat()}
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = soup.find('h1', class_='main-info__title-main')
            if title_elem:
                data['title'] = title_elem.get_text(strip=True)
            
            # –¶–µ–Ω–∞
            price_elem = soup.find('span', class_='info-data-price')
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                numbers = re.findall(r'\d+', price_text.replace('.', ''))
                if numbers:
                    data['price'] = int(''.join(numbers))
            
            # –ê–¥—Ä–µ—Å
            address_elem = soup.find('span', class_='main-info__title-minor')
            if address_elem:
                data['address'] = address_elem.get_text(strip=True)
            
            # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑ Google Maps Static API
            all_text = str(soup)
            maps_match = re.search(r'maps\.googleapis\.com/maps/api/staticmap[^"\'<>]*center=([\d.]+)%2C([\d.]+)', all_text)
            if maps_match:
                data['latitude'] = float(maps_match.group(1))
                data['longitude'] = float(maps_match.group(2))
            
            # –û–ø–∏—Å–∞–Ω–∏–µ
            description_elem = soup.find('div', class_='comment')
            if description_elem:
                for button in description_elem.find_all('button'):
                    button.decompose()
                data['description'] = description_elem.get_text(strip=True)
            
            # –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
            features = []
            details_section = soup.find('div', class_='details-property')
            if details_section:
                feature_items = details_section.find_all('li')
                for item in feature_items:
                    feature_text = item.get_text(strip=True)
                    if feature_text:
                        features.append(feature_text)
            
            data['features'] = features
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥)
            images = []
            
            # –í–∞—Ä–∏–∞–Ω—Ç 1: detail-image
            for img in soup.find_all('img'):
                img_url = img.get('src') or img.get('data-src') or img.get('data-ondemand-img')
                if img_url and img_url.startswith('http'):
                    # –ò—Å–∫–ª—é—á–∞–µ–º –∫–∞—Ä—Ç—ã –∏ –∏–∫–æ–Ω–∫–∏
                    if ('maps.googleapis.com' not in img_url and 
                        'idealista.it' in img_url and
                        any(ext in img_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp'])):
                        if img_url not in images:
                            images.append(img_url)
            
            # –í–∞—Ä–∏–∞–Ω—Ç 2: –ü–æ–∏—Å–∫ –≤ JavaScript —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–∫–∏
            all_text = str(soup)
            js_images = re.findall(r'https://img\d*\.idealista\.it[^\s"\'<>]+\.(?:jpg|jpeg|png|webp)', all_text)
            for img_url in js_images:
                if img_url not in images:
                    images.append(img_url)
            
            data['images'] = images[:20]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 20 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            
            # –ö–æ–Ω—Ç–∞–∫—Ç
            contact_elem = soup.find('p', class_='advertiser-name')
            if contact_elem:
                data['contact_name'] = contact_elem.get_text(strip=True)
            
            # ID –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            id_match = re.search(r'/immobile/(\d+)/', url)
            if id_match:
                data['external_id'] = f"idealista_{id_match.group(1)}"
            
            data['source'] = 'idealista'
            data['city'] = 'Roma'
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏–∑ features
            if features:
                area = self.extract_area_from_features(features)
                if area:
                    data['area_sqm'] = area
                
                rooms = self.extract_rooms_from_features(features)
                if rooms:
                    data['rooms'] = rooms
                
                bathrooms = self.extract_bathrooms_from_features(features)
                if bathrooms:
                    data['bathrooms'] = bathrooms
                
                floor = self.extract_floor_from_features(features)
                if floor:
                    data['floor'] = floor
            
            return data
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return None
    
    async def scrape_single_listing(self, session: aiohttp.ClientSession, url: str, index: int, total: int) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ"""
        print(f"[{index}/{total}] –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º: {url}")
        
        html = await self.fetch_html(session, url)
        
        if not html:
            self.stats['failed'] += 1
            print(f"    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML")
            return None
        
        listing_data = self.parse_detail_page(html, url)
        
        if listing_data:
            self.stats['success'] += 1
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
            if listing_data.get('latitude'):
                self.stats['coords_from_html'] += 1
            
            # –ö–æ—Ä–æ—Ç–∫–∏–π –≤—ã–≤–æ–¥
            coords_status = "üåç" if listing_data.get('latitude') else "‚ùå"
            images_count = len(listing_data.get('images', []))
            print(f"    ‚úÖ {listing_data.get('price', 0)}‚Ç¨ | {listing_data.get('address', 'N/A')[:30]} | {coords_status} | üñºÔ∏è  {images_count}")
        else:
            self.stats['failed'] += 1
            print(f"    ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å")
        
        return listing_data
    
    async def scrape_list_page(self, session: aiohttp.ClientSession, page_num: int) -> List[str]:
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å–ø–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
        if page_num == 1:
            list_url = f"{self.base_url}/affitto-case/roma-roma/?ordine=pubblicazione-desc"
        else:
            list_url = f"{self.base_url}/affitto-case/roma-roma/lista-{page_num}.htm?ordine=pubblicazione-desc"
        
        print(f"\nüîÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: {list_url}")
        
        html = await self.fetch_html(session, list_url)
        
        if not html:
            print(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML")
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        containers = soup.find_all('article', class_='item')
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(containers)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        
        urls = []
        for container in containers:
            listing_data = self.parse_listing_card(container)
            if listing_data:
                urls.append(listing_data['url'])
        
        return urls
    
    async def scrape_multiple_pages(self, max_pages: int = 5):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º (—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å)"""
        return await self.scrape_parallel(num_pages=max_pages)
    
    async def scrape_parallel(self, num_pages: int = 2):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º"""
        print("=" * 80)
        print(f"üöÄ –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì IDEALISTA (–¥–æ {self.max_concurrent} –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)")
        print("=" * 80)
        print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü —Å–ø–∏—Å–∫–∞: {num_pages}")
        print("=" * 80)
        
        start_time = datetime.utcnow()
        
        async with aiohttp.ClientSession() as session:
            # –≠–¢–ê–ü 1: –°–æ–±–∏—Ä–∞–µ–º URL —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü —Å–ø–∏—Å–∫–æ–≤
            print("\nüìã –≠–¢–ê–ü 1: –°–±–æ—Ä URL –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            print("-" * 80)
            
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –ø–∞—Ä—Å–∏–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–∏—Å–∫–æ–≤
            list_tasks = [
                self.scrape_list_page(session, page_num)
                for page_num in range(1, num_pages + 1)
            ]
            
            list_results = await asyncio.gather(*list_tasks)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ URL
            all_urls = []
            for urls in list_results:
                all_urls.extend(urls)
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            all_urls = list(dict.fromkeys(all_urls))
            
            print(f"\n{'=' * 80}")
            print(f"üìä –ò–¢–û–ì–û —Å–æ–±—Ä–∞–Ω–æ {len(all_urls)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL")
            print(f"{'=' * 80}")
            
            # –≠–¢–ê–ü 2: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –ø–∞—Ä—Å–∏–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            print("\nüîç –≠–¢–ê–ü 2: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")
            print("-" * 80)
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö URL
            detail_tasks = [
                self.scrape_single_listing(session, url, i+1, len(all_urls))
                for i, url in enumerate(all_urls)
            ]
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            results = await asyncio.gather(*detail_tasks)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º None
            all_listings = [r for r in results if r is not None]
        
        end_time = datetime.utcnow()
        elapsed = (end_time - start_time).total_seconds()
        
        # –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê
        print("\n" + "=" * 80)
        print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("=" * 80)
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {self.stats['success']}")
        print(f"‚ùå –û—à–∏–±–∫–∏: {self.stats['failed']}")
        print(f"üåç –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑ HTML: {self.stats['coords_from_html']}/{self.stats['success']} ({self.stats['coords_from_html']/max(self.stats['success'],1)*100:.1f}%)")
        print()
        print(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {elapsed:.1f} —Å–µ–∫—É–Ω–¥")
        print(f"‚ö° –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: {elapsed/len(all_urls):.1f} —Å–µ–∫")
        print(f"üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {len(all_urls)/elapsed*60:.1f} –æ–±—ä—è–≤–ª–µ–Ω–∏–π/–º–∏–Ω—É—Ç—É")
        
        return all_listings


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description='–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä Idealista')
    parser.add_argument('--pages', type=int, default=2, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2)')
    parser.add_argument('--concurrent', type=int, default=10, help='–û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)')
    
    args = parser.parse_args()
    
    scraper = IdealistaParallelScraper(max_concurrent=args.concurrent)
    
    # –ü–∞—Ä—Å–∏–º
    listings = await scraper.scrape_parallel(num_pages=args.pages)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    if listings:
        output_file = f'/tmp/idealista_parallel_results.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(listings, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª–µ–π
        print("\nüìã –ó–ê–ü–û–õ–ù–ï–ù–ù–û–°–¢–¨ –ü–û–õ–ï–ô:")
        fields_count = {}
        for listing in listings:
            for key, value in listing.items():
                if key not in fields_count:
                    fields_count[key] = 0
                if value and value != [] and value != 'N/A':
                    fields_count[key] += 1
        
        for key in sorted(fields_count.keys()):
            count = fields_count[key]
            percentage = (count / len(listings)) * 100
            print(f"   ‚Ä¢ {key:20s}: {count:3d}/{len(listings)} ({percentage:5.1f}%)")


if __name__ == "__main__":
    asyncio.run(main())

