#!/usr/bin/env python3
"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Immobiliare.it —á–µ—Ä–µ–∑ ScraperAPI
–ü–∞—Ä—Å–∏—Ç —Ç–æ–ª—å–∫–æ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∞—Ä–µ–Ω–¥—ã –≤ –†–∏–º–µ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤
"""
import asyncio
import aiohttp
import logging
import json
import re
import time
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from .base_parser import BaseParser
from ..core.config import settings

logger = logging.getLogger(__name__)


class ImmobiliareParser(BaseParser):
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Immobiliare.it
    –ü–∞—Ä—Å–∏—Ç —Ç–æ–ª—å–∫–æ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∞—Ä–µ–Ω–¥—ã –≤ –†–∏–º–µ
    """
    
    def __init__(self, enable_geocoding: bool = True):
        super().__init__(
            name="Immobiliare.it",
            base_url="https://www.immobiliare.it"
        )
        # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π URL –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∞—Ä–µ–Ω–¥—ã –≤ –†–∏–º–µ
        self.main_page_url = "https://www.immobiliare.it/affitto-case/roma/"
        # –§–ª–∞–≥ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è/–≤—ã–∫–ª—é—á–µ–Ω–∏—è –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.enable_geocoding = enable_geocoding
    
    def build_search_url(self, filters: Dict[str, Any] = None, page: int = 1) -> str:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã, –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        """
        if page > 1:
            return f"{self.main_page_url}?pag={page}"
        return self.main_page_url
    
    async def get_html_with_scraperapi(self, url: str, retries: int = 3) -> Optional[str]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç HTML —á–µ—Ä–µ–∑ ScraperAPI —Å JS —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–æ–º
        """
        if not settings.SCRAPERAPI_KEY:
            logger.error("‚ùå SCRAPERAPI_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return None
        
        params = {
            'api_key': settings.SCRAPERAPI_KEY,
            'url': url,
            'render': 'true',
            'premium': 'true',
            'country_code': 'eu'
        }
        
        timeout = aiohttp.ClientTimeout(total=180)
        
        for attempt in range(retries):
            try:
                logger.info(f"üì° ScraperAPI –∑–∞–ø—Ä–æ—Å (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {url}")
                
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get('https://api.scraperapi.com/', params=params) as response:
                        
                        if response.status >= 500:
                            logger.warning(f"‚ö†Ô∏è ScraperAPI –æ—à–∏–±–∫–∞ {response.status}. –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries}")
                            await asyncio.sleep(5 * (attempt + 1))
                            continue
                        
                        response.raise_for_status()
                        html_content = await response.text()
                        logger.info(f"‚úÖ HTML –ø–æ–ª—É—á–µ–Ω, —Ä–∞–∑–º–µ—Ä: {len(html_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                        return html_content
                        
            except aiohttp.ClientError as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ ScraperAPI: {e}")
                if attempt < retries - 1:
                    await asyncio.sleep(5 * (attempt + 1))
                    
        return None
    
    def extract_next_data_json(self, html_content: str) -> Optional[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –∏–∑ __NEXT_DATA__ —Ç–µ–≥–∞
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            script_tag = soup.find('script', id='__NEXT_DATA__')
            if not script_tag:
                logger.warning("‚ùå –¢–µ–≥ __NEXT_DATA__ –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return None
            return json.loads(script_tag.string)
        except (json.JSONDecodeError, TypeError) as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            return None
    
    def extract_all_photos(self, listing_json: Dict[str, Any]) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –í–°–ï —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏–∑ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        """
        photos = []
        try:
            estate = listing_json.get('realEstate', {})
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            multimedia = properties.get('multimedia', {})
            photo_list = multimedia.get('photos', [])
            
            for photo in photo_list:
                if isinstance(photo, dict) and 'urls' in photo:
                    urls = photo['urls']
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã —Ñ–æ—Ç–æ
                    for size in ['large', 'medium', 'small']:
                        if size in urls and urls[size]:
                            photos.append(urls[size])
                            break  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
            
            logger.info(f"üì∏ –ù–∞–π–¥–µ–Ω–æ {len(photos)} —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –¥–ª—è –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
            return photos
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–æ—Ç–æ: {e}")
            return []
    
    def parse_single_listing(self, listing_json: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç –æ–¥–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –∏–∑ JSON
        """
        try:
            estate = listing_json.get('realEstate', {})
            if not estate:
                return None
            
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            title = properties.get('caption')
            canonical_url = listing_json.get('seo', {}).get('url')
            
            if not title or not canonical_url:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: –Ω–µ—Ç URL –∏–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL
            external_id = None
            if canonical_url:
                match = re.search(r'/annunci/(\d+)/', canonical_url)
                if match:
                    external_id = match.group(1)
            
            # –¶–µ–Ω–∞
            price_info = estate.get('price', {})
            price = price_info.get('value')
            
            # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            rooms = None
            area = None
            bathrooms = None
            
            rooms_raw = properties.get('rooms', '')
            if rooms_raw:
                match = re.search(r'\d+', str(rooms_raw))
                if match:
                    rooms = int(match.group(0))
            
            area_raw = properties.get('surface', '')
            if area_raw:
                match = re.search(r'\d+', str(area_raw))
                if match:
                    area = int(match.group(0))
            
            bathrooms_raw = properties.get('bathrooms', '')
            if bathrooms_raw:
                match = re.search(r'\d+', str(bathrooms_raw))
                if match:
                    bathrooms = int(match.group(0))
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –í–°–ï —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            all_photos = self.extract_all_photos(listing_json)
            
            # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏ –∞–¥—Ä–µ—Å (–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫)
            if self.enable_geocoding:
                latitude, longitude = self._extract_coordinates(listing_json)
                address = self._extract_address(listing_json)
            else:
                latitude, longitude = None, None
                address = None
            
            # –¢–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
            property_type = self._normalize_property_type(properties, title)
            
            return {
                'external_id': external_id,
                'source': 'immobiliare',
                'url': canonical_url,
                'title': title,
                'description': properties.get('description', ''),
                'price': price,
                'price_currency': 'EUR',
                'property_type': property_type,
                'rooms': rooms,
                'bathrooms': bathrooms,
                'area': area,
                'floor': str(properties.get('floor', '')),
                'furnished': None,
                'pets_allowed': None,
                'features': None,
                'address': address,
                'city': 'Roma',
                'district': None,
                'postal_code': None,
                'latitude': latitude,
                'longitude': longitude,
                'images': all_photos,  # –í–°–ï —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ —Å–ø–∏—Å–∫–æ–º
                'virtual_tour_url': None,
                'agency_name': estate.get('advertiser', {}).get('agency', {}).get('displayName'),
                'contact_info': None,
                'is_active': True,
                'published_at': None,
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
            return None
    
    async def scrape_single_page(self, page_num: int) -> List[Dict[str, Any]]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–∞—Ä—Å–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É
        
        Args:
            page_num: –ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        """
        try:
            # –°—Ç—Ä–æ–∏–º URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_url = self.build_search_url(page=page_num)
            logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num}: {page_url}")
            
            # –ü–æ–ª—É—á–∞–µ–º HTML —á–µ—Ä–µ–∑ ScraperAPI
            html_content = await self.get_html_with_scraperapi(page_url)
            if not html_content:
                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                return []
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –¥–∞–Ω–Ω—ã–µ
            json_data = self.extract_next_data_json(html_content)
            if not json_data:
                logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω JSON –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
                return []
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ JSON
            try:
                results = json_data['props']['pageProps']['dehydratedState']['queries'][0]['state']['data']['results']
            except (KeyError, IndexError, TypeError):
                logger.info(f"üîö –ë–æ–ª—å—à–µ –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
                return []
            
            if not results:
                logger.info(f"üîö –ü—É—Å—Ç–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}")
                return []
            
            logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(results)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
            
            # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
            page_listings = []
            for listing_json in results:
                parsed_listing = self.parse_single_listing(listing_json)
                if parsed_listing:
                    page_listings.append(parsed_listing)
            
            logger.info(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(page_listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            return page_listings
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}: {e}")
            return []

    async def scrape_all_listings(self, max_pages: int = 10) -> List[Dict[str, Any]]:
        """
        –û–°–ù–û–í–ù–û–ô –ú–ï–¢–û–î: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–∞—Ä—Å–∏—Ç –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        Args:
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        """
        logger.info("üöÄ –ù–ê–ß–ò–ù–ê–ï–ú –ê–°–ò–ù–•–†–û–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì IMMOBILIARE.IT")
        logger.info(f"üéØ URL: {self.main_page_url}")
        logger.info(f"üìÑ –ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü: {max_pages}")
        logger.info(f"‚ö° –†–µ–∂–∏–º: –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô (–≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        tasks = []
        for page_num in range(1, max_pages + 1):
            task = self.scrape_single_page(page_num)
            tasks.append(task)
        
        logger.info(f"üîÑ –ó–∞–ø—É—Å–∫–∞–µ–º {len(tasks)} –∑–∞–¥–∞—á –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ...")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        all_listings = []
        successful_pages = 0
        error_pages = 0
        
        for page_num, result in enumerate(results, 1):
            if isinstance(result, Exception):
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}: {result}")
                error_pages += 1
            elif isinstance(result, list):
                all_listings.extend(result)
                if result:  # –ï—Å–ª–∏ –µ—Å—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏—è
                    successful_pages += 1
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {type(result)}")
        
        logger.info(f"üéâ –ê–°–ò–ù–•–†–û–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!")
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        logger.info(f"   ‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {successful_pages}")
        logger.info(f"   ‚ùå –û—à–∏–±–æ–∫: {error_pages}")
        logger.info(f"   üìã –í—Å–µ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(all_listings)}")
        
        return all_listings
    
    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def _extract_coordinates(self, listing_json: Dict[str, Any]) -> tuple[Optional[float], Optional[float]]:
        """–ò–∑–≤–ª–µ—á—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑ JSON"""
        try:
            estate = listing_json.get('realEstate', {})
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –º–µ—Å—Ç–∞ –≤ JSON
            location_sources = [
                properties.get('location', {}),
                estate.get('location', {}),
                properties.get('coordinates', {}),
                estate.get('coordinates', {})
            ]
            
            for location in location_sources:
                lat = location.get('latitude') or location.get('lat')
                lon = location.get('longitude') or location.get('lng')
                
                if lat and lon:
                    lat, lon = float(lat), float(lon)
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –ò—Ç–∞–ª–∏–∏
                    if 35.0 <= lat <= 47.0 and 6.0 <= lon <= 19.0:
                        return lat, lon
            
            return None, None
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç: {e}")
            return None, None
    
    def _extract_address(self, listing_json: Dict[str, Any]) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á—å –∞–¥—Ä–µ—Å –∏–∑ JSON"""
        try:
            estate = listing_json.get('realEstate', {})
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–ª—è —Å –∞–¥—Ä–µ—Å–æ–º
            address_fields = [
                properties.get('address'),
                properties.get('location', {}).get('address'),
                properties.get('street'),
                estate.get('address')
            ]
            
            for addr in address_fields:
                if addr and isinstance(addr, str) and len(addr.strip()) > 5:
                    return addr.strip()
            
            return None
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞–¥—Ä–µ—Å–∞: {e}")
            return None
    
    def _normalize_property_type(self, properties: Dict[str, Any], title: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø –∏–∑ JSON
        property_type = None
        if properties.get('typology', {}).get('name'):
            property_type = properties['typology']['name']
        
        # –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤
        type_mapping = {
            'Appartamento': 'apartment',
            'Villa': 'house',
            'Casa': 'house',
            'Attico': 'apartment',
            'Loft': 'apartment',
            'Monolocale': 'studio',
            'Bilocale': 'apartment',
            'Trilocale': 'apartment',
            'Quadrilocale': 'apartment'
        }
        
        if property_type and property_type in type_mapping:
            return type_mapping[property_type]
        
        # –ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        if 'monolocale' in title.lower():
            return 'studio'
        elif any(word in title.lower() for word in ['villa', 'casa']):
            return 'house'
        
        return 'apartment'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ BaseParser
    def parse_listings_from_page(self, html_content: str) -> List[Dict[str, Any]]:
        """–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞"""
        json_data = self.extract_next_data_json(html_content)
        if not json_data:
            return []
        
        try:
            results = json_data['props']['pageProps']['dehydratedState']['queries'][0]['state']['data']['results']
        except (KeyError, IndexError, TypeError):
            return []
        
        listings = []
        for listing_json in results:
            parsed = self.parse_single_listing(listing_json)
            if parsed:
                listings.append(parsed)
        
        return listings
    
    def normalize_listing_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞"""
        return raw_data
    
    # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
    async def scrape_listings(self, filters: Dict[str, Any] = None, max_pages: int = 10) -> List[Dict[str, Any]]:
        """–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º API"""
        return await self.scrape_all_listings(max_pages=max_pages) 