#!/usr/bin/env python3
"""
üöÄ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ê–°–ò–ù–•–†–û–ù–ù–´–ô –ü–ê–†–°–ï–† –î–õ–Ø IMMOBILIARE.IT
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞—Å—Ç–æ—è—â—É—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –ª–æ–≥–∏–∫—É ScraperAPI —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏

–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:
- ‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ HTTP
- ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
- ‚úÖ –õ—É—á—à–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ JSON –æ—Ç–≤–µ—Ç–æ–≤
- ‚úÖ Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π ScraperAPI –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å Async API
- ‚úÖ –ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–ø—Ä–æ—Å–∞ —Å—Ç–∞—Ç—É—Å–∞
"""
import asyncio
import aiohttp
import logging
import json
import re
import time
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from .base_parser import BaseParser
from ..core.config import settings

logger = logging.getLogger(__name__)


class ImmobiliareAsyncParser(BaseParser):
    """
    –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Immobiliare.it
    """
    
    def __init__(self, enable_geocoding: bool = True):
        super().__init__(
            name="Immobiliare.it (Async Fixed)",
            base_url="https://www.immobiliare.it"
        )
        self.main_page_url = "https://www.immobiliare.it/affitto-case/roma/"
        self.enable_geocoding = enable_geocoding
        self.async_api_url = "https://async.scraperapi.com/jobs"
        self.sync_api_url = "https://api.scraperapi.com"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–∞–π–º–∞—É—Ç–æ–≤
        self.job_timeout = 30  # –¢–∞–π–º–∞—É—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á–∏
        self.poll_timeout = 300  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        self.request_timeout = 90  # –¢–∞–π–º–∞—É—Ç –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    
    def build_search_url(self, filters: Dict[str, Any] = None, page: int = 1) -> str:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        if page > 1:
            return f"{self.main_page_url}?pag={page}"
        return self.main_page_url
    
    async def scrape_page_fallback(self, page_num: int) -> List[Dict[str, Any]]:
        """
        –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π fallback –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ –æ–±—ã—á–Ω—ã–π ScraperAPI
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        url = self.build_search_url(page=page_num)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        if page_num > 1:
            delay = min(1 + (page_num * 0.3), 3)  # –û—Ç 1.3 –¥–æ 3 —Å–µ–∫—É–Ω–¥
            logger.info(f"‚è≥ –ó–∞–¥–µ—Ä–∂–∫–∞ {delay:.1f}—Å –ø–µ—Ä–µ–¥ fallback –∑–∞–ø—Ä–æ—Å–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
            await asyncio.sleep(delay)
        
        timeout = aiohttp.ClientTimeout(total=self.request_timeout)
        
        # –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∏–∑ —É—Å–ø–µ—à–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞)
        working_params = [
            # –ü–æ–ø—ã—Ç–∫–∞ 1: –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—Ä–∞–±–æ—Ç–∞–µ—Ç!)
            {
                'api_key': settings.SCRAPERAPI_KEY,
                'url': url,
                'render': 'false'  # –ö–ª—é—á–µ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ - –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º JS —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥
            },
            # –ü–æ–ø—ã—Ç–∫–∞ 2: –ú–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è (—Ä–∞–±–æ—Ç–∞–µ—Ç!)
            {
                'api_key': settings.SCRAPERAPI_KEY,
                'url': url,
                'render': 'true',
                'device_type': 'mobile'
            },
            # –ü–æ–ø—ã—Ç–∫–∞ 3: –ë–µ–∑ –≥–µ–æ—Ç–∞—Ä–≥–µ—Ç–∏–Ω–≥–∞ (—Ä–∞–±–æ—Ç–∞–µ—Ç!)
            {
                'api_key': settings.SCRAPERAPI_KEY,
                'url': url,
                'render': 'true',
                'premium': 'true'
            }
        ]
        
        for attempt_num, attempt_params in enumerate(working_params, 1):
            try:
                logger.info(f"üîÑ Fallback –ø–æ–ø—ã—Ç–∫–∞ {attempt_num}/3 –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(self.sync_api_url, params=attempt_params) as response:
                        if response.status == 200:
                            html_content = await response.text()
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –≤–∞–ª–∏–¥–Ω—ã–π HTML —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
                            if len(html_content) > 10000 and any(indicator in html_content.lower() for indicator in 
                                                                ['immobiliare', 'affitto', 'roma', 'annunci']):
                                logger.info(f"‚úÖ Fallback —É—Å–ø–µ—à–µ–Ω –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} (–ø–æ–ø—ã—Ç–∫–∞ {attempt_num}), —Ä–∞–∑–º–µ—Ä: {len(html_content)}")
                                return await self.parse_html_content(html_content, page_num)
                            else:
                                logger.warning(f"‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω HTML –±–µ–∑ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt_num}: {len(html_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                        else:
                            logger.warning(f"‚ö†Ô∏è Fallback HTTP {response.status} –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt_num} –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                            
                # –ö–æ—Ä–æ—Ç–∫–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
                if attempt_num < len(working_params):
                    await asyncio.sleep(1)
                        
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Fallback –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt_num} –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
                if attempt_num < len(working_params):
                    await asyncio.sleep(1)
        
        logger.error(f"‚ùå –í—Å–µ fallback –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
        return []
    
    async def submit_scraping_job(self, url: str, meta: Dict[str, Any] = None) -> Optional[Dict[str, Any]]:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–¥–∞—á—É —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –≤ ScraperAPI Async —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        """
        if not settings.SCRAPERAPI_KEY:
            logger.error("‚ùå SCRAPERAPI_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return None
        
        payload = {
            "apiKey": settings.SCRAPERAPI_KEY,
            "url": url,
            "apiParams": {
                "render": True,
                "premium": True,
                "country_code": "it",
                "device_type": "desktop",
                "autoparse": False,
                "retry_404": True,
                "follow_redirect": True
            }
        }
        
        if meta:
            payload["meta"] = meta
        
        timeout = aiohttp.ClientTimeout(total=self.job_timeout)
        
        try:
            logger.info(f"üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É –≤ ScraperAPI Async: {url}")
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    self.async_api_url, 
                    json=payload,
                    headers={'Content-Type': 'application/json'}
                ) as response:
                    
                    # –ß–∏—Ç–∞–µ–º –æ—Ç–≤–µ—Ç –∫–∞–∫ —Ç–µ–∫—Å—Ç —Å–Ω–∞—á–∞–ª–∞
                    response_text = await response.text()
                    
                    if response.status == 200:
                        try:
                            job_data = json.loads(response_text)
                            logger.info(f"‚úÖ –ó–∞–¥–∞—á–∞ —Å–æ–∑–¥–∞–Ω–∞: {job_data.get('id')} | –°—Ç–∞—Ç—É—Å: {job_data.get('status')}")
                            return job_data
                        except json.JSONDecodeError as e:
                            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç–≤–µ—Ç–∞: {e}")
                            logger.debug(f"–û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {response_text[:500]}")
                            return None
                    else:
                        logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ {response.status}: {response_text[:200]}")
                        return None
                    
        except asyncio.TimeoutError:
            logger.error(f"‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∑–∞–¥–∞—á–∏ –¥–ª—è {url}")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á–∏: {e}")
            return None
    
    async def poll_job_status(self, status_url: str, max_wait_time: int = None) -> Optional[Dict[str, Any]]:
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π –æ–ø—Ä–æ—Å —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–æ–π
        """
        if max_wait_time is None:
            max_wait_time = self.poll_timeout
            
        timeout = aiohttp.ClientTimeout(total=30)
        start_time = time.time()
        poll_interval = 3  # –ù–∞—á–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
        max_poll_interval = 20  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
        
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                while time.time() - start_time < max_wait_time:
                    elapsed = time.time() - start_time
                    logger.info(f"üîÑ –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å... ({elapsed:.1f}s)")
                    
                    try:
                        async with session.get(status_url) as response:
                            response_text = await response.text()
                            
                            if response.status != 200:
                                logger.warning(f"‚ö†Ô∏è HTTP {response.status} –ø—Ä–∏ –æ–ø—Ä–æ—Å–µ —Å—Ç–∞—Ç—É—Å–∞")
                                await asyncio.sleep(poll_interval)
                                continue
                            
                            # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
                            try:
                                job_status = json.loads(response_text)
                            except json.JSONDecodeError:
                                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON: {response_text[:100]}")
                                await asyncio.sleep(poll_interval)
                                continue
                            
                            status = job_status.get("status")
                            job_id = job_status.get("id", "unknown")
                            
                            if status == "finished":
                                logger.info(f"‚úÖ –ó–∞–¥–∞—á–∞ {job_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
                                return job_status
                            
                            elif status == "failed":
                                fail_reason = job_status.get("failReason", "unknown")
                                logger.error(f"‚ùå –ó–∞–¥–∞—á–∞ {job_id} –ø—Ä–æ–≤–∞–ª–∏–ª–∞—Å—å: {fail_reason}")
                                return None
                            
                            elif status in ["running", "pending"]:
                                logger.info(f"‚è≥ –ó–∞–¥–∞—á–∞ {job_id} –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è... –ñ–¥–µ–º {poll_interval}s")
                                await asyncio.sleep(poll_interval)
                                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª –æ–ø—Ä–æ—Å–∞ (—ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ)
                                poll_interval = min(poll_interval * 1.2, max_poll_interval)
                            
                            else:
                                logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å—Ç–∞—Ç—É—Å: {status}")
                                await asyncio.sleep(poll_interval)
                    
                    except asyncio.TimeoutError:
                        logger.warning("‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–ø—Ä–æ—Å–µ —Å—Ç–∞—Ç—É—Å–∞, –ø–æ–≤—Ç–æ—Ä—è–µ–º...")
                        await asyncio.sleep(poll_interval)
                        
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–æ—Å–µ —Å—Ç–∞—Ç—É—Å–∞: {e}")
                        await asyncio.sleep(poll_interval)
                
                logger.error(f"‚è∞ –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è ({max_wait_time}s)")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –æ–ø—Ä–æ—Å–∞ —Å—Ç–∞—Ç—É—Å–∞: {e}")
            return None
    
    async def scrape_page_async(self, page_num: int) -> List[Dict[str, Any]]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å–∫—Ä–∞–ø–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π API
        """
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è —á–µ—Ä–µ–∑ Async API
        url = self.build_search_url(page=page_num)
        meta = {"page_num": page_num, "timestamp": time.time()}
        
        # –®–∞–≥ 1: –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É
        job_data = await self.submit_scraping_job(url, meta)
        if not job_data:
            logger.warning(f"‚ö†Ô∏è Async API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
            return await self.scrape_page_fallback(page_num)
        
        # –®–∞–≥ 2: –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        status_url = job_data.get("statusUrl")
        if not status_url:
            logger.warning(f"‚ö†Ô∏è –ù–µ—Ç statusUrl –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
            return await self.scrape_page_fallback(page_num)
        
        result = await self.poll_job_status(status_url)
        if not result:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
            return await self.scrape_page_fallback(page_num)
        
        # –®–∞–≥ 3: –ò–∑–≤–ª–µ–∫–∞–µ–º HTML –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        try:
            response_data = result.get("response", {})
            html_content = response_data.get("body")
            status_code = response_data.get("statusCode", 0)
            
            if not html_content:
                logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π HTML –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                return await self.scrape_page_fallback(page_num)
            
            # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ 403 –∏–ª–∏ –¥—Ä—É–≥—É—é –æ—à–∏–±–∫—É, —Å—Ä–∞–∑—É –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
            if status_code == 403:
                logger.warning(f"‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω 403 (Forbidden) –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                return await self.scrape_page_fallback(page_num)
            elif status_code != 200:
                logger.warning(f"‚ö†Ô∏è –°—Ç–∞—Ç—É—Å –∫–æ–¥ {status_code} –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                return await self.scrape_page_fallback(page_num)
            
            logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω HTML –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}, —Ä–∞–∑–º–µ—Ä: {len(html_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –®–∞–≥ 4: –ü–∞—Ä—Å–∏–º HTML
            parsed_listings = await self.parse_html_content(html_content, page_num)
            
            # –ï—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º fallback
            if not parsed_listings:
                logger.warning(f"‚ö†Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}, –ø—Ä–æ–±—É–µ–º fallback")
                return await self.scrape_page_fallback(page_num)
            
            return parsed_listings
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è HTML –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
            return await self.scrape_page_fallback(page_num)
    
    async def parse_html_content(self, html_content: str, page_num: int) -> List[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç HTML –∫–æ–Ω—Ç–µ–Ω—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        """
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –¥–∞–Ω–Ω—ã–µ
            json_data = self.extract_next_data_json(html_content)
            if not json_data:
                logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω JSON –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
                return []
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ JSON
            try:
                results = json_data['props']['pageProps']['dehydratedState']['queries'][0]['state']['data']['results']
            except (KeyError, IndexError, TypeError):
                logger.info(f"üîö –ë–æ–ª—å—à–µ –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
                return []
            
            if not results:
                logger.info(f"üîö –ü—É—Å—Ç–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}")
                return []
            
            logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(results)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
            
            # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            page_listings = []
            for listing_json in results:
                parsed_listing = await self.parse_single_listing_async(listing_json)
                if parsed_listing:
                    page_listings.append(parsed_listing)
            
            logger.info(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(page_listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            return page_listings
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
            return []
    
    def extract_next_data_json(self, html_content: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –∏–∑ __NEXT_DATA__ —Ç–µ–≥–∞"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            script_tag = soup.find('script', id='__NEXT_DATA__')
            if not script_tag:
                logger.warning("‚ùå –¢–µ–≥ __NEXT_DATA__ –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return None
            return json.loads(script_tag.string)
        except (json.JSONDecodeError, TypeError) as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            return None
    
    def extract_all_photos(self, listing_json: Dict[str, Any]) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –í–°–ï —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏–∑ –æ–±—ä—è–≤–ª–µ–Ω–∏—è (—É–Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–æ –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞)
        –¶–µ–ª—å: –ø–æ–ª—É—á–∏—Ç—å ~20+ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
        """
        photos = []
        try:
            estate = listing_json.get('realEstate', {})
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            multimedia = properties.get('multimedia', {})
            photo_list = multimedia.get('photos', [])
            
            for photo in photo_list:
                if isinstance(photo, dict) and 'urls' in photo:
                    urls = photo['urls']
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã —Ñ–æ—Ç–æ (–ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–≤—ã–º)
                    for size in ['large', 'medium', 'small']:
                        if size in urls and urls[size]:
                            photo_url = urls[size]
                            if photo_url and photo_url not in photos:  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                                photos.append(photo_url)
                            break  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–æ—Ç–æ
            
            logger.info(f"üì∏ –ù–∞–π–¥–µ–Ω–æ {len(photos)} —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –¥–ª—è –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
            return photos
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–æ—Ç–æ: {e}")
            return []
    
    async def parse_single_listing_async(self, listing_json: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–∞—Ä—Å–∏—Ç –æ–¥–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –∏–∑ JSON —Å –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        try:
            estate = listing_json.get('realEstate', {})
            if not estate:
                return None
            
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            title = properties.get('caption')
            canonical_url = listing_json.get('seo', {}).get('url')
            
            if not title or not canonical_url:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL
            external_id = None
            if canonical_url:
                match = re.search(r'/annunci/(\d+)/', canonical_url)
                if match:
                    external_id = match.group(1)
            
            # –¶–µ–Ω–∞
            price_info = estate.get('price', {})
            price = price_info.get('value')
            
            # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            rooms = self._extract_number(properties.get('rooms', ''))
            area = self._extract_number(properties.get('surface', ''))
            bathrooms = self._extract_number(properties.get('bathrooms', ''))
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –í–°–ï —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ (—É–Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–æ –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞)
            all_photos = self.extract_all_photos(listing_json)
            
            # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏ –∞–¥—Ä–µ—Å —Å –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            address = self._extract_address(listing_json)
            if self.enable_geocoding:
                latitude, longitude = await self._extract_coordinates_with_geocoding(listing_json)
            else:
                latitude, longitude = self._extract_coordinates(listing_json)
            
            # –¢–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ (—É–Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞)
            property_type = self._normalize_property_type(properties, title)
            
            return {
                'external_id': external_id,
                'source': 'immobiliare',
                'url': canonical_url,
                'title': title,
                'description': properties.get('description', ''),
                'price': price,
                'price_currency': 'EUR',
                'property_type': property_type,
                'rooms': rooms,
                'bathrooms': bathrooms,
                'area': area,
                'floor': str(properties.get('floor', '')),
                'furnished': self._is_furnished(properties, title),
                'pets_allowed': None,  # –ù–µ –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω–æ –≤ JSON
                'features': self._extract_features(properties),
                'address': address,
                'city': 'Roma',
                'district': None,
                'postal_code': None,
                'latitude': latitude,
                'longitude': longitude,
                'images': all_photos,  # –í–°–ï —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ —Å–ø–∏—Å–∫–æ–º
                'virtual_tour_url': None,
                'agency_name': estate.get('advertiser', {}).get('agency', {}).get('displayName'),
                'contact_info': None,
                'is_active': True,
                'published_at': None,
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
            return None
    
    def parse_single_listing(self, listing_json: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (–±–µ–∑ –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è)"""
        try:
            estate = listing_json.get('realEstate', {})
            if not estate:
                return None
            
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            title = properties.get('caption')
            canonical_url = listing_json.get('seo', {}).get('url')
            
            if not title or not canonical_url:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL
            external_id = None
            if canonical_url:
                match = re.search(r'/annunci/(\d+)/', canonical_url)
                if match:
                    external_id = match.group(1)
            
            # –¶–µ–Ω–∞
            price_info = estate.get('price', {})
            price = price_info.get('value')
            
            # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            rooms = self._extract_number(properties.get('rooms', ''))
            area = self._extract_number(properties.get('surface', ''))
            bathrooms = self._extract_number(properties.get('bathrooms', ''))
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –í–°–ï —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            all_photos = self.extract_all_photos(listing_json)
            
            # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏ –∞–¥—Ä–µ—Å (—Ç–æ–ª—å–∫–æ –∏–∑ JSON)
            address = self._extract_address(listing_json)
            latitude, longitude = self._extract_coordinates(listing_json)
            
            # –¢–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
            property_type = self._normalize_property_type(properties, title)
            
            return {
                'external_id': external_id,
                'source': 'immobiliare',
                'url': canonical_url,
                'title': title,
                'description': properties.get('description', ''),
                'price': price,
                'price_currency': 'EUR',
                'property_type': property_type,
                'rooms': rooms,
                'bathrooms': bathrooms,
                'area': area,
                'floor': str(properties.get('floor', '')),
                'furnished': self._is_furnished(properties, title),
                'pets_allowed': None,
                'features': self._extract_features(properties),
                'address': address,
                'city': 'Roma',
                'district': None,
                'postal_code': None,
                'latitude': latitude,
                'longitude': longitude,
                'images': all_photos,
                'virtual_tour_url': None,
                'agency_name': estate.get('advertiser', {}).get('agency', {}).get('displayName'),
                'contact_info': None,
                'is_active': True,
                'published_at': None,
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
            return None
    
    async def scrape_listings(self, filters: Dict[str, Any] = None, max_pages: int = 10) -> List[Dict[str, Any]]:
        """
        –û–°–ù–û–í–ù–û–ô –ú–ï–¢–û–î: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–∞—Ä—Å–∏—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        """
        logger.info("üöÄ –ù–ê–ß–ò–ù–ê–ï–ú –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ê–°–ò–ù–•–†–û–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì IMMOBILIARE.IT")
        logger.info(f"üéØ URL: {self.main_page_url}")
        logger.info(f"üìÑ –ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü: {max_pages}")
        logger.info(f"‚ö° –†–µ–∂–∏–º: ScraperAPI Async + Fallback")
        
        start_time = time.time()
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        semaphore = asyncio.Semaphore(3)  # –ú–∞–∫—Å–∏–º—É–º 3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞
        
        async def scrape_page_with_semaphore(page_num: int):
            async with semaphore:
                return await self.scrape_page_async(page_num)
        
        tasks = []
        for page_num in range(1, max_pages + 1):
            task = scrape_page_with_semaphore(page_num)
            tasks.append(task)
        
        logger.info(f"üîÑ –ó–∞–ø—É—Å–∫–∞–µ–º {len(tasks)} –∑–∞–¥–∞—á —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞...")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_listings = []
        successful_pages = 0
        error_pages = 0
        fallback_pages = 0
        
        for page_num, result in enumerate(results, 1):
            if isinstance(result, Exception):
                logger.error(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}: {result}")
                error_pages += 1
            elif isinstance(result, list):
                all_listings.extend(result)
                if result:
                    successful_pages += 1
                    logger.info(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: {len(result)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
                else:
                    logger.info(f"üîö –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –ø—É—Å—Ç–∞—è")
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {type(result)}")
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        unique_listings = []
        seen_ids = set()
        
        for listing in all_listings:
            listing_id = listing.get('external_id')
            if listing_id and listing_id not in seen_ids:
                seen_ids.add(listing_id)
                unique_listings.append(listing)
        
        elapsed_time = time.time() - start_time
        
        logger.info(f"üéâ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ê–°–ò–ù–•–†–û–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!")
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        logger.info(f"   ‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed_time:.1f}—Å")
        logger.info(f"   ‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {successful_pages}")
        logger.info(f"   ‚ùå –û—à–∏–±–æ–∫: {error_pages}")
        logger.info(f"   üìã –í—Å–µ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(all_listings)}")
        logger.info(f"   üîÑ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(unique_listings)}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è–º
        if unique_listings:
            photo_counts = [len(listing.get('images', [])) for listing in unique_listings]
            avg_photos = sum(photo_counts) / len(photo_counts)
            logger.info(f"   üì∏ –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ—Ç–æ: {avg_photos:.1f}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º
            with_coords = sum(1 for listing in unique_listings if listing.get('latitude') and listing.get('longitude'))
            coord_percentage = (with_coords / len(unique_listings)) * 100
            logger.info(f"   üó∫Ô∏è –° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {with_coords}/{len(unique_listings)} ({coord_percentage:.1f}%)")
        
        return unique_listings
    
    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def _extract_number(self, value: str) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        if not value:
            return None
        match = re.search(r'\d+', str(value))
        return int(match.group(0)) if match else None
    
    async def _extract_coordinates_with_geocoding(self, listing_json: Dict[str, Any]) -> tuple[Optional[float], Optional[float]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑ JSON –∏–ª–∏ —á–µ—Ä–µ–∑ –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ OpenStreetMap
        –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ JSON, –∑–∞—Ç–µ–º –≥–µ–æ–∫–æ–¥–∏—Ä—É–µ—Ç –∞–¥—Ä–µ—Å
        """
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ JSON (–∫–∞–∫ –≤ —Å—Ç–∞—Ä–æ–º –ø–∞—Ä—Å–µ—Ä–µ)
            estate = listing_json.get('realEstate', {})
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            location_sources = [
                properties.get('location', {}),
                estate.get('location', {}),
                properties.get('coordinates', {}),
                estate.get('coordinates', {})
            ]
            
            for location in location_sources:
                lat = location.get('latitude') or location.get('lat')
                lon = location.get('longitude') or location.get('lng')
                
                if lat and lon:
                    lat, lon = float(lat), float(lon)
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –ò—Ç–∞–ª–∏–∏
                    if 35.0 <= lat <= 47.0 and 6.0 <= lon <= 19.0:
                        logger.debug(f"üìç –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–∞–π–¥–µ–Ω—ã –≤ JSON: {lat}, {lon}")
                        return lat, lon
            
            # –ï—Å–ª–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –Ω–µ—Ç –≤ JSON, –ø—ã—Ç–∞–µ–º—Å—è –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∞–¥—Ä–µ—Å
            address = self._extract_address(listing_json)
            if address and len(address.strip()) > 10:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–¥—Ä–µ—Å
                logger.debug(f"üó∫Ô∏è –ì–µ–æ–∫–æ–¥–∏—Ä—É–µ–º –∞–¥—Ä–µ—Å: {address}")
                return await self._geocode_address(address, "Roma, Italy")
            
            return None, None
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç: {e}")
            return None, None
    
    async def _geocode_address(self, address: str, city: str) -> tuple[Optional[float], Optional[float]]:
        """
        –ì–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ —á–µ—Ä–µ–∑ OpenStreetMap Nominatim API
        """
        try:
            # –°—Ç—Ä–æ–∏–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
            full_address = f"{address}, {city}"
            url = "https://nominatim.openstreetmap.org/search"
            
            params = {
                'q': full_address,
                'format': 'json',
                'limit': 1,
                'countrycodes': 'it',  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–æ–∏—Å–∫ –ò—Ç–∞–ª–∏–µ–π
                'addressdetails': 1
            }
            
            headers = {
                'User-Agent': 'ITA_RENT_BOT/1.0 (rental search bot)'  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π User-Agent –¥–ª—è Nominatim
            }
            
            timeout = aiohttp.ClientTimeout(total=10)
            
            async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        if data and len(data) > 0:
                            result = data[0]
                            lat = float(result.get('lat', 0))
                            lon = float(result.get('lon', 0))
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ò—Ç–∞–ª–∏–∏
                            if 35.0 <= lat <= 47.0 and 6.0 <= lon <= 19.0:
                                logger.debug(f"‚úÖ –ì–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ: {lat}, {lon}")
                                return lat, lon
                            else:
                                logger.debug(f"‚ö†Ô∏è –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤–Ω–µ –ò—Ç–∞–ª–∏–∏: {lat}, {lon}")
                        else:
                            logger.debug(f"üîç –ê–¥—Ä–µ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {full_address}")
                    else:
                        logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: HTTP {response.status}")
                        
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ª–∏–º–∏—Ç–æ–≤ API
                    await asyncio.sleep(1)
                    
            return None, None
            
        except Exception as e:
            logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è '{address}': {e}")
            return None, None
    
    def _extract_coordinates(self, listing_json: Dict[str, Any]) -> tuple[Optional[float], Optional[float]]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (—Ç–æ–ª—å–∫–æ –∏–∑ JSON)"""
        try:
            estate = listing_json.get('realEstate', {})
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            location_sources = [
                properties.get('location', {}),
                estate.get('location', {}),
                properties.get('coordinates', {}),
                estate.get('coordinates', {})
            ]
            
            for location in location_sources:
                lat = location.get('latitude') or location.get('lat')
                lon = location.get('longitude') or location.get('lng')
                
                if lat and lon:
                    lat, lon = float(lat), float(lon)
                    if 35.0 <= lat <= 47.0 and 6.0 <= lon <= 19.0:
                        return lat, lon
            
            return None, None
            
        except Exception:
            return None, None
    
    def _extract_address(self, listing_json: Dict[str, Any]) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–¥—Ä–µ—Å –∏–∑ JSON"""
        try:
            estate = listing_json.get('realEstate', {})
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            address_fields = [
                properties.get('address'),
                properties.get('location', {}).get('address'),
                properties.get('street'),
                estate.get('address')
            ]
            
            for addr in address_fields:
                if addr and isinstance(addr, str) and len(addr.strip()) > 5:
                    return addr.strip()
            
            return None
            
        except Exception:
            return None
    
    def _normalize_property_type(self, properties: Dict[str, Any], title: str) -> str:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ (—É–Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–æ –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞)
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: apartment, house, studio, room
        """
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø –∏–∑ JSON
        property_type = None
        if properties.get('typology', {}).get('name'):
            property_type = properties['typology']['name']
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
        type_mapping = {
            'Appartamento': 'apartment',
            'Villa': 'house',
            'Casa': 'house',
            'Villetta': 'house',
            'Attico': 'apartment',
            'Loft': 'apartment',
            'Monolocale': 'studio',
            'Bilocale': 'apartment',
            'Trilocale': 'apartment',
            'Quadrilocale': 'apartment',
            'Stanza': 'room',
            'Posto letto': 'room',
            'Camera': 'room'
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä—è–º–æ–π –º–∞–ø–ø–∏–Ω–≥
        if property_type and property_type in type_mapping:
            return type_mapping[property_type]
        
        # –ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–∫–∞–∫ –≤ —Å—Ç–∞—Ä–æ–º –ø–∞—Ä—Å–µ—Ä–µ)
        title_lower = title.lower()
        if 'monolocale' in title_lower or 'studio' in title_lower:
            return 'studio'
        elif any(word in title_lower for word in ['villa', 'casa', 'villetta', 'casa indipendente']):
            return 'house'
        elif any(word in title_lower for word in ['stanza', 'posto letto', 'camera']):
            return 'room'
        
        return 'apartment'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def _is_furnished(self, properties: Dict[str, Any], title: str) -> Optional[bool]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –º–µ–±–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ª–∏ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ —Å–≤–æ–π—Å—Ç–≤–∞—Ö
        furnished_info = properties.get('furnished')
        if furnished_info is not None:
            return bool(furnished_info)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
        title_lower = title.lower()
        if any(word in title_lower for word in ['arredato', 'arredata', 'arredati', 'furnished']):
            return True
        elif any(word in title_lower for word in ['non arredato', 'non arredata', 'vuoto', 'unfurnished']):
            return False
        
        return None
    
    def _extract_features(self, properties: Dict[str, Any]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"""
        features = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–ª—è
        if properties.get('hasElevator'):
            features.append('elevator')
        if properties.get('hasParking'):
            features.append('parking')
        if properties.get('hasBalcony'):
            features.append('balcony')
        if properties.get('hasTerrace'):
            features.append('terrace')
        if properties.get('hasGarden'):
            features.append('garden')
        
        return features
    
    def parse_listings_from_page(self, html_content: str) -> List[Dict[str, Any]]:
        """–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ - —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        # –≠—Ç–æ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é parse_html_content
        import asyncio
        return asyncio.run(self.parse_html_content(html_content, 1))
    
    def normalize_listing_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞"""
        return raw_data 