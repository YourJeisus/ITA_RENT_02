#!/usr/bin/env python3
"""
üöÄ –£–ü–†–û–©–ï–ù–ù–´–ô –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–ê–†–°–ï–† IMMOBILIARE.IT
–ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –±–µ–∑ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ + –ø–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è
"""
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from src.core.config import settings
import json
import re
from datetime import datetime
from typing import List, Dict, Any, Optional

class ImmobiliareScraper:
    """–ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä Immobiliare –±–µ–∑ –ª–∏—à–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    
    def __init__(self, enable_geocoding: bool = False):
        self.base_url = "https://www.immobiliare.it"
        self.search_url = "https://www.immobiliare.it/affitto-case/roma/?criterio=data&ordine=desc"
        self.api_url = "https://api.scraperapi.com/"
        self.api_key = settings.SCRAPERAPI_KEY
        self.enable_geocoding = enable_geocoding  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º
        
        self.stats = {
            'list_pages_success': 0,
            'list_pages_failed': 0,
            'details_success': 0,
            'details_failed': 0,
            'with_description': 0,
            'with_coords': 0,
        }
    
    async def fetch_html(self, session: aiohttp.ClientSession, url: str, use_simple: bool = True) -> Optional[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —á–µ—Ä–µ–∑ ScraperAPI"""
        if use_simple:
            # –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å - –±—ã—Å—Ç—Ä—ã–π –∏ –¥–µ—à–µ–≤—ã–π
            params = {
                'api_key': self.api_key,
                'url': url
            }
        else:
            # –î–ª—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∏—Å–ø–æ–ª—å–∑—É–µ–º ultra_premium
            params = {
                'api_key': self.api_key,
                'url': url,
                'render': 'true',
                'ultra_premium': 'true'
            }
        
        try:
            timeout = aiohttp.ClientTimeout(total=90)
            async with session.get(self.api_url, params=params, timeout=timeout) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    print(f"    ‚ùå HTTP {response.status}")
                return None
        except asyncio.TimeoutError:
            print(f"    ‚è∞ –¢–∞–π–º–∞—É—Ç")
            return None
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞: {e}")
            return None
    
    def extract_next_data(self, html: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ __NEXT_DATA__"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            script = soup.find('script', id='__NEXT_DATA__')
            
            if script and script.string:
                return json.loads(script.string)
            
            return None
        except Exception:
            return None
    
    def parse_list_page(self, html: str) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å–æ —Å–ø–∏—Å–∫–æ–º"""
        next_data = self.extract_next_data(html)
        
        if not next_data:
            return []
        
        try:
            results = next_data['props']['pageProps']['dehydratedState']['queries'][0]['state']['data']['results']
            
            listings = []
            for item in results:
                estate = item.get('realEstate', {})
                properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
                
                # URL
                canonical_url = item.get('seo', {}).get('url')
                if not canonical_url:
                    continue
                
                # ID
                external_id = None
                match = re.search(r'/annunci/(\d+)/', canonical_url)
                if match:
                    external_id = match.group(1)
                
                if not external_id:
                    continue
                
                # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
                lat, lon = self._extract_coords(item)
                
                listing = {
                    'external_id': f"immobiliare_{external_id}",
                    'url': canonical_url,
                    'title': properties.get('caption', ''),
                    'price': estate.get('price', {}).get('value'),
                    'property_type': self._normalize_property_type(properties.get('typology', {}).get('name', '')),
                    'rooms': self._extract_number(properties.get('rooms')),
                    'bathrooms': self._extract_number(properties.get('bathrooms')),
                    'area_sqm': self._extract_number(properties.get('surface')),
                    'floor': self._extract_floor(properties.get('floor')),
                    'address': properties.get('location', {}).get('caption', ''),
                    'latitude': lat,
                    'longitude': lon,
                    'images': self._extract_images(item),
                    'agency_name': estate.get('advertiser', {}).get('agency', {}).get('displayName'),
                    'features': [],
                    'description': '',  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–∑–∂–µ
                    'source': 'immobiliare',
                    'city': 'Roma',
                    'scraped_at': datetime.utcnow().isoformat()
                }
                
                listings.append(listing)
            
            return listings
            
        except KeyError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª—é—á–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å–ø–∏—Å–∫–∞: {e}")
            print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏: {list(next_data.get('props', {}).get('pageProps', {}).keys()) if next_data else 'N/A'}")
            return []
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–ø–∏—Å–∫–∞: {e}")
            return []
    
    def _extract_number(self, value) -> Optional[int]:
        if not value:
            return None
        match = re.search(r'\d+', str(value))
        return int(match.group(0)) if match else None
    
    def _extract_floor(self, floor_data) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —ç—Ç–∞–∂"""
        if not floor_data:
            return None
        if isinstance(floor_data, dict):
            return floor_data.get('value') or floor_data.get('abbreviation')
        return str(floor_data)
    
    def _extract_coords(self, item: Dict) -> tuple:
        try:
            estate = item.get('realEstate', {})
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            location = properties.get('location', {})
            lat = location.get('latitude') or location.get('lat')
            lon = location.get('longitude') or location.get('lng')
            
            if lat and lon:
                return float(lat), float(lon)
            
            return None, None
        except:
            return None, None
    
    def _extract_images(self, item: Dict) -> List[str]:
        images = []
        try:
            estate = item.get('realEstate', {})
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            multimedia = properties.get('multimedia', {})
            photos = multimedia.get('photos', [])
            
            for photo in photos[:20]:  # –ú–∞–∫—Å–∏–º—É–º 20 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                if isinstance(photo, dict) and 'urls' in photo:
                    urls = photo['urls']
                    # –ë–µ—Ä–µ–º –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
                    for size in ['large', 'medium', 'small']:
                        if size in urls and urls[size]:
                            if urls[size] not in images:
                                images.append(urls[size])
                            break
            
            return images
        except:
            return []
    
    def _normalize_property_type(self, type_str: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
        mapping = {
            'Appartamento': 'apartment',
            'Villa': 'house',
            'Casa': 'house',
            'Attico': 'penthouse',
            'Superattico': 'penthouse',
            'Monolocale': 'studio',
            'Studio': 'studio',
            'Stanza': 'room',
            'Camera': 'room'
        }
        return mapping.get(type_str, 'apartment')
    
    def parse_detail_page(self, html: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            next_data = self.extract_next_data(html)
            
            if not next_data:
                return None
            
            # –í–∞—Ä–∏–∞–Ω—Ç 1: –≤ props.listing
            page_props = next_data.get('props', {}).get('pageProps', {})
            listing_data = page_props.get('listing', {})
            
            if 'properties' in listing_data:
                desc = listing_data['properties'].get('description')
                if desc:
                    return desc
            
            # –í–∞—Ä–∏–∞–Ω—Ç 2: –≤ dehydratedState.queries
            queries = page_props.get('dehydratedState', {}).get('queries', [])
            for query in queries:
                state_data = query.get('state', {}).get('data', {})
                if 'properties' in state_data:
                    desc = state_data['properties'].get('description')
                    if desc:
                        return desc
            
            # –í–∞—Ä–∏–∞–Ω—Ç 3: –ü–æ–∏—Å–∫ –≤ HTML
            soup = BeautifulSoup(html, 'html.parser')
            desc_div = soup.find('div', class_=lambda x: x and 'description' in str(x).lower())
            if desc_div:
                return desc_div.get_text(strip=True)
            
            return None
            
        except Exception as e:
            return None
    
    async def scrape_multiple_pages(self, max_pages: int = 5):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º (—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å)"""
        return await self.scrape_listings(num_pages=max_pages, max_details=0)
    
    async def scrape_listings(self, num_pages: int = 2, max_details: int = 10):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        print("=" * 80)
        print(f"üöÄ –ü–ê–†–°–ò–ù–ì IMMOBILIARE.IT –° –î–ï–¢–ê–õ–¨–ù–û–ô –ò–ù–§–û–†–ú–ê–¶–ò–ï–ô")
        print("=" * 80)
        print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü —Å–ø–∏—Å–∫–æ–≤: {num_pages}")
        print(f"üìù –î–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: –¥–æ {max_details}")
        print("=" * 80)
        
        start_time = datetime.utcnow()
        
        async with aiohttp.ClientSession() as session:
            # –≠–¢–ê–ü 1: –°–æ–±–∏—Ä–∞–µ–º URL —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü —Å–ø–∏—Å–∫–æ–≤
            print("\nüìã –≠–¢–ê–ü 1: –°–±–æ—Ä –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
            print("-" * 80)
            
            all_listings = []
            for page_num in range(1, num_pages + 1):
                if page_num == 1:
                    page_url = self.search_url
                else:
                    page_url = f"{self.search_url}&pag={page_num}"
                
                print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: {page_url}")
                html = await self.fetch_html(session, page_url, use_simple=True)
                
                if html:
                    print(f"    ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
                    listings = self.parse_list_page(html)
                    print(f"    üìä –ù–∞–π–¥–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
                    all_listings.extend(listings)
                    self.stats['list_pages_success'] += 1
                else:
                    print(f"    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML")
                    self.stats['list_pages_failed'] += 1
            
            print(f"\nüìä –í–°–ï–ì–û: {len(all_listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            listings_to_detail = all_listings[:max_details]
            
            # –≠–¢–ê–ü 2: –ü–∞—Ä—Å–∏–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            print("\n" + "=" * 80)
            print(f"üîç –≠–¢–ê–ü 2: –ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü ({len(listings_to_detail)} —à—Ç)")
            print("=" * 80)
            
            for i, listing in enumerate(listings_to_detail, 1):
                print(f"[{i}/{len(listings_to_detail)}] {listing['url']}")
                
                detail_html = await self.fetch_html(session, listing['url'], use_simple=False)
                
                if detail_html:
                    description = self.parse_detail_page(detail_html)
                    
                    if description:
                        listing['description'] = description
                        self.stats['with_description'] += 1
                        print(f"    ‚úÖ –û–ø–∏—Å–∞–Ω–∏–µ: {len(description)} —Å–∏–º–≤–æ–ª–æ–≤")
                    else:
                        print(f"    ‚ö†Ô∏è –û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                    
                    self.stats['details_success'] += 1
                else:
                    print(f"    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É")
                    self.stats['details_failed'] += 1
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
                if listing.get('latitude'):
                    self.stats['with_coords'] += 1
        
        end_time = datetime.utcnow()
        elapsed = (end_time - start_time).total_seconds()
        
        # –ò–¢–û–ì–ò
        print("\n" + "=" * 80)
        print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("=" * 80)
        print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü —Å–ø–∏—Å–∫–æ–≤:")
        print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ: {self.stats['list_pages_success']}")
        print(f"   ‚ùå –û—à–∏–±–∫–∏: {self.stats['list_pages_failed']}")
        print()
        print(f"üìù –î–µ—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:")
        print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ: {self.stats['details_success']}")
        print(f"   ‚ùå –û—à–∏–±–∫–∏: {self.stats['details_failed']}")
        print()
        print(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:")
        print(f"   üìù –° –ø–æ–ª–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º: {self.stats['with_description']}/{len(listings_to_detail)}")
        print(f"   üåç –° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {self.stats['with_coords']}/{len(all_listings)}")
        print(f"   üñºÔ∏è  –° –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏: {sum(1 for l in all_listings if l.get('images'))}/{len(all_listings)}")
        print()
        print(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {elapsed:.1f} —Å–µ–∫—É–Ω–¥")
        if listings_to_detail:
            print(f"‚ö° –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {elapsed/len(listings_to_detail):.2f} —Å–µ–∫")
        
        return all_listings


async def main():
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--pages', type=int, default=2, help='–°—Ç—Ä–∞–Ω–∏—Ü —Å–ø–∏—Å–∫–æ–≤')
    parser.add_argument('--details', type=int, default=10, help='–î–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü')
    
    args = parser.parse_args()
    
    scraper = ImmobiliareSimpleScraper()
    listings = await scraper.scrape_listings(num_pages=args.pages, max_details=args.details)
    
    if listings:
        output_file = '/tmp/immobiliare_results.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(listings, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")


if __name__ == "__main__":
    asyncio.run(main())

