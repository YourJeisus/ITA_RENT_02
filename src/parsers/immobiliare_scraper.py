#!/usr/bin/env python3
"""
üöÄ –ù–û–í–´–ô –ê–°–ò–ù–•–†–û–ù–ù–´–ô –°–ö–†–ê–ü–ï–† –î–õ–Ø IMMOBILIARE.IT V2
–°–æ–∑–¥–∞–Ω —Å –Ω—É–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ ScraperAPI

–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:
‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è ScraperAPI Async API
‚úÖ Job submission -> Status polling -> Result extraction
‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ external_id
‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å —Ä–∞–∑–Ω—ã–º–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏
‚úÖ Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π API –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö
‚úÖ –ì–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ OpenStreetMap
"""
import asyncio
import aiohttp
import logging
import json
import re
import time
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
from bs4 import BeautifulSoup
from ..core.config import settings

logger = logging.getLogger(__name__)


class ImmobiliareScraper:
    """
    –ù–æ–≤—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–µ—Ä –¥–ª—è Immobiliare.it
    –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è ScraperAPI Async API
    """
    
    def __init__(self, enable_geocoding: bool = True):
        self.name = "Immobiliare.it Async Scraper V2"
        self.base_url = "https://www.immobiliare.it"
        self.search_url = "https://www.immobiliare.it/affitto-case/roma/?criterio=data&ordine=desc"
        self.enable_geocoding = enable_geocoding
        
        # ScraperAPI endpoints
        self.async_jobs_url = "https://async.scraperapi.com/jobs"
        self.sync_api_url = "https://api.scraperapi.com"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–∞–π–º–∞—É—Ç–æ–≤
        self.job_submit_timeout = 30
        self.job_poll_timeout = 300  # 5 –º–∏–Ω—É—Ç –º–∞–∫—Å–∏–º—É–º
        self.sync_request_timeout = 70  # –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        
        # –ö–µ—à –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        self.seen_listing_ids: Set[str] = set()
    
    def build_page_url(self, page: int) -> str:
        """–°—Ç—Ä–æ–∏—Ç URL –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        if page <= 1:
            return self.search_url
        return f"{self.search_url}&pag={page}"
    
    async def submit_async_job(self, url: str, page_num: int) -> Optional[Dict[str, Any]]:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–¥–∞—á—É –≤ ScraperAPI Async Jobs API
        
        –°–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:
        POST https://async.scraperapi.com/jobs
        {
            "apiKey": "YOUR_API_KEY",
            "url": "target_url",
            "render": true/false,
            "premium": true/false,
            ...
        }
        """
        if not settings.SCRAPERAPI_KEY:
            logger.error("‚ùå SCRAPERAPI_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            return None
        
        payload = {
            "apiKey": settings.SCRAPERAPI_KEY,
            "url": url,
            "render": False,  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º JS —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ (–æ–Ω –≤—ã–∑—ã–≤–∞–ª –æ—à–∏–±–∫–∏)
            "premium": False,  # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–∫—Å–∏
            "country_code": "it",
            "device_type": "desktop"
        }
        
        headers = {
            "Content-Type": "application/json"
        }
        
        timeout = aiohttp.ClientTimeout(total=self.job_submit_timeout)
        
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    self.async_jobs_url,
                    json=payload,
                    headers=headers
                ) as response:
                    
                    response_text = await response.text()
                    
                    if response.status == 200:
                        try:
                            job_data = json.loads(response_text)
                            job_id = job_data.get("id")
                            status = job_data.get("status")
                            status_url = job_data.get("statusUrl")
                            
                            logger.debug(f"üì§ Job {job_id} —Å–æ–∑–¥–∞–Ω –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                            return {
                                "id": job_id,
                                "status": status,
                                "statusUrl": status_url,
                                "page_num": page_num,
                                "url": url
                            }
                        except json.JSONDecodeError as e:
                            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç–≤–µ—Ç–∞: {e}")
                            return None
                    else:
                        logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ {response.status} –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                        return None
                        
        except Exception as e:
            logger.error(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ job –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
            return None
    
    async def poll_job_status(self, job_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        –û–ø—Ä–∞—à–∏–≤–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        
        GET https://async.scraperapi.com/jobs/{job_id}
        """
        job_id = job_data.get("id")
        status_url = job_data.get("statusUrl")
        page_num = job_data.get("page_num")
        
        if not status_url:
            logger.error(f"‚ùå –ù–µ—Ç statusUrl –¥–ª—è job {job_id}")
            return None
        
        start_time = time.time()
        poll_interval = 3  # –ù–∞—á–∏–Ω–∞–µ–º —Å 3 —Å–µ–∫—É–Ω–¥
        max_poll_interval = 15  # –ú–∞–∫—Å–∏–º—É–º 15 —Å–µ–∫—É–Ω–¥
        
        timeout = aiohttp.ClientTimeout(total=30)
        
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                while time.time() - start_time < self.job_poll_timeout:
                    elapsed = time.time() - start_time
                    
                    try:
                        async with session.get(status_url) as response:
                            if response.status != 200:
                                await asyncio.sleep(poll_interval)
                                continue
                            
                            response_text = await response.text()
                            
                            try:
                                job_status = json.loads(response_text)
                            except json.JSONDecodeError:
                                await asyncio.sleep(poll_interval)
                                continue
                            
                            status = job_status.get("status")
                            
                            if status == "finished":
                                logger.debug(f"‚úÖ Job {job_id} (—Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}) –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.1f}s")
                                return job_status
                            
                            elif status == "failed":
                                fail_reason = job_status.get("failReason", "unknown")
                                logger.error(f"‚ùå Job {job_id} –ø—Ä–æ–≤–∞–ª–∏–ª—Å—è: {fail_reason}")
                                return None
                            
                            elif status in ["queued", "running"]:
                                # –ü—Ä–æ—Å—Ç–æ –∂–¥–µ–º –±–µ–∑ –ª–∏—à–Ω–∏—Ö –ª–æ–≥–æ–≤
                                await asyncio.sleep(poll_interval)
                                # –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª
                                poll_interval = min(poll_interval * 1.2, max_poll_interval)
                                continue
                            
                            else:
                                logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å—Ç–∞—Ç—É—Å {status} –¥–ª—è job {job_id}")
                                await asyncio.sleep(poll_interval)
                                continue
                    
                    except asyncio.TimeoutError:
                        logger.warning(f"‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–ø—Ä–æ—Å–µ job {job_id}")
                        await asyncio.sleep(poll_interval)
                        continue
                    
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–æ—Å–µ job {job_id}: {e}")
                        await asyncio.sleep(poll_interval)
                        continue
                
                # –ü—Ä–µ–≤—ã—à–µ–Ω –æ–±—â–∏–π —Ç–∞–π–º–∞—É—Ç
                logger.error(f"‚è∞ –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è job {job_id}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–æ—Å–µ job {job_id}: {e}")
            return None
    
    async def scrape_page_sync_fallback(self, page_num: int) -> Optional[str]:
        """
        Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π ScraperAPI –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å Async API
        """
        url = self.build_page_url(page_num)
        
        params = {
            "api_key": settings.SCRAPERAPI_KEY,
            "url": url,
            "render": "false",  # –ë–µ–∑ JS —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞
        }
        
        timeout = aiohttp.ClientTimeout(total=self.sync_request_timeout)
        
        try:
            logger.info(f"üîÑ Fallback sync API –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(self.sync_api_url, params=params) as response:
                    if response.status == 200:
                        html_content = await response.text()
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –≤–∞–ª–∏–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                        if len(html_content) > 5000 and "immobiliare" in html_content.lower():
                            logger.info(f"‚úÖ Sync fallback —É—Å–ø–µ—à–µ–Ω –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                            return html_content
                        else:
                            logger.warning(f"‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                            return None
                    else:
                        logger.error(f"‚ùå Sync fallback HTTP {response.status} –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                        return None
                        
        except Exception as e:
            logger.error(f"‚ùå Sync fallback –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
            return None
    
    async def scrape_single_page(self, page_num: int) -> List[Dict[str, Any]]:
        """
        –°–∫—Ä–∞–ø–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É —á–µ—Ä–µ–∑ Async API —Å fallback
        """
        url = self.build_page_url(page_num)
        
        # –®–∞–≥ 1: –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ Async API
        job_data = await self.submit_async_job(url, page_num)
        
        html_content = None
        
        if job_data:
            # –®–∞–≥ 2: –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è job
            job_result = await self.poll_job_status(job_data)
            
            if job_result:
                # –®–∞–≥ 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ HTML –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                response_data = job_result.get("response", {})
                html_content = response_data.get("body")
                status_code = response_data.get("statusCode", 0)
                
                if html_content and status_code == 200:
                    logger.info(f"‚úÖ Async API —É—Å–ø–µ—à–µ–Ω –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                else:
                    logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å Async API –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} (–∫–æ–¥: {status_code})")
                    html_content = None
        
        # Fallback –Ω–∞ sync API –µ—Å–ª–∏ async –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
        if not html_content:
            html_content = await self.scrape_page_sync_fallback(page_num)
        
        if not html_content:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
            return []
        
        # –ü–∞—Ä—Å–∏–º HTML
        return await self.parse_html_content(html_content, page_num)
    
    async def parse_html_content(self, html_content: str, page_num: int) -> List[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç HTML –∫–æ–Ω—Ç–µ–Ω—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        """
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –¥–∞–Ω–Ω—ã–µ –∏–∑ __NEXT_DATA__
            soup = BeautifulSoup(html_content, 'html.parser')
            script_tag = soup.find('script', id='__NEXT_DATA__')
            
            if not script_tag:
                logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω __NEXT_DATA__ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
                return []
            
            try:
                json_data = json.loads(script_tag.string)
            except json.JSONDecodeError as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}: {e}")
                return []
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∞—Å—Å–∏–≤ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            try:
                results = json_data['props']['pageProps']['dehydratedState']['queries'][0]['state']['data']['results']
            except (KeyError, IndexError, TypeError) as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –º–∞—Å—Å–∏–≤ results –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}: {e}")
                return []
            
            if not results:
                logger.info(f"üîö –ü—É—Å—Ç–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}")
                return []
            
            # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
            page_listings = []
            for listing_json in results:
                parsed_listing = await self.parse_single_listing(listing_json)
                if parsed_listing:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
                    listing_id = parsed_listing.get('external_id')
                    if listing_id and listing_id not in self.seen_listing_ids:
                        self.seen_listing_ids.add(listing_id)
                        page_listings.append(parsed_listing)
            
            logger.debug(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: {len(page_listings)}/{len(results)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö")
            return page_listings
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
            return []
    
    async def parse_single_listing(self, listing_json: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç –æ–¥–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –∏–∑ JSON
        """
        try:
            estate = listing_json.get('realEstate', {})
            if not estate:
                return None
            
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            title = properties.get('caption')
            canonical_url = listing_json.get('seo', {}).get('url')
            
            if not title or not canonical_url:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL
            external_id = None
            if canonical_url:
                match = re.search(r'/annunci/(\d+)/', canonical_url)
                if match:
                    external_id = match.group(1)
            
            if not external_id:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å ID –∏–∑ URL: {canonical_url}")
                return None
            
            # –¶–µ–Ω–∞
            price_info = estate.get('price', {})
            price = price_info.get('value')
            
            # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            rooms = self._extract_number(properties.get('rooms', ''))
            area = self._extract_number(properties.get('surface', ''))
            bathrooms = self._extract_number(properties.get('bathrooms', ''))
            
            # –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            images = self._extract_images(listing_json)
            
            # –ê–¥—Ä–µ—Å –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
            address = self._extract_address(listing_json)
            
            if self.enable_geocoding:
                latitude, longitude = await self._extract_coordinates_with_geocoding(listing_json)
            else:
                latitude, longitude = self._extract_coordinates(listing_json)
            
            # –¢–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
            property_type = self._normalize_property_type(properties, title)
            
            return {
                'external_id': external_id,
                'source': 'immobiliare',
                'url': canonical_url,
                'title': title,
                'description': properties.get('description', ''),
                'price': price,
                'price_currency': 'EUR',
                'property_type': property_type,
                'rooms': rooms,
                'bathrooms': bathrooms,
                'area': area,
                'floor': str(properties.get('floor', '')),
                'furnished': self._is_furnished(properties, title),
                'pets_allowed': None,
                'features': self._extract_features(properties),
                'address': address,
                'city': 'Roma',
                'district': None,
                'postal_code': None,
                'latitude': latitude,
                'longitude': longitude,
                'images': images,
                'virtual_tour_url': None,
                'agency_name': estate.get('advertiser', {}).get('agency', {}).get('displayName'),
                'contact_info': None,
                'is_active': True,
                'published_at': None,
                'scraped_at': datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
            return None
    
    async def scrape_multiple_pages(self, max_pages: int = 10) -> List[Dict[str, Any]]:
        """
        –û–°–ù–û–í–ù–û–ô –ú–ï–¢–û–î: –°–∫—Ä–∞–ø–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        """
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü...")
        
        start_time = time.time()
        
        # –û—á–∏—â–∞–µ–º –∫–µ—à –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        self.seen_listing_ids.clear()
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        semaphore = asyncio.Semaphore(3)  # –ú–∞–∫—Å–∏–º—É–º 3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞
        
        async def scrape_page_with_semaphore(page_num: int):
            async with semaphore:
                return await self.scrape_single_page(page_num)
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏
        tasks = []
        for page_num in range(1, max_pages + 1):
            task = scrape_page_with_semaphore(page_num)
            tasks.append(task)
        
        logger.info(f"üîÑ –ó–∞–ø—É—Å–∫–∞–µ–º {len(tasks)} –∑–∞–¥–∞—á —Å —Å–µ–º–∞—Ñ–æ—Ä–æ–º...")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_listings = []
        successful_pages = 0
        error_pages = 0
        
        for page_num, result in enumerate(results, 1):
            if isinstance(result, Exception):
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}: {result}")
                error_pages += 1
            elif isinstance(result, list):
                all_listings.extend(result)
                if result:
                    successful_pages += 1
                else:
                    logger.debug(f"üîö –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –ø—É—Å—Ç–∞—è")
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {type(result)}")
        
        elapsed_time = time.time() - start_time
        
        logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(all_listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∑–∞ {elapsed_time:.1f}—Å")
        
        return all_listings
    
    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def _extract_number(self, value: str) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        if not value:
            return None
        match = re.search(r'\d+', str(value))
        return int(match.group(0)) if match else None
    
    def _extract_images(self, listing_json: Dict[str, Any]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏–∑ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        images = []
        try:
            estate = listing_json.get('realEstate', {})
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            multimedia = properties.get('multimedia', {})
            photo_list = multimedia.get('photos', [])
            
            for photo in photo_list:
                if isinstance(photo, dict) and 'urls' in photo:
                    urls = photo['urls']
                    # –ë–µ—Ä–µ–º –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
                    for size in ['large', 'medium', 'small']:
                        if size in urls and urls[size]:
                            photo_url = urls[size]
                            if photo_url and photo_url not in images:
                                images.append(photo_url)
                            break
            
            return images
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–æ—Ç–æ: {e}")
            return []
    
    def _extract_address(self, listing_json: Dict[str, Any]) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–¥—Ä–µ—Å –∏–∑ JSON"""
        try:
            estate = listing_json.get('realEstate', {})
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            address_fields = [
                properties.get('address'),
                properties.get('location', {}).get('address'),
                properties.get('street'),
                estate.get('address')
            ]
            
            for addr in address_fields:
                if addr and isinstance(addr, str) and len(addr.strip()) > 5:
                    return addr.strip()
            
            return None
            
        except Exception:
            return None
    
    def _extract_coordinates(self, listing_json: Dict[str, Any]) -> tuple[Optional[float], Optional[float]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑ JSON"""
        try:
            estate = listing_json.get('realEstate', {})
            properties = estate.get('properties', [{}])[0] if estate.get('properties') else {}
            
            location_sources = [
                properties.get('location', {}),
                estate.get('location', {}),
                properties.get('coordinates', {}),
                estate.get('coordinates', {})
            ]
            
            for location in location_sources:
                lat = location.get('latitude') or location.get('lat')
                lon = location.get('longitude') or location.get('lng')
                
                if lat and lon:
                    lat, lon = float(lat), float(lon)
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –ò—Ç–∞–ª–∏–∏
                    if 35.0 <= lat <= 47.0 and 6.0 <= lon <= 19.0:
                        return lat, lon
            
            return None, None
            
        except Exception:
            return None, None
    
    async def _extract_coordinates_with_geocoding(self, listing_json: Dict[str, Any]) -> tuple[Optional[float], Optional[float]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑ JSON –∏–ª–∏ —á–µ—Ä–µ–∑ –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≤ JSON
        lat, lon = self._extract_coordinates(listing_json)
        if lat and lon:
            return lat, lon
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –≤ JSON, –ø—ã—Ç–∞–µ–º—Å—è –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∞–¥—Ä–µ—Å
        address = self._extract_address(listing_json)
        if address and len(address.strip()) > 10:
            return await self._geocode_address(address, "Roma, Italy")
        
        return None, None
    
    async def _geocode_address(self, address: str, city: str) -> tuple[Optional[float], Optional[float]]:
        """–ì–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ —á–µ—Ä–µ–∑ OpenStreetMap Nominatim API"""
        try:
            full_address = f"{address}, {city}"
            url = "https://nominatim.openstreetmap.org/search"
            
            params = {
                'q': full_address,
                'format': 'json',
                'limit': 1,
                'countrycodes': 'it',
                'addressdetails': 1
            }
            
            headers = {
                'User-Agent': 'ITA_RENT_BOT/2.0 (rental search bot)'
            }
            
            timeout = aiohttp.ClientTimeout(total=10)
            
            async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        if data and len(data) > 0:
                            result = data[0]
                            lat = float(result.get('lat', 0))
                            lon = float(result.get('lon', 0))
                            
                            if 35.0 <= lat <= 47.0 and 6.0 <= lon <= 19.0:
                                return lat, lon
                        
                    await asyncio.sleep(1)  # –£–≤–∞–∂–∞–µ–º –ª–∏–º–∏—Ç—ã API
                    
            return None, None
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è '{address}': {e}")
            return None, None
    
    def _normalize_property_type(self, properties: Dict[str, Any], title: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
        property_type = None
        if properties.get('typology', {}).get('name'):
            property_type = properties['typology']['name']
        
        type_mapping = {
            'Appartamento': 'apartment',
            'Villa': 'house',
            'Casa': 'house',
            'Villetta': 'house',
            'Attico': 'penthouse',         # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –ø–µ–Ω—Ç—Ö–∞—É—Å
            'Superattico': 'penthouse',    # –°—É–ø–µ—Ä-–ø–µ–Ω—Ç—Ö–∞—É—Å
            'Loft': 'apartment',
            'Monolocale': 'studio',        # –°—Ç—É–¥–∏—è
            'Studio': 'studio',            # –°—Ç—É–¥–∏—è (–∞–Ω–≥–ª.)
            'Bilocale': 'apartment',
            'Trilocale': 'apartment',
            'Quadrilocale': 'apartment',
            'Plurilocale': 'apartment',
            'Stanza': 'room',
            'Posto letto': 'room',
            'Camera': 'room'
        }
        
        if property_type and property_type in type_mapping:
            return type_mapping[property_type]
        
        title_lower = title.lower()
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        if any(word in title_lower for word in ['attico', 'superattico', 'penthouse']):
            return 'penthouse'
        elif any(word in title_lower for word in ['monolocale', 'studio']):
            return 'studio'
        elif any(word in title_lower for word in ['villa', 'casa', 'villetta']):
            return 'house'
        elif any(word in title_lower for word in ['stanza', 'posto letto', 'camera', 'room']):
            return 'room'
        
        return 'apartment'
    
    def _is_furnished(self, properties: Dict[str, Any], title: str) -> Optional[bool]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –º–µ–±–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ª–∏ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å"""
        furnished_info = properties.get('furnished')
        if furnished_info is not None:
            return bool(furnished_info)
        
        title_lower = title.lower()
        if any(word in title_lower for word in ['arredato', 'arredata', 'arredati', 'furnished']):
            return True
        elif any(word in title_lower for word in ['non arredato', 'non arredata', 'vuoto']):
            return False
        
        return None
    
    def _extract_features(self, properties: Dict[str, Any]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"""
        features = []
        
        if properties.get('hasElevator'):
            features.append('elevator')
        if properties.get('hasParking'):
            features.append('parking')
        if properties.get('hasBalcony'):
            features.append('balcony')
        if properties.get('hasTerrace'):
            features.append('terrace')
        if properties.get('hasGarden'):
            features.append('garden')
        
        return features 