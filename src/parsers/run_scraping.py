#!/usr/bin/env python3
"""
üöÄ –°–ö–†–ò–ü–¢ –î–õ–Ø –ó–ê–ü–£–°–ö–ê –ü–ê–†–°–ò–ù–ì–ê IMMOBILIARE.IT

–ü–∞—Ä—Å–∏—Ç –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É https://www.immobiliare.it/affitto-case/roma/
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏ –≤—Å–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ë–î

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    cd src/parsers
    python run_scraping.py
    
–ò–ª–∏ –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞:
    python src/parsers/run_scraping.py
"""
import sys
import asyncio
import logging
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.parsers.immobiliare_scraper import ImmobiliareScraper
from src.services.scraping_service import ScrapingService
from src.db.database import SessionLocal

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


async def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞
    """
    print("üöÄ –ü–∞—Ä—Å–∏–Ω–≥ Immobiliare.it (–†–∏–º)")
    
    # –ì–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∫–ª—é—á–µ–Ω–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    import sys
    enable_geocoding = True
    
    if len(sys.argv) > 1 and sys.argv[1] == "--no-geo":
        enable_geocoding = False
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∫—Ä–∞–ø–µ—Ä —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    scraper = ImmobiliareScraper(enable_geocoding=enable_geocoding)
    scraping_service = ScrapingService()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º ScraperAPI –∫–ª—é—á
    from src.core.config import settings
    if not settings.SCRAPERAPI_KEY:
        print("‚ùå –û–®–ò–ë–ö–ê: SCRAPERAPI_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω!")
        return
    
    try:
        
        import time
        start_time = time.time()
        
        listings = await scraper.scrape_multiple_pages(max_pages=10)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        if not listings:
            print("‚ùå –û–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            return
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        db = SessionLocal()
        try:
            saved_stats = scraping_service.save_listings_to_db(listings, db)
            
            # –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞ —Å —Ç–∞–π–º–µ—Ä–æ–º
            from datetime import datetime, timedelta
            next_run = datetime.now() + timedelta(hours=1)
            
            print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∑–∞ {execution_time:.1f}—Å")
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {saved_stats['created']} –Ω–æ–≤—ã—Ö, {saved_stats['updated']} –æ–±–Ω–æ–≤–ª–µ–Ω–æ")
            print(f"‚è∞ –°–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—É—Å–∫: {next_run.strftime('%H:%M %d.%m.%Y')} (—á–µ—Ä–µ–∑ 1—á)")
            
        finally:
            db.close()
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")
        print(f"‚ùå –û–®–ò–ë–ö–ê: {e}")


if __name__ == "__main__":
    asyncio.run(main()) 