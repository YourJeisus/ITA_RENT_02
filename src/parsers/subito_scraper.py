#!/usr/bin/env python3
"""
üöÄ –ê–°–ò–ù–•–†–û–ù–ù–´–ô –°–ö–†–ê–ü–ï–† –î–õ–Ø SUBITO.IT V1
–°–æ–∑–¥–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã ImmobiliareScraper

–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:
‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è ScraperAPI Async API
‚úÖ Job submission -> Status polling -> Result extraction
‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ external_id
‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å —Ä–∞–∑–Ω—ã–º–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏
‚úÖ Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π API –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö
‚úÖ –ì–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ OpenStreetMap
"""
import asyncio
import aiohttp
import logging
import json
import re
import time
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
from ..core.config import settings

logger = logging.getLogger(__name__)


class SubitoScraper:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–µ—Ä –¥–ª—è Subito.it
    –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ ImmobiliareScraper
    """
    
    def __init__(self, enable_geocoding: bool = True):
        self.name = "Subito.it Async Scraper V1"
        self.base_url = "https://www.subito.it"
        self.search_url = "https://www.subito.it/annunci-lazio/affitto/immobili/roma/roma/"
        self.enable_geocoding = enable_geocoding
        
        # ScraperAPI endpoints
        self.async_jobs_url = "https://async.scraperapi.com/jobs"
        self.sync_api_url = "https://api.scraperapi.com"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–∞–π–º–∞—É—Ç–æ–≤
        self.job_submit_timeout = 30
        self.job_poll_timeout = 300  # 5 –º–∏–Ω—É—Ç –º–∞–∫—Å–∏–º—É–º
        self.sync_request_timeout = 70  # –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        
        # –ö–µ—à –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        self.seen_listing_ids: Set[str] = set()
    
    def build_page_url(self, page: int) -> str:
        """–°—Ç—Ä–æ–∏—Ç URL –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        if page <= 1:
            return self.search_url
        return f"{self.search_url}?o={page}"
    
    async def submit_async_job(self, url: str, page_num: int) -> Optional[Dict[str, Any]]:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–¥–∞—á—É –≤ ScraperAPI Async Jobs API
        """
        if not settings.SCRAPERAPI_KEY:
            logger.error("‚ùå SCRAPERAPI_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            return None
        
        payload = {
            "apiKey": settings.SCRAPERAPI_KEY,
            "url": url,
            "render": False,  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º JS —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥
            "premium": False,  # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–∫—Å–∏
            "country_code": "it",
            "device_type": "desktop"
        }
        
        headers = {
            "Content-Type": "application/json"
        }
        
        timeout = aiohttp.ClientTimeout(total=self.job_submit_timeout)
        
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    self.async_jobs_url,
                    json=payload,
                    headers=headers
                ) as response:
                    
                    response_text = await response.text()
                    
                    if response.status == 200:
                        try:
                            job_data = json.loads(response_text)
                            job_id = job_data.get("id")
                            status = job_data.get("status")
                            status_url = job_data.get("statusUrl")
                            
                            logger.debug(f"üì§ Job {job_id} —Å–æ–∑–¥–∞–Ω –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                            return {
                                "id": job_id,
                                "status": status,
                                "statusUrl": status_url,
                                "page_num": page_num,
                                "url": url
                            }
                        except json.JSONDecodeError as e:
                            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç–≤–µ—Ç–∞: {e}")
                            return None
                    else:
                        logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ {response.status} –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                        return None
                        
        except Exception as e:
            logger.error(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ job –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
            return None
    
    async def poll_job_status(self, job_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        –û–ø—Ä–∞—à–∏–≤–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        """
        job_id = job_data.get("id")
        status_url = job_data.get("statusUrl")
        page_num = job_data.get("page_num")
        
        if not status_url:
            logger.error(f"‚ùå –ù–µ—Ç statusUrl –¥–ª—è job {job_id}")
            return None
        
        start_time = time.time()
        poll_interval = 3  # –ù–∞—á–∏–Ω–∞–µ–º —Å 3 —Å–µ–∫—É–Ω–¥
        max_poll_interval = 15  # –ú–∞–∫—Å–∏–º—É–º 15 —Å–µ–∫—É–Ω–¥
        
        timeout = aiohttp.ClientTimeout(total=30)
        
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                while time.time() - start_time < self.job_poll_timeout:
                    elapsed = time.time() - start_time
                    
                    try:
                        async with session.get(status_url) as response:
                            if response.status != 200:
                                await asyncio.sleep(poll_interval)
                                continue
                            
                            response_text = await response.text()
                            
                            try:
                                job_status = json.loads(response_text)
                            except json.JSONDecodeError:
                                await asyncio.sleep(poll_interval)
                                continue
                            
                            status = job_status.get("status")
                            
                            if status == "finished":
                                logger.debug(f"‚úÖ Job {job_id} (—Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}) –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.1f}s")
                                return job_status
                            
                            elif status == "failed":
                                fail_reason = job_status.get("failReason", "unknown")
                                logger.error(f"‚ùå Job {job_id} –ø—Ä–æ–≤–∞–ª–∏–ª—Å—è: {fail_reason}")
                                return None
                            
                            elif status in ["queued", "running"]:
                                # –ü—Ä–æ—Å—Ç–æ –∂–¥–µ–º –±–µ–∑ –ª–∏—à–Ω–∏—Ö –ª–æ–≥–æ–≤
                                await asyncio.sleep(poll_interval)
                                # –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª
                                poll_interval = min(poll_interval * 1.2, max_poll_interval)
                                continue
                            
                            else:
                                logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å—Ç–∞—Ç—É—Å {status} –¥–ª—è job {job_id}")
                                await asyncio.sleep(poll_interval)
                                continue
                    
                    except asyncio.TimeoutError:
                        logger.warning(f"‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–ø—Ä–æ—Å–µ job {job_id}")
                        await asyncio.sleep(poll_interval)
                        continue
                    
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–æ—Å–µ job {job_id}: {e}")
                        await asyncio.sleep(poll_interval)
                        continue
                
                # –ü—Ä–µ–≤—ã—à–µ–Ω –æ–±—â–∏–π —Ç–∞–π–º–∞—É—Ç
                logger.error(f"‚è∞ –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è job {job_id}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–æ—Å–µ job {job_id}: {e}")
            return None
    
    async def scrape_page_sync_fallback(self, page_num: int) -> Optional[str]:
        """
        Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π ScraperAPI –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å Async API
        """
        url = self.build_page_url(page_num)
        
        params = {
            "api_key": settings.SCRAPERAPI_KEY,
            "url": url,
            "render": "false",  # –ë–µ–∑ JS —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞
        }
        
        timeout = aiohttp.ClientTimeout(total=self.sync_request_timeout)
        
        try:
            logger.info(f"üîÑ Fallback sync API –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(self.sync_api_url, params=params) as response:
                    if response.status == 200:
                        html_content = await response.text()
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –≤–∞–ª–∏–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                        if len(html_content) > 5000 and "subito" in html_content.lower():
                            logger.info(f"‚úÖ Sync fallback —É—Å–ø–µ—à–µ–Ω –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                            return html_content
                        else:
                            logger.warning(f"‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                            return None
                    else:
                        logger.error(f"‚ùå Sync fallback HTTP {response.status} –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                        return None
                        
        except Exception as e:
            logger.error(f"‚ùå Sync fallback –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
            return None
    
    async def scrape_single_page(self, page_num: int) -> List[Dict[str, Any]]:
        """
        –°–∫—Ä–∞–ø–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É —á–µ—Ä–µ–∑ Async API —Å fallback
        """
        url = self.build_page_url(page_num)
        
        # –®–∞–≥ 1: –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ Async API
        job_data = await self.submit_async_job(url, page_num)
        
        html_content = None
        
        if job_data:
            # –®–∞–≥ 2: –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è job
            job_result = await self.poll_job_status(job_data)
            
            if job_result:
                # –®–∞–≥ 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ HTML –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                response_data = job_result.get("response", {})
                html_content = response_data.get("body")
                status_code = response_data.get("statusCode", 0)
                
                if html_content and status_code == 200:
                    logger.info(f"‚úÖ Async API —É—Å–ø–µ—à–µ–Ω –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                else:
                    logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å Async API –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} (–∫–æ–¥: {status_code})")
                    html_content = None
        
        # Fallback –Ω–∞ sync API –µ—Å–ª–∏ async –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
        if not html_content:
            html_content = await self.scrape_page_sync_fallback(page_num)
        
        if not html_content:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
            return []
        
        # –ü–∞—Ä—Å–∏–º HTML
        return await self.parse_html_content(html_content, page_num)
    
    async def parse_html_content(self, html_content: str, page_num: int) -> List[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç HTML –∫–æ–Ω—Ç–µ–Ω—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Subito.it –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–∞—Ä—Ç–æ—á–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å –∫–ª–∞—Å—Å–æ–º .item-card
            listing_containers = soup.select('.item-card')
            logger.debug(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(listing_containers)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º .item-card")
            
            if not listing_containers:
                logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã –æ–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
                return []
            
            # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
            page_listings = []
            for container in listing_containers:
                parsed_listing = await self.parse_single_listing_from_html(container)
                if parsed_listing:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
                    listing_id = parsed_listing.get('external_id')
                    if listing_id and listing_id not in self.seen_listing_ids:
                        self.seen_listing_ids.add(listing_id)
                        page_listings.append(parsed_listing)
            
            logger.debug(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: {len(page_listings)}/{len(listing_containers)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö")
            return page_listings
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
            return []
    
    async def parse_single_listing_from_html(self, container) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç –æ–¥–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –∏–∑ HTML –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        """
        try:
            # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ (–≤—Å–µ–≥–¥–∞ –ø–µ—Ä–≤–∞—è —Å—Å—ã–ª–∫–∞ –≤ –∫–∞—Ä—Ç–æ—á–∫–µ)
            link_elem = container.find('a', href=True)
            if not link_elem:
                return None
            
            listing_url = link_elem.get('href')
            if not listing_url:
                return None
            
            # –î–µ–ª–∞–µ–º URL –∞–±—Å–æ–ª—é—Ç–Ω—ã–º
            if listing_url.startswith('/'):
                listing_url = urljoin(self.base_url, listing_url)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL
            external_id = self._extract_id_from_url(listing_url)
            if not external_id:
                return None
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ (–≤—Å–µ–≥–¥–∞ –≤ h2)
            title_elem = container.select_one('h2')
            title = title_elem.get_text(strip=True) if title_elem else None
            if not title:
                return None
            
            # –¶–µ–Ω–∞ (–≤ —ç–ª–µ–º–µ–Ω—Ç–µ —Å –∫–ª–∞—Å—Å–æ–º —Å–æ–¥–µ—Ä–∂–∞—â–∏–º "price")
            price = self._extract_price_from_card(container)
            
            # –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ (–≤ —ç–ª–µ–º–µ–Ω—Ç–µ —Å –∫–ª–∞—Å—Å–æ–º —Å–æ–¥–µ—Ä–∂–∞—â–∏–º "location")
            address = self._extract_location_from_card(container)
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = self._extract_images_from_card(container)
            
            # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            rooms = self._extract_rooms_from_title(title)
            area = self._extract_area_from_title(title)
            
            # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
            if self.enable_geocoding and address:
                latitude, longitude = await self._geocode_address(address, "Roma, Italy")
            else:
                latitude, longitude = None, None
            
            # –¢–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –∏–∑ URL –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            property_type = self._normalize_property_type_from_url_and_title(listing_url, title)
            
            return {
                'external_id': external_id,
                'source': 'subito',
                'url': listing_url,
                'title': title,
                'description': '',  # Subito –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –≤ –∫–∞—Ä—Ç–æ—á–∫–∞—Ö
                'price': price,
                'price_currency': 'EUR',
                'property_type': property_type,
                'rooms': rooms,
                'bathrooms': None,  # Subito –æ–±—ã—á–Ω–æ –Ω–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç –≤–∞–Ω–Ω—ã–µ –æ—Ç–¥–µ–ª—å–Ω–æ
                'area': area,
                'floor': None,
                'furnished': self._is_furnished_from_title(title),
                'pets_allowed': None,
                'features': [],
                'address': address or 'Roma',
                'city': 'Roma',
                'district': None,
                'postal_code': None,
                'latitude': latitude,
                'longitude': longitude,
                'images': images,
                'virtual_tour_url': None,
                'agency_name': None,
                'contact_info': None,
                'is_active': True,
                'published_at': None,
                'scraped_at': datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
            return None
    
    async def scrape_multiple_pages(self, max_pages: int = 10) -> List[Dict[str, Any]]:
        """
        –û–°–ù–û–í–ù–û–ô –ú–ï–¢–û–î: –°–∫—Ä–∞–ø–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        """
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ Subito.it: {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü...")
        
        start_time = time.time()
        
        # –û—á–∏—â–∞–µ–º –∫–µ—à –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        self.seen_listing_ids.clear()
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        semaphore = asyncio.Semaphore(3)  # –ú–∞–∫—Å–∏–º—É–º 3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞
        
        async def scrape_page_with_semaphore(page_num: int):
            async with semaphore:
                return await self.scrape_single_page(page_num)
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏
        tasks = []
        for page_num in range(1, max_pages + 1):
            task = scrape_page_with_semaphore(page_num)
            tasks.append(task)
        
        logger.info(f"üîÑ –ó–∞–ø—É—Å–∫–∞–µ–º {len(tasks)} –∑–∞–¥–∞—á —Å —Å–µ–º–∞—Ñ–æ—Ä–æ–º...")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_listings = []
        successful_pages = 0
        error_pages = 0
        
        for page_num, result in enumerate(results, 1):
            if isinstance(result, Exception):
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}: {result}")
                error_pages += 1
            elif isinstance(result, list):
                all_listings.extend(result)
                if result:
                    successful_pages += 1
                else:
                    logger.debug(f"üîö –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –ø—É—Å—Ç–∞—è")
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {type(result)}")
        
        elapsed_time = time.time() - start_time
        
        logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ Subito.it –∑–∞–≤–µ—Ä—à–µ–Ω: {len(all_listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∑–∞ {elapsed_time:.1f}—Å")
        
        return all_listings
    
    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
    def _extract_id_from_url(self, url: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç ID –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ URL"""
        try:
            # Subito.it –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–æ—Ä–º–∞—Ç: /category/title-ID.htm
            # –ü—Ä–∏–º–µ—Ä: /appartamenti/appio-latino-bilocale-arredato-roma-610923878.htm
            match = re.search(r'-(\d+)\.htm', url)
            if match:
                return match.group(1)
            
            return None
        except Exception:
            return None
    
    def _extract_price_from_card(self, container) -> Optional[float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω—É –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏ Subito.it"""
        try:
            price_elem = container.select_one('[class*="price"]')
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                return self._parse_price_from_text(price_text)
            return None
        except Exception:
            return None
    
    def _parse_price_from_text(self, price_text: str) -> Optional[float]:
        """–ü–∞—Ä—Å–∏—Ç —Ü–µ–Ω—É –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not price_text:
            return None
        
        # –£–±–∏—Ä–∞–µ–º —Å–∏–º–≤–æ–ª—ã –≤–∞–ª—é—Ç—ã –∏ –ø—Ä–æ–±–µ–ª—ã, –∑–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—ã–µ –Ω–∞ —Ç–æ—á–∫–∏
        price_clean = re.sub(r'[‚Ç¨$\s]', '', price_text)
        price_clean = price_clean.replace(',', '.')
        match = re.search(r'(\d+(?:\.\d+)?)', price_clean)
        
        if match:
            return float(match.group(1))
        
        return None
    
    def _extract_location_from_card(self, container) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏ Subito.it"""
        try:
            location_elem = container.select_one('[class*="location"]')
            if location_elem:
                location_text = location_elem.get_text(strip=True)
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –≥–æ—Ä–æ–¥ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤–∏–¥–∞ "Roma(RM)Oggi alle 14:35"
                match = re.search(r'([A-Za-z\s]+)', location_text)
                if match:
                    return match.group(1).strip()
            return None
        except Exception:
            return None
    
    def _extract_images_from_card(self, container) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏ Subito.it"""
        images = []
        
        try:
            # –ò—â–µ–º img —Ç–µ–≥–∏ –≤ –∫–∞—Ä—Ç–æ—á–∫–µ
            img_elements = container.find_all('img')
            for img in img_elements:
                src = img.get('src') or img.get('data-src') or img.get('data-lazy')
                if src and src.startswith('http') and 'camera.svg' not in src:
                    if src not in images:
                        images.append(src)
        except Exception:
            pass
        
        return images
    
    def _extract_rooms_from_title(self, title: str) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        text = title.lower()
        
        # –ò—Ç–∞–ª—å—è–Ω—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–Ω–∞—Ç
        if 'monolocale' in text:
            return 1
        elif 'bilocale' in text:
            return 2
        elif 'trilocale' in text:
            return 3
        elif 'quadrilocale' in text:
            return 4
        elif 'cinque locali' in text or '5 locali' in text:
            return 5
        
        # –ò—â–µ–º —á–∏—Å–ª–∞ –ø–µ—Ä–µ–¥ "–∫–æ–º–Ω", "stanze", "locali"
        patterns = [
            r'(\d+)\s*locali',
            r'(\d+)\s*stanze',
            r'(\d+)\s*–∫–æ–º–Ω',
            r'(\d+)\s*room'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return int(match.group(1))
        
        return None
    
    def _extract_area_from_title(self, title: str) -> Optional[float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–ª–æ—â–∞–¥—å –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        # –ò—â–µ–º —á–∏—Å–ª–∞ –ø–µ—Ä–µ–¥ "–º¬≤", "mq", "metri"
        patterns = [
            r'(\d+)\s*m[¬≤q2]',
            r'(\d+)\s*metri\s*quadr',
            r'(\d+)\s*sq'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, title.lower())
            if match:
                return float(match.group(1))
        
        return None
    
    def _normalize_property_type_from_url_and_title(self, url: str, title: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –∏–∑ URL –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º URL –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        if '/appartamenti/' in url:
            return 'apartment'
        elif '/camere-posti-letto/' in url:
            return 'room'
        elif '/case-indipendenti/' in url:
            return 'house'
        elif '/attici-mansarde/' in url:
            return 'penthouse'
        elif '/uffici-locali-commerciali/' in url:
            return 'commercial'  # –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å
        elif '/garage-e-box/' in url:
            return 'garage'
        
        # –ï—Å–ª–∏ URL –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        text = title.lower()
        
        if any(word in text for word in ['monolocale', 'studio']):
            return 'studio'
        elif any(word in text for word in ['villa', 'casa']):
            return 'house'
        elif any(word in text for word in ['stanza', 'camera', 'posto letto']):
            return 'room'
        elif any(word in text for word in ['attico', 'mansarda']):
            return 'penthouse'
        elif any(word in text for word in ['appartamento', 'bilocale', 'trilocale', 'quadrilocale']):
            return 'apartment'
        
        return 'apartment'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def _is_furnished_from_title(self, title: str) -> Optional[bool]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –º–µ–±–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ª–∏ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        text = title.lower()
        
        if any(word in text for word in ['arredato', 'arredata', 'arredati', 'furnished']):
            return True
        elif any(word in text for word in ['non arredato', 'vuoto', 'unfurnished']):
            return False
        
        return None
    
    async def _geocode_address(self, address: str, city: str) -> tuple[Optional[float], Optional[float]]:
        """–ì–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ —á–µ—Ä–µ–∑ OpenStreetMap Nominatim API"""
        try:
            full_address = f"{address}, {city}"
            url = "https://nominatim.openstreetmap.org/search"
            
            params = {
                'q': full_address,
                'format': 'json',
                'limit': 1,
                'countrycodes': 'it',
                'addressdetails': 1
            }
            
            headers = {
                'User-Agent': 'ITA_RENT_BOT/2.0 (rental search bot)'
            }
            
            timeout = aiohttp.ClientTimeout(total=10)
            
            async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        if data and len(data) > 0:
                            result = data[0]
                            lat = float(result.get('lat', 0))
                            lon = float(result.get('lon', 0))
                            
                            if 35.0 <= lat <= 47.0 and 6.0 <= lon <= 19.0:
                                return lat, lon
                        
                    await asyncio.sleep(1)  # –£–≤–∞–∂–∞–µ–º –ª–∏–º–∏—Ç—ã API
                    
            return None, None
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è '{address}': {e}")
            return None, None 