#!/usr/bin/env python3
"""
ðŸš€ ÐŸÐÐ ÐÐ›Ð›Ð•Ð›Ð¬ÐÐ«Ð™ ÐŸÐÐ Ð¡Ð•Ð  SUBITO.IT
ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ñ‡ÐµÑ€ÐµÐ· __NEXT_DATA__ JSON (ÐºÐ°Ðº Casa.it)
"""
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from src.core.config import settings
import json
import re
from datetime import datetime
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin

class SubitoScraper:
    """Ð‘Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐµÑ€ Subito Ñ‡ÐµÑ€ÐµÐ· JSON"""
    
    def __init__(self, enable_geocoding: bool = False, fetch_coords: bool = False):
        self.base_url = "https://www.subito.it"
        self.search_url = "https://www.subito.it/annunci-lazio/affitto/immobili/roma/roma/"
        self.api_url = "https://api.scraperapi.com/"
        self.api_key = settings.SCRAPERAPI_KEY
        self.enable_geocoding = enable_geocoding  # Ð”Ð»Ñ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ñ Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹ÑÐ¾Ð¼
        self.fetch_coords = fetch_coords  # ÐŸÐ°Ñ€ÑÐ¸Ñ‚ÑŒ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚Ñ‹ Ñ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†
        
        self.stats = {
            'success': 0,
            'failed': 0,
            'with_coords': 0,
            'with_images': 0,
        }
    
    async def fetch_html(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ HTML Ñ‡ÐµÑ€ÐµÐ· ScraperAPI (Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ)"""
        params = {
            'api_key': self.api_key,
            'url': url
        }
        
        try:
            timeout = aiohttp.ClientTimeout(total=90)
            async with session.get(self.api_url, params=params, timeout=timeout) as response:
                if response.status == 200:
                    return await response.text()
                return None
        except Exception as e:
            print(f"    âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
            return None
    
    def extract_next_data(self, html: str) -> Optional[Dict[str, Any]]:
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ __NEXT_DATA__ Ð¸Ð· HTML"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            script = soup.find('script', id='__NEXT_DATA__')
            
            if script and script.string:
                return json.loads(script.string)
            
            return None
        except Exception as e:
            print(f"    âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° JSON: {e}")
            return None
    
    def parse_listing_data(self, item_wrapper: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ÐŸÐ°Ñ€ÑÐ¸Ñ‚ Ð¾Ð´Ð½Ð¾ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ðµ Ð¸Ð· JSON"""
        try:
            # item_wrapper Ð¸Ð¼ÐµÐµÑ‚ ÐºÐ»ÑŽÑ‡Ð¸: ['before', 'item', 'after', 'kind']
            item = item_wrapper.get('item')
            
            if not item or not isinstance(item, dict):
                return None
            
            # ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ
            external_id = item.get('urn')
            title = item.get('subject')
            
            if not external_id or not title:
                return None
            
            # URL
            url = item.get('urls', {}).get('default', '')
            if not url.startswith('http'):
                url = urljoin(self.base_url, url)
            
            # Features - ÑÑ‚Ð¾ dict, Ð° Ð½Ðµ list!
            features_dict = item.get('features', {})
            
            # Ð¦ÐµÐ½Ð° (values[0] - ÑÑ‚Ð¾ dict Ñ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸ 'key' Ð¸ 'value')
            price = None
            if '/price' in features_dict:
                price_feature = features_dict['/price']
                values = price_feature.get('values', [])
                if values:
                    value_dict = values[0]
                    if isinstance(value_dict, dict):
                        price_str = value_dict.get('key', '')  # key ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ñ‡Ð¸ÑÐ»Ð¾
                    else:
                        price_str = str(value_dict)
                    
                    try:
                        price = int(price_str)
                    except:
                        pass
            
            # Ð¥Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð½ÐµÐ´Ð²Ð¸Ð¶Ð¸Ð¼Ð¾ÑÑ‚Ð¸
            rooms = None
            if '/room' in features_dict:
                room_values = features_dict['/room'].get('values', [])
                if room_values:
                    value_dict = room_values[0]
                    if isinstance(value_dict, dict):
                        room_str = value_dict.get('key', '')
                    else:
                        room_str = str(value_dict)
                    
                    try:
                        rooms = int(re.search(r'\d+', room_str).group())
                    except:
                        pass
            
            area = None
            if '/size' in features_dict:
                size_values = features_dict['/size'].get('values', [])
                if size_values:
                    value_dict = size_values[0]
                    if isinstance(value_dict, dict):
                        size_str = value_dict.get('key', '')
                    else:
                        size_str = str(value_dict)
                    
                    try:
                        area = int(re.search(r'\d+', size_str).group())
                    except:
                        pass
            
            floor = None
            if '/floor' in features_dict:
                floor_values = features_dict['/floor'].get('values', [])
                if floor_values:
                    value_dict = floor_values[0]
                    if isinstance(value_dict, dict):
                        floor = value_dict.get('value', '')
                    else:
                        floor = str(value_dict)
            
            # Ð¢Ð¸Ð¿ Ð½ÐµÐ´Ð²Ð¸Ð¶Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¸Ð· ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸
            property_type = 'apartment'
            category = item.get('category', {})
            cat_name = category.get('friendlyName', '').lower()
            
            if 'stanza' in cat_name or 'camera' in cat_name or 'posto-letto' in cat_name:
                property_type = 'room'
            elif 'monolocale' in cat_name:
                property_type = 'studio'
            elif 'villa' in cat_name or 'casa' in cat_name:
                property_type = 'house'
            elif 'appartamenti' in cat_name:
                property_type = 'apartment'
            
            # Ð“ÐµÐ¾Ð»Ð¾ÐºÐ°Ñ†Ð¸Ñ
            geo = item.get('geo', {})
            map_data = geo.get('map', {})
            
            # Ð’ ÑÐ¿Ð¸ÑÐºÐ°Ñ… Subito Ð½Ðµ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚Ñ‹
            latitude = map_data.get('lat') if map_data else None
            longitude = map_data.get('lng') if map_data else None
            address = map_data.get('address', '') if map_data else None
            
            # Ð Ð°Ð¹Ð¾Ð½
            town = geo.get('town', {}).get('value', '')
            
            # Ð•ÑÐ»Ð¸ Ð½ÐµÑ‚ Ð°Ð´Ñ€ÐµÑÐ°, Ð±ÐµÑ€ÐµÐ¼ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð³Ð¾Ñ€Ð¾Ð´Ð°
            if not address:
                city_value = geo.get('city', {}).get('value', '')
                if city_value:
                    address = city_value
            
            # Ð˜Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ (Ð² JSON Ñ‚Ð¾Ð»ÑŒÐºÐ¾ baseUrl, Ð½ÑƒÐ¶Ð½Ð¾ Ð´Ð¾Ð¿Ð¾Ð»Ð½ÑÑ‚ÑŒ)
            images = []
            gallery = item.get('images', [])
            for img_item in gallery[:20]:  # ÐœÐ°ÐºÑÐ¸Ð¼ÑƒÐ¼ 20
                base_url = img_item.get('cdnBaseUrl') or img_item.get('url')
                if base_url:
                    # Ð”Ð»Ñ Subito Ð½ÑƒÐ¶Ð½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð°
                    if 'sbito.it' in base_url:
                        img_url = f"{base_url}?rule=width-300"
                    else:
                        img_url = base_url
                    
                    if img_url not in images:
                        images.append(img_url)
            
            # ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ
            description = item.get('body', '')
            
            # Ð”Ð°Ñ‚Ð° Ð¿ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸
            published_at = item.get('date')
            
            return {
                'external_id': f"subito_{external_id}",
                'source': 'subito',
                'url': url,
                'title': title,
                'description': description,
                'price': price,
                'property_type': property_type,
                'rooms': rooms,
                'area': area,
                'floor': floor,
                'bathrooms': None,  # Subito Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ Ð½Ðµ ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚
                'latitude': float(latitude) if latitude else None,
                'longitude': float(longitude) if longitude else None,
                'address': address or town,
                'city': 'Roma',
                'district': town,
                'images': images,
                'published_at': published_at,
                'scraped_at': datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            print(f"    âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ñ: {e}")
            return None
    
    def parse_page(self, html: str) -> List[Dict[str, Any]]:
        """ÐŸÐ°Ñ€ÑÐ¸Ñ‚ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ Ð¸ Ð¸Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð²ÑÐµ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ñ"""
        next_data = self.extract_next_data(html)
        
        if not next_data:
            return []
        
        try:
            items_list = next_data['props']['pageProps']['initialState']['items']['list']
            
            listings = []
            for item_wrapper in items_list:
                listing = self.parse_listing_data(item_wrapper)
                if listing:
                    listings.append(listing)
                    
                    # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
                    if listing.get('latitude'):
                        self.stats['with_coords'] += 1
                    if listing.get('images'):
                        self.stats['with_images'] += 1
            
            return listings
            
        except Exception as e:
            print(f"    âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹: {e}")
            return []
    
    def parse_detail_page_for_coords(self, html: str) -> Optional[tuple]:
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚Ñ‹ Ð¸Ð· Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹"""
        try:
            next_data = self.extract_next_data(html)
            if not next_data:
                return None
            
            # ÐÐ°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ñ: props.pageProps.ad.geo.map
            ad_data = next_data.get('props', {}).get('pageProps', {}).get('ad', {})
            geo = ad_data.get('geo', {})
            map_data = geo.get('map', {})
            
            latitude = map_data.get('latitude')
            longitude = map_data.get('longitude')
            
            if latitude and longitude:
                return float(latitude), float(longitude)
            
            return None
        except Exception as e:
            return None
    
    async def scrape_multiple_pages(self, max_pages: int = 5):
        """ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ñ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ñ‹Ð¼ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð¾Ð¼ (ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ)"""
        return await self.scrape_pages(num_pages=max_pages, fetch_coords=self.fetch_coords)
    
    async def scrape_pages(self, num_pages: int = 2, fetch_coords: bool = False, max_coords_fetch: int = 20, coords_concurrent: int = 10):
        """ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†"""
        print("=" * 80)
        print(f"ðŸš€ ÐŸÐÐ ÐÐ›Ð›Ð•Ð›Ð¬ÐÐ«Ð™ ÐŸÐÐ Ð¡Ð˜ÐÐ“ SUBITO.IT")
        print("=" * 80)
        print(f"ðŸ“„ Ð¡Ñ‚Ñ€Ð°Ð½Ð¸Ñ† ÑÐ¿Ð¸ÑÐºÐ¾Ð²: {num_pages}")
        if fetch_coords:
            print(f"ðŸŒ ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚: Ð´Ð¾ {max_coords_fetch} Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹")
            print(f"âš¡ ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²: {coords_concurrent}")
        print("=" * 80)
        
        start_time = datetime.utcnow()
        
        async with aiohttp.ClientSession() as session:
            # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ URLs Ð´Ð»Ñ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†
            page_urls = []
            for page_num in range(1, num_pages + 1):
                if page_num == 1:
                    page_urls.append(self.search_url)
                else:
                    page_urls.append(f"{self.search_url}?o={page_num}")
            
            print(f"\nðŸ“‹ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° {len(page_urls)} ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†...")
            print("-" * 80)
            
            # ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ°
            tasks = [self.fetch_html(session, url) for url in page_urls]
            htmls = await asyncio.gather(*tasks)
            
            # ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð²ÑÐµÑ… ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†
            all_listings = []
            for i, html in enumerate(htmls, 1):
                if html:
                    print(f"Ð¡Ñ‚Ñ€Ð°Ð½Ð¸Ñ†Ð° {i}: {len(html)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
                    listings = self.parse_page(html)
                    print(f"    âœ… ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(listings)} Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹")
                    all_listings.extend(listings)
                    self.stats['success'] += 1
                else:
                    print(f"Ð¡Ñ‚Ñ€Ð°Ð½Ð¸Ñ†Ð° {i}: âŒ Ð½Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°")
                    self.stats['failed'] += 1
            
            # Ð­Ð¢ÐÐŸ 2: ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚ Ñ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ† (ÐµÑÐ»Ð¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾)
            if fetch_coords and all_listings:
                print("\n" + "=" * 80)
                print(f"ðŸŒ Ð­Ð¢ÐÐŸ 2: ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚")
                print("=" * 80)
                
                listings_to_fetch = all_listings[:max_coords_fetch]
                print(f"ðŸ“ ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ {len(listings_to_fetch)} Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾...")
                
                # ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†
                semaphore = asyncio.Semaphore(coords_concurrent)
                
                async def fetch_and_parse_coords(listing, index):
                    async with semaphore:
                        detail_html = await self.fetch_html(session, listing['url'])
                        
                        if detail_html:
                            coords = self.parse_detail_page_for_coords(detail_html)
                            
                            if coords:
                                listing['latitude'], listing['longitude'] = coords
                                self.stats['with_coords'] += 1
                                print(f"[{index}/{len(listings_to_fetch)}] âœ… {listing['title'][:40]}... â†’ {coords[0]:.6f}, {coords[1]:.6f}")
                                return True
                            else:
                                print(f"[{index}/{len(listings_to_fetch)}] âš ï¸ {listing['title'][:40]}... â†’ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚Ñ‹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹")
                                return False
                        else:
                            print(f"[{index}/{len(listings_to_fetch)}] âŒ {listing['title'][:40]}... â†’ Ð½Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°")
                            return False
                
                # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð²ÑÐµ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾
                tasks = [fetch_and_parse_coords(listing, i) for i, listing in enumerate(listings_to_fetch, 1)]
                await asyncio.gather(*tasks)
        
        end_time = datetime.utcnow()
        elapsed = (end_time - start_time).total_seconds()
        
        # Ð˜Ð¢ÐžÐ“Ð˜
        print("\n" + "=" * 80)
        print("ðŸ“Š Ð˜Ð¢ÐžÐ“ÐžÐ’ÐÐ¯ Ð¡Ð¢ÐÐ¢Ð˜Ð¡Ð¢Ð˜ÐšÐ")
        print("=" * 80)
        print(f"âœ… Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†: {self.stats['success']}")
        print(f"âŒ ÐžÑˆÐ¸Ð±Ð¾Ðº Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸: {self.stats['failed']}")
        print(f"ðŸ“¦ Ð’ÑÐµÐ³Ð¾ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹: {len(all_listings)}")
        print()
        print(f"ðŸ“Š ÐšÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ñ…:")
        if all_listings:
            print(f"   ðŸŒ Ð¡ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚Ð°Ð¼Ð¸: {self.stats['with_coords']}/{len(all_listings)} ({self.stats['with_coords']/len(all_listings)*100:.0f}%)")
            print(f"   ðŸ–¼ï¸  Ð¡ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸: {self.stats['with_images']}/{len(all_listings)} ({self.stats['with_images']/len(all_listings)*100:.0f}%)")
        
        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ð¿Ð¾Ð»ÐµÐ¹
        if all_listings:
            with_price = sum(1 for l in all_listings if l.get('price'))
            with_rooms = sum(1 for l in all_listings if l.get('rooms'))
            with_area = sum(1 for l in all_listings if l.get('area_sqm'))
            with_description = sum(1 for l in all_listings if l.get('description'))
            
            print(f"   ðŸ’° Ð¡ Ñ†ÐµÐ½Ð¾Ð¹: {with_price}/{len(all_listings)} ({with_price/len(all_listings)*100:.0f}%)")
            print(f"   ðŸšª Ð¡ ÐºÐ¾Ð¼Ð½Ð°Ñ‚Ð°Ð¼Ð¸: {with_rooms}/{len(all_listings)} ({with_rooms/len(all_listings)*100:.0f}%)")
            print(f"   ðŸ“ Ð¡ Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÑŒÑŽ: {with_area}/{len(all_listings)} ({with_area/len(all_listings)*100:.0f}%)")
            print(f"   ðŸ“ Ð¡ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸ÐµÐ¼: {with_description}/{len(all_listings)} ({with_description/len(all_listings)*100:.0f}%)")
        print()
        print(f"â±ï¸  ÐžÐ±Ñ‰ÐµÐµ Ð²Ñ€ÐµÐ¼Ñ: {elapsed:.1f} ÑÐµÐºÑƒÐ½Ð´")
        if all_listings:
            print(f"âš¡ Ð¡ÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ: {len(all_listings)/elapsed*60:.1f} Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹/Ð¼Ð¸Ð½ÑƒÑ‚Ñƒ")
            print(f"ðŸ“ˆ Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ Ð²Ñ€ÐµÐ¼Ñ: {elapsed/len(all_listings):.2f} ÑÐµÐº/Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ðµ")
        
        return all_listings


async def main():
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--pages', type=int, default=2, help='ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ† ÑÐ¿Ð¸ÑÐºÐ¾Ð²')
    parser.add_argument('--coords', action='store_true', help='ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚Ñ‹ Ñ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†')
    parser.add_argument('--max-coords', type=int, default=20, help='ÐœÐ°ÐºÑÐ¸Ð¼ÑƒÐ¼ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚')
    parser.add_argument('--concurrent', type=int, default=10, help='ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð´Ð»Ñ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚ (default: 10)')
    
    args = parser.parse_args()
    
    scraper = SubitoParallelScraper()
    listings = await scraper.scrape_pages(
        num_pages=args.pages,
        fetch_coords=args.coords,
        max_coords_fetch=args.max_coords,
        coords_concurrent=args.concurrent
    )
    
    if listings:
        output_file = '/tmp/subito_results.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(listings, f, ensure_ascii=False, indent=2)
        
        print(f"\nðŸ’¾ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð² {output_file}")
        
        # ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ñ
        print("\nðŸ“ ÐŸÐ Ð˜ÐœÐ•Ð  ÐžÐ‘ÐªÐ¯Ð’Ð›Ð•ÐÐ˜Ð¯:")
        print("=" * 80)
        first = listings[0]
        for key, value in first.items():
            if key == 'images':
                print(f"   {key}: {len(value)} ÑˆÑ‚")
            elif key == 'description' and len(str(value)) > 100:
                print(f"   {key}: {str(value)[:100]}...")
            else:
                print(f"   {key}: {value}")


if __name__ == "__main__":
    asyncio.run(main())

