#!/usr/bin/env python3
"""
üöÄ –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–ê–†–°–ï–† SUBITO.IT
–ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ __NEXT_DATA__ JSON (–∫–∞–∫ Casa.it)
"""
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from src.core.config import settings
from src.parsers.description_analyzer import DescriptionAnalyzer
import json
import re
from datetime import datetime
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin

class SubitoScraper:
    """–ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–µ—Ä Subito —á–µ—Ä–µ–∑ JSON"""
    
    def __init__(self, enable_geocoding: bool = False, fetch_coords: bool = False):
        self.base_url = "https://www.subito.it"
        # URL —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏: advt=0 (—Ç–æ–ª—å–∫–æ —á–∞—Å—Ç–Ω—ã–µ), bc —É–∫–∞–∑—ã–≤–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
        self.search_url = "https://www.subito.it/annunci-lazio/affitto/immobili/roma/"
        self.api_url = "https://api.scraperapi.com/"
        self.api_key = settings.SCRAPERAPI_KEY
        self.enable_geocoding = enable_geocoding  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º
        self.fetch_coords = fetch_coords  # –ü–∞—Ä—Å–∏—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        
        self.stats = {
            'success': 0,
            'failed': 0,
            'with_coords': 0,
            'with_images': 0,
        }
    
    async def fetch_html(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —á–µ—Ä–µ–∑ ScraperAPI —Å —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–æ–º JavaScript –¥–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏"""
        params = {
            'api_key': self.api_key,
            'url': url,
            'render': 'true',  # –í–ê–ñ–ù–û: Subito —Ç—Ä–µ–±—É–µ—Ç JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –¥–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
        }
        
        try:
            timeout = aiohttp.ClientTimeout(total=120)
            async with session.get(self.api_url, params=params, timeout=timeout) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    print(f"    ‚ùå HTTP {response.status}: {await response.text()}")
                return None
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞: {e}")
            return None
    
    def extract_next_data(self, html: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç __NEXT_DATA__ –∏–∑ HTML"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            script = soup.find('script', id='__NEXT_DATA__')
            
            if script and script.string:
                return json.loads(script.string)
            
            return None
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            return None
    
    def parse_listing_data(self, item_wrapper: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –∏–∑ JSON"""
        try:
            # item_wrapper –∏–º–µ–µ—Ç –∫–ª—é—á–∏: ['before', 'item', 'after', 'kind']
            item = item_wrapper.get('item')
            
            if not item or not isinstance(item, dict):
                return None
            
            # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            external_id = item.get('urn')
            title = item.get('subject')
            
            if not external_id or not title:
                return None
            
            # URL
            url = item.get('urls', {}).get('default', '')
            if not url.startswith('http'):
                url = urljoin(self.base_url, url)
            
            # Features - —ç—Ç–æ dict, –∞ –Ω–µ list!
            features_dict = item.get('features', {})
            
            # –¶–µ–Ω–∞ (values[0] - —ç—Ç–æ dict —Å –∫–ª—é—á–∞–º–∏ 'key' –∏ 'value')
            price = None
            if '/price' in features_dict:
                price_feature = features_dict['/price']
                values = price_feature.get('values', [])
                if values:
                    value_dict = values[0]
                    if isinstance(value_dict, dict):
                        price_str = value_dict.get('key', '')  # key —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Å–ª–æ
                    else:
                        price_str = str(value_dict)
                    
                    try:
                        price = int(price_str)
                    except:
                        pass
            
            # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
            rooms = None
            if '/room' in features_dict:
                room_values = features_dict['/room'].get('values', [])
                if room_values:
                    value_dict = room_values[0]
                    if isinstance(value_dict, dict):
                        room_str = value_dict.get('key', '')
                    else:
                        room_str = str(value_dict)
                    
                    try:
                        rooms = int(re.search(r'\d+', room_str).group())
                    except:
                        pass
            
            area = None
            if '/size' in features_dict:
                size_values = features_dict['/size'].get('values', [])
                if size_values:
                    value_dict = size_values[0]
                    if isinstance(value_dict, dict):
                        size_str = value_dict.get('key', '')
                    else:
                        size_str = str(value_dict)
                    
                    try:
                        area = int(re.search(r'\d+', size_str).group())
                    except:
                        pass
            
            floor = None
            if '/floor' in features_dict:
                floor_values = features_dict['/floor'].get('values', [])
                if floor_values:
                    value_dict = floor_values[0]
                    if isinstance(value_dict, dict):
                        floor = value_dict.get('value', '')
                    else:
                        floor = str(value_dict)
            
            # –¢–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            property_type = 'apartment'
            category = item.get('category', {})
            cat_name = category.get('friendlyName', '').lower()
            
            if 'stanza' in cat_name or 'camera' in cat_name or 'posto-letto' in cat_name:
                property_type = 'room'
            elif 'monolocale' in cat_name:
                property_type = 'studio'
            elif 'villa' in cat_name or 'casa' in cat_name:
                property_type = 'house'
            elif 'appartamenti' in cat_name:
                property_type = 'apartment'
            
            # –¢–∏–ø –æ–±—ä—è–≤–ª–µ–Ω–∏—è (—á–∞—Å—Ç–Ω–æ–µ/–∞–≥–µ–Ω—Ç—Å—Ç–≤–æ)
            advertiser = item.get('advertiser', {})
            advertiser_type = advertiser.get('type')
            is_company = advertiser.get('company', False)
            
            agency_commission = None
            if advertiser_type == 0 or (not is_company and advertiser_type is not None):
                agency_commission = False  # –ß–∞—Å—Ç–Ω–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ - –±–µ–∑ –∫–æ–º–∏—Å—Å–∏–∏
            elif advertiser_type == 1 or is_company:
                agency_commission = True  # –ê–≥–µ–Ω—Ç—Å—Ç–≤–æ - –µ—Å—Ç—å –∫–æ–º–∏—Å—Å–∏—è
            
            # –°–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –∏–∑ features
            renovation_type = None
            building_type = None
            
            if '/buildingcondition' in features_dict:
                bc_values = features_dict['/buildingcondition'].get('values', [])
                if bc_values:
                    bc_value_dict = bc_values[0]
                    if isinstance(bc_value_dict, dict):
                        bc_code = bc_value_dict.get('key', '')
                        bc_label = bc_value_dict.get('value', '').lower()
                        
                        # –ú–∞–ø–ø–∏–Ω–≥ –Ω–∞ –Ω–∞—à–∏ —Ç–∏–ø—ã —Ä–µ–º–æ–Ω—Ç–∞
                        if 'nuova costruzione' in bc_label or bc_code == '10':
                            building_type = 'new_construction'
                            renovation_type = 'renovated'  # –ù–æ–≤–∞—è –ø–æ—Å—Ç—Ä–æ–π–∫–∞ = –æ—Ç—Ä–µ–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è
                        elif 'ottimo' in bc_label or 'ristrutturato' in bc_label or bc_code == '20':
                            renovation_type = 'renovated'
                        elif 'buono' in bc_label or 'abitabile' in bc_label or bc_code == '30':
                            renovation_type = 'partially_renovated'
                        elif 'da ristrutturare' in bc_label or bc_code == '40':
                            renovation_type = 'not_renovated'
            
            # –ì–µ–æ–ª–æ–∫–∞—Ü–∏—è
            geo = item.get('geo', {})
            map_data = geo.get('map', {})
            
            # –í —Å–ø–∏—Å–∫–∞—Ö Subito –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
            latitude = map_data.get('lat') if map_data else None
            longitude = map_data.get('lng') if map_data else None
            address = map_data.get('address', '') if map_data else None
            
            # –†–∞–π–æ–Ω
            town = geo.get('town', {}).get('value', '')
            
            # –ï—Å–ª–∏ –Ω–µ—Ç –∞–¥—Ä–µ—Å–∞, –±–µ—Ä–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞
            if not address:
                city_value = geo.get('city', {}).get('value', '')
                if city_value:
                    address = city_value
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–≤ JSON —Ç–æ–ª—å–∫–æ baseUrl, –Ω—É–∂–Ω–æ –¥–æ–ø–æ–ª–Ω—è—Ç—å)
            images = []
            gallery = item.get('images', [])
            for img_item in gallery[:20]:  # –ú–∞–∫—Å–∏–º—É–º 20
                base_url = img_item.get('cdnBaseUrl') or img_item.get('url')
                if base_url:
                    # –î–ª—è Subito –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞–∑–º–µ—Ä–∞
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º card-mobile-large-1x-auto (—Ä–∞–±–æ—Ç–∞—é—â–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä)
                    if 'sbito.it' in base_url and 'rule=' not in base_url:
                        img_url = f"{base_url}?rule=card-mobile-large-1x-auto"
                    else:
                        img_url = base_url
                    
                    if img_url not in images:
                        images.append(img_url)
            
            # –û–ø–∏—Å–∞–Ω–∏–µ
            description = item.get('body', '')
            
            # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            published_at = item.get('date')
            
            listing_data = {
                'external_id': f"subito_{external_id}",
                'source': 'subito',
                'url': url,
                'title': title,
                'description': description,
                'price': price,
                'property_type': property_type,
                'rooms': rooms,
                'area': area,
                'floor': floor,
                'bathrooms': None,  # Subito –æ–±—ã—á–Ω–æ –Ω–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç
                'latitude': float(latitude) if latitude else None,
                'longitude': float(longitude) if longitude else None,
                'address': address or town,
                'city': 'Roma',
                'district': town,
                'images': images,
                'published_at': published_at,
                'scraped_at': datetime.utcnow().isoformat()
            }
            
            # –ê–Ω–∞–ª–∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä–æ–≤
            if description:
                analysis = DescriptionAnalyzer.analyze(description, floor=floor)
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Subito API –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ –æ–ø–∏—Å–∞–Ω–∏—è
                listing_data['agency_commission'] = agency_commission if agency_commission is not None else analysis.get('agency_commission')
                listing_data['renovation_type'] = renovation_type if renovation_type else analysis.get('renovation_type')
                listing_data['building_type'] = building_type if building_type else analysis.get('building_type')
                
                # –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª—è –±–µ—Ä–µ–º –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ –æ–ø–∏—Å–∞–Ω–∏—è
                listing_data['pets_allowed'] = analysis.get('pets_allowed')
                listing_data['children_friendly'] = analysis.get('children_friendly')
                listing_data['year_built'] = analysis.get('year_built')
                listing_data['total_floors'] = analysis.get('total_floors')
                listing_data['floor_number'] = analysis.get('floor_number')
                listing_data['is_first_floor'] = analysis.get('is_first_floor')
                listing_data['is_top_floor'] = analysis.get('is_top_floor')
                listing_data['park_nearby'] = analysis.get('park_nearby')
                listing_data['noisy_roads_nearby'] = analysis.get('noisy_roads_nearby')
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –∏–∑ API
                listing_data['agency_commission'] = agency_commission
                listing_data['renovation_type'] = renovation_type
                listing_data['building_type'] = building_type
                listing_data['pets_allowed'] = None
                listing_data['children_friendly'] = None
                listing_data['year_built'] = None
                listing_data['total_floors'] = None
                listing_data['floor_number'] = None
                listing_data['is_first_floor'] = None
                listing_data['is_top_floor'] = None
                listing_data['park_nearby'] = None
                listing_data['noisy_roads_nearby'] = None
            
            return listing_data
            
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
            return None
    
    def parse_page(self, html: str) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        next_data = self.extract_next_data(html)
        
        if not next_data:
            return []
        
        try:
            items_list = next_data['props']['pageProps']['initialState']['items']['list']
            
            listings = []
            for item_wrapper in items_list:
                listing = self.parse_listing_data(item_wrapper)
                if listing:
                    listings.append(listing)
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                    if listing.get('latitude'):
                        self.stats['with_coords'] += 1
                    if listing.get('images'):
                        self.stats['with_images'] += 1
            
            return listings
            
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
            return []
    
    def parse_detail_page_for_coords(self, html: str) -> Optional[tuple]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            next_data = self.extract_next_data(html)
            if not next_data:
                return None
            
            # –ù–∞–≤–∏–≥–∞—Ü–∏—è: props.pageProps.ad.geo.map
            ad_data = next_data.get('props', {}).get('pageProps', {}).get('ad', {})
            geo = ad_data.get('geo', {})
            map_data = geo.get('map', {})
            
            latitude = map_data.get('latitude')
            longitude = map_data.get('longitude')
            
            if latitude and longitude:
                return float(latitude), float(longitude)
            
            return None
        except Exception as e:
            return None
    
    async def scrape_multiple_pages(self, max_pages: int = 5):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º (—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å)"""
        return await self.scrape_pages(num_pages=max_pages, fetch_coords=self.fetch_coords)
    
    async def scrape_pages(self, num_pages: int = 2, fetch_coords: bool = False, max_coords_fetch: int = 20, coords_concurrent: int = 10):
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü"""
        print("=" * 80)
        print(f"üöÄ –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì SUBITO.IT")
        print("=" * 80)
        print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü —Å–ø–∏—Å–∫–æ–≤: {num_pages}")
        if fetch_coords:
            print(f"üåç –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç: –¥–æ {max_coords_fetch} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            print(f"‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {coords_concurrent}")
        print("=" * 80)
        
        start_time = datetime.utcnow()
        
        async with aiohttp.ClientSession() as session:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º URLs –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü
            page_urls = []
            for page_num in range(1, num_pages + 1):
                if page_num == 1:
                    page_urls.append(self.search_url)
                else:
                    page_urls.append(f"{self.search_url}?o={page_num}")
            
            print(f"\nüìã –ó–∞–≥—Ä—É–∑–∫–∞ {len(page_urls)} —Å—Ç—Ä–∞–Ω–∏—Ü...")
            print("-" * 80)
            
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
            tasks = [self.fetch_html(session, url) for url in page_urls]
            htmls = await asyncio.gather(*tasks)
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            all_listings = []
            seen_ids = set()  # –î–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
            
            for i, html in enumerate(htmls, 1):
                if html:
                    print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {i}: {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
                    listings = self.parse_page(html)
                    print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
                    
                    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è (–≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π)
                    unique_listings = []
                    for listing in listings:
                        external_id = listing.get('external_id')
                        if external_id and external_id not in seen_ids:
                            seen_ids.add(external_id)
                            unique_listings.append(listing)
                    
                    if len(unique_listings) < len(listings):
                        duplicates = len(listings) - len(unique_listings)
                        print(f"    ‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ {duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (–ø—Ä–æ–±–ª–µ–º–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ ScraperAPI)")
                    
                    all_listings.extend(unique_listings)
                    self.stats['success'] += 1
                else:
                    print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {i}: ‚ùå –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                    self.stats['failed'] += 1
            
            # –≠–¢–ê–ü 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç —Å –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
            if fetch_coords and all_listings:
                print("\n" + "=" * 80)
                print(f"üåç –≠–¢–ê–ü 2: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç")
                print("=" * 80)
                
                listings_to_fetch = all_listings[:max_coords_fetch]
                print(f"üìç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(listings_to_fetch)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ...")
                
                # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
                semaphore = asyncio.Semaphore(coords_concurrent)
                
                async def fetch_and_parse_coords(listing, index):
                    async with semaphore:
                        detail_html = await self.fetch_html(session, listing['url'])
                        
                        if detail_html:
                            coords = self.parse_detail_page_for_coords(detail_html)
                            
                            if coords:
                                listing['latitude'], listing['longitude'] = coords
                                self.stats['with_coords'] += 1
                                print(f"[{index}/{len(listings_to_fetch)}] ‚úÖ {listing['title'][:40]}... ‚Üí {coords[0]:.6f}, {coords[1]:.6f}")
                                return True
                            else:
                                print(f"[{index}/{len(listings_to_fetch)}] ‚ö†Ô∏è {listing['title'][:40]}... ‚Üí –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                                return False
                        else:
                            print(f"[{index}/{len(listings_to_fetch)}] ‚ùå {listing['title'][:40]}... ‚Üí –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                            return False
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                tasks = [fetch_and_parse_coords(listing, i) for i, listing in enumerate(listings_to_fetch, 1)]
                await asyncio.gather(*tasks)
        
        end_time = datetime.utcnow()
        elapsed = (end_time - start_time).total_seconds()
        
        # –ò–¢–û–ì–ò
        print("\n" + "=" * 80)
        print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("=" * 80)
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {self.stats['success']}")
        print(f"‚ùå –û—à–∏–±–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏: {self.stats['failed']}")
        print(f"üì¶ –í—Å–µ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(all_listings)}")
        print()
        print(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:")
        if all_listings:
            print(f"   üåç –° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {self.stats['with_coords']}/{len(all_listings)} ({self.stats['with_coords']/len(all_listings)*100:.0f}%)")
            print(f"   üñºÔ∏è  –° –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏: {self.stats['with_images']}/{len(all_listings)} ({self.stats['with_images']/len(all_listings)*100:.0f}%)")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥—Ä—É–≥–∏—Ö –ø–æ–ª–µ–π
        if all_listings:
            with_price = sum(1 for l in all_listings if l.get('price'))
            with_rooms = sum(1 for l in all_listings if l.get('rooms'))
            with_area = sum(1 for l in all_listings if l.get('area_sqm'))
            with_description = sum(1 for l in all_listings if l.get('description'))
            
            print(f"   üí∞ –° —Ü–µ–Ω–æ–π: {with_price}/{len(all_listings)} ({with_price/len(all_listings)*100:.0f}%)")
            print(f"   üö™ –° –∫–æ–º–Ω–∞—Ç–∞–º–∏: {with_rooms}/{len(all_listings)} ({with_rooms/len(all_listings)*100:.0f}%)")
            print(f"   üìê –° –ø–ª–æ—â–∞–¥—å—é: {with_area}/{len(all_listings)} ({with_area/len(all_listings)*100:.0f}%)")
            print(f"   üìù –° –æ–ø–∏—Å–∞–Ω–∏–µ–º: {with_description}/{len(all_listings)} ({with_description/len(all_listings)*100:.0f}%)")
        print()
        print(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {elapsed:.1f} —Å–µ–∫—É–Ω–¥")
        if all_listings:
            print(f"‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {len(all_listings)/elapsed*60:.1f} –æ–±—ä—è–≤–ª–µ–Ω–∏–π/–º–∏–Ω—É—Ç—É")
            print(f"üìà –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {elapsed/len(all_listings):.2f} —Å–µ–∫/–æ–±—ä—è–≤–ª–µ–Ω–∏–µ")
        
        return all_listings


async def main():
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--pages', type=int, default=2, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü —Å–ø–∏—Å–∫–æ–≤')
    parser.add_argument('--coords', action='store_true', help='–ü–æ–ª—É—á–∏—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü')
    parser.add_argument('--max-coords', type=int, default=20, help='–ú–∞–∫—Å–∏–º—É–º –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç')
    parser.add_argument('--concurrent', type=int, default=10, help='–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (default: 10)')
    
    args = parser.parse_args()
    
    scraper = SubitoScraper()
    listings = await scraper.scrape_pages(
        num_pages=args.pages,
        fetch_coords=args.coords,
        max_coords_fetch=args.max_coords,
        coords_concurrent=args.concurrent
    )
    
    if listings:
        output_file = '/tmp/subito_results.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(listings, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
        
        # –ü—Ä–∏–º–µ—Ä –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        print("\nüìù –ü–†–ò–ú–ï–† –û–ë–™–Ø–í–õ–ï–ù–ò–Ø:")
        print("=" * 80)
        first = listings[0]
        for key, value in first.items():
            if key == 'images':
                print(f"   {key}: {len(value)} —à—Ç")
            elif key == 'description' and len(str(value)) > 100:
                print(f"   {key}: {str(value)[:100]}...")
            else:
                print(f"   {key}: {value}")


if __name__ == "__main__":
    asyncio.run(main())

