#!/usr/bin/env python3
"""
üß™ –¢–ï–°–¢–û–í–´–ô –°–ö–†–ò–ü–¢ –î–õ–Ø –ü–†–ê–í–ò–õ–¨–ù–û–ì–û –ê–°–ò–ù–•–†–û–ù–ù–û–ì–û –ü–ê–†–°–ï–†–ê
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–µ ScraperAPI

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python src/parsers/test_async_scraper.py [–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_—Å—Ç—Ä–∞–Ω–∏—Ü]
    
–ü—Ä–∏–º–µ—Ä—ã:
    python src/parsers/test_async_scraper.py 2    # –¢–µ—Å—Ç 2 —Å—Ç—Ä–∞–Ω–∏—Ü
    python src/parsers/test_async_scraper.py 1    # –¢–µ—Å—Ç 1 —Å—Ç—Ä–∞–Ω–∏—Ü—ã
"""
import sys
import os
import asyncio
import logging
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from src.parsers.immobiliare_parser_async import ImmobiliareAsyncParser
from src.core.config import settings

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'async_scraper_test_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)

logger = logging.getLogger(__name__)


async def test_single_job_submission():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ—Ç–ø—Ä–∞–≤–∫—É –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏ –≤ ScraperAPI Async"""
    logger.info("üß™ –¢–ï–°–¢ 1: –û—Ç–ø—Ä–∞–≤–∫–∞ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏ –≤ ScraperAPI Async")
    
    parser = ImmobiliareAsyncParser()
    url = "https://www.immobiliare.it/affitto-case/roma/"
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É
    job_data = await parser.submit_scraping_job(url, {"test": "single_job"})
    
    if job_data:
        logger.info(f"‚úÖ –ó–∞–¥–∞—á–∞ —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ:")
        logger.info(f"   üÜî Job ID: {job_data.get('id')}")
        logger.info(f"   üìä –°—Ç–∞—Ç—É—Å: {job_data.get('status')}")
        logger.info(f"   üîó Status URL: {job_data.get('statusUrl')}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å
        status_url = job_data.get('statusUrl')
        if status_url:
            result = await parser.poll_job_status(status_url, max_wait_time=120)
            if result:
                response_data = result.get('response', {})
                html_size = len(response_data.get('body', ''))
                status_code = response_data.get('statusCode')
                
                logger.info(f"‚úÖ –ó–∞–¥–∞—á–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
                logger.info(f"   üìÑ HTML —Ä–∞–∑–º–µ—Ä: {html_size} —Å–∏–º–≤–æ–ª–æ–≤")
                logger.info(f"   üî¢ –°—Ç–∞—Ç—É—Å –∫–æ–¥: {status_code}")
                
                return True
            else:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                return False
        else:
            logger.error("‚ùå –ù–µ—Ç status URL")
            return False
    else:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∑–∞–¥–∞—á—É")
        return False


async def test_batch_scraping(max_pages: int = 2):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –±–∞—Ç—á —Å–∫—Ä–∞–ø–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü"""
    logger.info(f"üß™ –¢–ï–°–¢ 2: –ë–∞—Ç—á —Å–∫—Ä–∞–ø–∏–Ω–≥ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
    
    parser = ImmobiliareAsyncParser(enable_geocoding=True)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
    start_time = asyncio.get_event_loop().time()
    listings = await parser.scrape_listings(max_pages=max_pages)
    end_time = asyncio.get_event_loop().time()
    
    elapsed_time = end_time - start_time
    
    logger.info(f"üéâ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ë–ê–¢–ß –°–ö–†–ê–ü–ò–ù–ì–ê:")
    logger.info(f"   ‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed_time:.1f}—Å")
    logger.info(f"   üìã –ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
    
    if listings:
        # –ê–Ω–∞–ª–∏–∑ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
        photo_counts = [len(listing.get('images', [])) for listing in listings]
        avg_photos = sum(photo_counts) / len(photo_counts)
        max_photos = max(photo_counts)
        min_photos = min(photo_counts)
        
        logger.info(f"   üì∏ –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏:")
        logger.info(f"      üìä –°—Ä–µ–¥–Ω–µ–µ: {avg_photos:.1f}")
        logger.info(f"      üìà –ú–∞–∫—Å–∏–º—É–º: {max_photos}")
        logger.info(f"      üìâ –ú–∏–Ω–∏–º—É–º: {min_photos}")
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
        property_types = {}
        for listing in listings:
            prop_type = listing.get('property_type', 'unknown')
            property_types[prop_type] = property_types.get(prop_type, 0) + 1
        
        logger.info(f"   üè† –¢–∏–ø—ã –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏:")
        for prop_type, count in property_types.items():
            logger.info(f"      {prop_type}: {count}")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        with_coords = sum(1 for listing in listings if listing.get('latitude') and listing.get('longitude'))
        coord_percentage = (with_coords / len(listings)) * 100
        
        logger.info(f"   üó∫Ô∏è –ì–µ–æ–ª–æ–∫–∞—Ü–∏—è:")
        logger.info(f"      –° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {with_coords}/{len(listings)} ({coord_percentage:.1f}%)")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        logger.info(f"   üìù –ü—Ä–∏–º–µ—Ä—ã –æ–±—ä—è–≤–ª–µ–Ω–∏–π:")
        for i, listing in enumerate(listings[:3], 1):
            logger.info(f"      {i}. {listing.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]}...")
            logger.info(f"         üí∞ {listing.get('price', 'N/A')}‚Ç¨ | üè† {listing.get('property_type', 'N/A')} | üì∏ {len(listing.get('images', []))} —Ñ–æ—Ç–æ")
        
        return True
    else:
        logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
        return False


async def test_api_parameters():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å API –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    logger.info("üß™ –¢–ï–°–¢ 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ API –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    
    parser = ImmobiliareAsyncParser()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–ø—Ä–æ—Å–∞
    test_payload = {
        "apiKey": "test_key",
        "url": "https://example.com",
        "apiParams": {
            "render": True,
            "premium": True,
            "country_code": "it",
            "device_type": "desktop",
            "autoparse": False,
            "retry_404": True,
            "follow_redirect": True
        },
        "meta": {"test": True}
    }
    
    logger.info("‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ ScraperAPI:")
    logger.info(f"   üìã Payload: {test_payload}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL
    assert parser.async_api_url == "https://async.scraperapi.com/jobs"
    logger.info(f"   üîó –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π URL: {parser.async_api_url}")
    
    return True


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    logger.info("üöÄ –ù–ê–ß–ò–ù–ê–ï–ú –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–†–ê–í–ò–õ–¨–ù–û–ì–û –ê–°–ò–ù–•–†–û–ù–ù–û–ì–û –ü–ê–†–°–ï–†–ê")
    logger.info(f"üîë SCRAPERAPI_KEY –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {'‚úÖ' if settings.SCRAPERAPI_KEY else '‚ùå'}")
    
    if not settings.SCRAPERAPI_KEY:
        logger.error("‚ùå SCRAPERAPI_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –≤ .env —Ñ–∞–π–ª–µ")
        return
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    max_pages = 2
    if len(sys.argv) > 1:
        try:
            max_pages = int(sys.argv[1])
        except ValueError:
            logger.warning("‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º 2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
    
    logger.info(f"üìÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏")
    
    tests_results = []
    
    # –¢–µ—Å—Ç 1: –û—Ç–ø—Ä–∞–≤–∫–∞ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏
    try:
        result1 = await test_single_job_submission()
        tests_results.append(("–û—Ç–ø—Ä–∞–≤–∫–∞ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏", result1))
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ 1: {e}")
        tests_results.append(("–û—Ç–ø—Ä–∞–≤–∫–∞ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏", False))
    
    # –¢–µ—Å—Ç 2: –ë–∞—Ç—á —Å–∫—Ä–∞–ø–∏–Ω–≥
    try:
        result2 = await test_batch_scraping(max_pages)
        tests_results.append(("–ë–∞—Ç—á —Å–∫—Ä–∞–ø–∏–Ω–≥", result2))
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ 2: {e}")
        tests_results.append(("–ë–∞—Ç—á —Å–∫—Ä–∞–ø–∏–Ω–≥", False))
    
    # –¢–µ—Å—Ç 3: API –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    try:
        result3 = await test_api_parameters()
        tests_results.append(("API –ø–∞—Ä–∞–º–µ—Ç—Ä—ã", result3))
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ 3: {e}")
        tests_results.append(("API –ø–∞—Ä–∞–º–µ—Ç—Ä—ã", False))
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    logger.info("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
    passed = 0
    for test_name, result in tests_results:
        status = "‚úÖ –ü–†–û–®–ï–õ" if result else "‚ùå –ü–†–û–í–ê–õ–ï–ù"
        logger.info(f"   {status}: {test_name}")
        if result:
            passed += 1
    
    total_tests = len(tests_results)
    logger.info(f"üìà –†–µ–∑—É–ª—å—Ç–∞—Ç: {passed}/{total_tests} —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ—à–ª–∏ ({(passed/total_tests)*100:.1f}%)")
    
    if passed == total_tests:
        logger.info("üéâ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–®–õ–ò! –ü–∞—Ä—Å–µ—Ä –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.")
    else:
        logger.warning("‚ö†Ô∏è –ù–ï–ö–û–¢–û–†–´–ï –¢–ï–°–¢–´ –ü–†–û–í–ê–õ–ò–õ–ò–°–¨. –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞.")


if __name__ == "__main__":
    asyncio.run(main()) 