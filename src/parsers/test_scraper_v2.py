#!/usr/bin/env python3
"""
üß™ –¢–ï–°–¢–û–í–´–ô –°–ö–†–ò–ü–¢ –î–õ–Ø –ù–û–í–û–ì–û –ê–°–ò–ù–•–†–û–ù–ù–û–ì–û –°–ö–†–ê–ü–ï–†–ê V2
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python src/parsers/test_scraper_v2.py [–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_—Å—Ç—Ä–∞–Ω–∏—Ü]
"""
import sys
import os
import asyncio
import logging
import json
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from src.parsers.immobiliare_async_scraper_v2 import ImmobiliareAsyncScraperV2
from src.core.config import settings

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'scraper_v2_test_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)

logger = logging.getLogger(__name__)


async def test_scraper_v2():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–µ—Ä V2"""
    logger.info("üöÄ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–û–í–û–ì–û –ê–°–ò–ù–•–†–û–ù–ù–û–ì–û –°–ö–†–ê–ü–ï–†–ê V2")
    logger.info(f"üîë SCRAPERAPI_KEY –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {'‚úÖ' if settings.SCRAPERAPI_KEY else '‚ùå'}")
    
    if not settings.SCRAPERAPI_KEY:
        logger.error("‚ùå SCRAPERAPI_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –≤ .env —Ñ–∞–π–ª–µ")
        return False
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    max_pages = 5
    if len(sys.argv) > 1:
        try:
            max_pages = int(sys.argv[1])
        except ValueError:
            logger.warning("‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º 5 —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
    
    logger.info(f"üìÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö...")
    
    # –°–æ–∑–¥–∞–µ–º —Å–∫—Ä–∞–ø–µ—Ä —Å –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    scraper = ImmobiliareAsyncScraperV2(enable_geocoding=True)
    
    start_time = asyncio.get_event_loop().time()
    
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥
        listings = await scraper.scrape_multiple_pages(max_pages=max_pages)
        
        end_time = asyncio.get_event_loop().time()
        elapsed_time = end_time - start_time
        
        if not listings:
            logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
            return False
        
        # –ü–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        logger.info("üìä –ü–û–î–†–û–ë–ù–´–ô –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
        logger.info(f"   ‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {elapsed_time:.1f}—Å")
        logger.info(f"   üìã –ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
        logger.info(f"   ‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {len(listings)/elapsed_time:.2f} –æ–±—ä—è–≤–ª–µ–Ω–∏–π/—Å–µ–∫")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ external_id
        external_ids = [listing.get('external_id') for listing in listings if listing.get('external_id')]
        unique_ids = set(external_ids)
        
        logger.info(f"   üîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏:")
        logger.info(f"      –í—Å–µ–≥–æ external_id: {len(external_ids)}")
        logger.info(f"      –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö external_id: {len(unique_ids)}")
        logger.info(f"      –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(external_ids) - len(unique_ids)}")
        
        if len(external_ids) == len(unique_ids):
            logger.info("   ‚úÖ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –∏–¥–µ–∞–ª—å–Ω–æ!")
        else:
            logger.warning("   ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã!")
        
        # –ê–Ω–∞–ª–∏–∑ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
        photo_counts = [len(listing.get('images', [])) for listing in listings]
        if photo_counts:
            avg_photos = sum(photo_counts) / len(photo_counts)
            max_photos = max(photo_counts)
            min_photos = min(photo_counts)
            total_photos = sum(photo_counts)
            
            logger.info(f"   üì∏ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π:")
            logger.info(f"      üìä –°—Ä–µ–¥–Ω–µ–µ: {avg_photos:.1f} —Ñ–æ—Ç–æ/–æ–±—ä—è–≤–ª–µ–Ω–∏–µ")
            logger.info(f"      üìà –ú–∞–∫—Å–∏–º—É–º: {max_photos} —Ñ–æ—Ç–æ")
            logger.info(f"      üìâ –ú–∏–Ω–∏–º—É–º: {min_photos} —Ñ–æ—Ç–æ")
            logger.info(f"      üéØ –í—Å–µ–≥–æ —Ñ–æ—Ç–æ: {total_photos}")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        with_coords = sum(1 for listing in listings if listing.get('latitude') and listing.get('longitude'))
        coord_percentage = (with_coords / len(listings)) * 100
        
        logger.info(f"   üó∫Ô∏è –ì–µ–æ–ª–æ–∫–∞—Ü–∏—è:")
        logger.info(f"      –° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {with_coords}/{len(listings)} ({coord_percentage:.1f}%)")
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
        property_types = {}
        for listing in listings:
            prop_type = listing.get('property_type', 'unknown')
            property_types[prop_type] = property_types.get(prop_type, 0) + 1
        
        logger.info(f"   üè† –¢–∏–ø—ã –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏:")
        for prop_type, count in sorted(property_types.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(listings)) * 100
            logger.info(f"      {prop_type}: {count} ({percentage:.1f}%)")
        
        # –ê–Ω–∞–ª–∏–∑ —Ü–µ–Ω
        prices = [listing.get('price') for listing in listings if listing.get('price')]
        if prices:
            avg_price = sum(prices) / len(prices)
            min_price = min(prices)
            max_price = max(prices)
            
            logger.info(f"   üí∞ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü–µ–Ω:")
            logger.info(f"      üìä –°—Ä–µ–¥–Ω—è—è: {avg_price:.0f}‚Ç¨")
            logger.info(f"      üìâ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è: {min_price}‚Ç¨")
            logger.info(f"      üìà –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è: {max_price}‚Ç¨")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–ª–æ—â–∞–¥–∏
        areas = [listing.get('area') for listing in listings if listing.get('area')]
        if areas:
            avg_area = sum(areas) / len(areas)
            min_area = min(areas)
            max_area = max(areas)
            
            logger.info(f"   üìê –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–ª–æ—â–∞–¥–∏:")
            logger.info(f"      üìä –°—Ä–µ–¥–Ω—è—è: {avg_area:.0f}–º¬≤")
            logger.info(f"      üìâ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è: {min_area}–º¬≤")
            logger.info(f"      üìà –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è: {max_area}–º¬≤")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        logger.info(f"   üìù –ü—Ä–∏–º–µ—Ä—ã –æ–±—ä—è–≤–ª–µ–Ω–∏–π:")
        for i, listing in enumerate(listings[:5], 1):
            title = listing.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:60]
            price = listing.get('price', 'N/A')
            prop_type = listing.get('property_type', 'N/A')
            photos = len(listing.get('images', []))
            coords = '‚úÖ' if listing.get('latitude') and listing.get('longitude') else '‚ùå'
            external_id = listing.get('external_id', 'N/A')
            
            logger.info(f"      {i}. {title}...")
            logger.info(f"         üí∞ {price}‚Ç¨ | üè† {prop_type} | üì∏ {photos} —Ñ–æ—Ç–æ | üó∫Ô∏è {coords} | üÜî {external_id}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ external_id (—á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–Ω—ã–µ)
        if len(unique_ids) >= max_pages * 15:  # –û–∂–∏–¥–∞–µ–º –º–∏–Ω–∏–º—É–º 15 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
            logger.info("   ‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ–¥–µ—Ä–∂–∞—Ç —Ä–∞–∑–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è!")
        else:
            logger.warning(f"   ‚ö†Ô∏è –ú–∞–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–ª—è {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON —Ñ–∞–π–ª
        output_file = f"scraper_v2_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        output_data = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "scraper_version": "ImmobiliareAsyncScraperV2",
                "total_listings": len(listings),
                "unique_listings": len(unique_ids),
                "pages_scraped": max_pages,
                "elapsed_time_seconds": elapsed_time,
                "statistics": {
                    "photos": {
                        "average": avg_photos if photo_counts else 0,
                        "total": sum(photo_counts) if photo_counts else 0,
                        "max": max(photo_counts) if photo_counts else 0,
                        "min": min(photo_counts) if photo_counts else 0
                    },
                    "coordinates": {
                        "with_coords": with_coords,
                        "percentage": coord_percentage
                    },
                    "property_types": property_types,
                    "prices": {
                        "average": avg_price if prices else 0,
                        "min": min_price if prices else 0,
                        "max": max_price if prices else 0,
                        "count": len(prices)
                    } if prices else None,
                    "areas": {
                        "average": avg_area if areas else 0,
                        "min": min_area if areas else 0,
                        "max": max_area if areas else 0,
                        "count": len(areas)
                    } if areas else None
                }
            },
            "listings": listings
        }
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_file}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        
        # –û—Ü–µ–Ω–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        success_criteria = [
            len(listings) >= max_pages * 10,  # –ú–∏–Ω–∏–º—É–º 10 –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
            len(unique_ids) == len(external_ids),  # –ù–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            coord_percentage >= 70,  # –ú–∏–Ω–∏–º—É–º 70% —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏
            avg_photos >= 10 if photo_counts else False,  # –ú–∏–Ω–∏–º—É–º 10 —Ñ–æ—Ç–æ –≤ —Å—Ä–µ–¥–Ω–µ–º
            len(property_types) >= 2,  # –ú–∏–Ω–∏–º—É–º 2 —Ç–∏–ø–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
            elapsed_time <= max_pages * 30  # –ú–∞–∫—Å–∏–º—É–º 30 —Å–µ–∫—É–Ω–¥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        ]
        
        passed_criteria = sum(success_criteria)
        logger.info(f"üìà –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏: {passed_criteria}/{len(success_criteria)}")
        
        if passed_criteria >= 5:
            logger.info("üéâ –¢–ï–°–¢ –£–°–ü–ï–®–ù–û –ü–†–û–ô–î–ï–ù! –°–∫—Ä–∞–ø–µ—Ä V2 —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ.")
            return True
        elif passed_criteria >= 4:
            logger.info("‚úÖ –¢–ï–°–¢ –í –û–°–ù–û–í–ù–û–ú –ü–†–û–ô–î–ï–ù. –°–∫—Ä–∞–ø–µ—Ä V2 —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ.")
            return True
        else:
            logger.warning("‚ö†Ô∏è –¢–ï–°–¢ –ù–ï –ü–†–û–ô–î–ï–ù. –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞.")
            return False
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    result = await test_scraper_v2()
    
    if result:
        logger.info("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∫—Ä–∞–ø–µ—Ä–∞ V2 –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        sys.exit(0)
    else:
        logger.error("‚ùå –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∫—Ä–∞–ø–µ—Ä–∞ V2 –ø—Ä–æ–≤–∞–ª–µ–Ω–æ!")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main()) 