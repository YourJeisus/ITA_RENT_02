#!/usr/bin/env python3
"""
üß™ –ë–ê–ó–û–í–´–ô –¢–ï–°–¢ SCRAPERAPI
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å ScraperAPI –Ω–∞ –ø—Ä–æ—Å—Ç—ã—Ö —Å–∞–π—Ç–∞—Ö

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python src/parsers/test_scraperapi_basic.py
"""
import sys
import os
import asyncio
import aiohttp
import logging
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from src.core.config import settings

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'scraperapi_basic_test_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)

logger = logging.getLogger(__name__)


async def test_scraperapi_basic():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç ScraperAPI –Ω–∞ –ø—Ä–æ—Å—Ç—ã—Ö —Å–∞–π—Ç–∞—Ö"""
    logger.info("üöÄ –ë–ê–ó–û–í–´–ô –¢–ï–°–¢ SCRAPERAPI")
    logger.info(f"üîë SCRAPERAPI_KEY –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {'‚úÖ' if settings.SCRAPERAPI_KEY else '‚ùå'}")
    
    if not settings.SCRAPERAPI_KEY:
        logger.error("‚ùå SCRAPERAPI_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω!")
        return False
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Å–∞–π—Ç—ã
    test_sites = [
        {
            "name": "HTTPBin (–ø—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç)",
            "url": "https://httpbin.org/html",
            "expected_content": "<h1>Herman Melville - Moby-Dick</h1>"
        },
        {
            "name": "Example.com",
            "url": "https://example.com",
            "expected_content": "Example Domain"
        },
        {
            "name": "Google (–≥–ª–∞–≤–Ω–∞—è)",
            "url": "https://google.com",
            "expected_content": "google"
        }
    ]
    
    sync_api_url = "https://api.scraperapi.com"
    timeout = aiohttp.ClientTimeout(total=30)
    
    successful_tests = 0
    
    for i, test_site in enumerate(test_sites, 1):
        logger.info(f"üß™ –¢–µ—Å—Ç {i}/{len(test_sites)}: {test_site['name']}")
        
        params = {
            'api_key': settings.SCRAPERAPI_KEY,
            'url': test_site['url'],
            'render': 'false'  # –ë–µ–∑ JS –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        }
        
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(sync_api_url, params=params) as response:
                    logger.info(f"   üìä –°—Ç–∞—Ç—É—Å: {response.status}")
                    
                    if response.status == 200:
                        content = await response.text()
                        logger.info(f"   üìÑ –†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                        
                        if test_site['expected_content'].lower() in content.lower():
                            logger.info(f"   ‚úÖ –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω: {test_site['expected_content'][:30]}...")
                            successful_tests += 1
                        else:
                            logger.warning(f"   ‚ö†Ô∏è –û–∂–∏–¥–∞–µ–º—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
                            logger.debug(f"   –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤: {content[:200]}")
                    else:
                        logger.error(f"   ‚ùå HTTP –æ—à–∏–±–∫–∞: {response.status}")
                        content = await response.text()
                        logger.debug(f"   –û—Ç–≤–µ—Ç: {content[:200]}")
                        
        except Exception as e:
            logger.error(f"   ‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–µ—Å—Ç–∞–º–∏
        if i < len(test_sites):
            await asyncio.sleep(2)
    
    logger.info(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ë–ê–ó–û–í–û–ì–û –¢–ï–°–¢–ê:")
    logger.info(f"   ‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {successful_tests}/{len(test_sites)}")
    
    if successful_tests >= 2:
        logger.info("üéâ ScraperAPI —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ!")
        return True
    elif successful_tests >= 1:
        logger.warning("‚ö†Ô∏è ScraperAPI —Ä–∞–±–æ—Ç–∞–µ—Ç —á–∞—Å—Ç–∏—á–Ω–æ")
        return True
    else:
        logger.error("‚ùå ScraperAPI –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç!")
        return False


async def test_immobiliare_alternatives():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Immobiliare.it"""
    logger.info("üè† –¢–ï–°–¢ –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–´–• –ü–ê–†–ê–ú–ï–¢–†–û–í –î–õ–Ø IMMOBILIARE.IT")
    
    base_url = "https://www.immobiliare.it/affitto-case/roma/"
    sync_api_url = "https://api.scraperapi.com"
    timeout = aiohttp.ClientTimeout(total=60)
    
    # –†–∞–∑–ª–∏—á–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    param_sets = [
        {
            "name": "–ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã",
            "params": {
                'api_key': settings.SCRAPERAPI_KEY,
                'url': base_url,
                'render': 'false'
            }
        },
        {
            "name": "–° —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–æ–º JS",
            "params": {
                'api_key': settings.SCRAPERAPI_KEY,
                'url': base_url,
                'render': 'true'
            }
        },
        {
            "name": "–° –ø—Ä–µ–º–∏—É–º –ø—Ä–æ–∫—Å–∏",
            "params": {
                'api_key': settings.SCRAPERAPI_KEY,
                'url': base_url,
                'render': 'true',
                'premium': 'true'
            }
        },
        {
            "name": "–ú–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è",
            "params": {
                'api_key': settings.SCRAPERAPI_KEY,
                'url': base_url,
                'render': 'true',
                'device_type': 'mobile'
            }
        },
        {
            "name": "–ë–µ–∑ –≥–µ–æ—Ç–∞—Ä–≥–µ—Ç–∏–Ω–≥–∞",
            "params": {
                'api_key': settings.SCRAPERAPI_KEY,
                'url': base_url,
                'render': 'true',
                'premium': 'true'
            }
        },
        {
            "name": "–° –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏",
            "params": {
                'api_key': settings.SCRAPERAPI_KEY,
                'url': base_url,
                'render': 'true',
                'premium': 'true',
                'keep_headers': 'true'
            }
        }
    ]
    
    successful_attempts = 0
    
    for i, param_set in enumerate(param_sets, 1):
        logger.info(f"üß™ –ü–æ–ø—ã—Ç–∫–∞ {i}/{len(param_sets)}: {param_set['name']}")
        
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(sync_api_url, params=param_set['params']) as response:
                    logger.info(f"   üìä –°—Ç–∞—Ç—É—Å: {response.status}")
                    
                    if response.status == 200:
                        content = await response.text()
                        logger.info(f"   üìÑ –†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —É—Å–ø–µ—à–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
                        success_indicators = [
                            'immobiliare' in content.lower(),
                            'affitto' in content.lower(),
                            'roma' in content.lower(),
                            '__NEXT_DATA__' in content,
                            'annunci' in content.lower()
                        ]
                        
                        found_indicators = sum(success_indicators)
                        logger.info(f"   üéØ –ù–∞–π–¥–µ–Ω–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: {found_indicators}/5")
                        
                        if found_indicators >= 3:
                            logger.info(f"   ‚úÖ –£—Å–ø–µ—à–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥!")
                            successful_attempts += 1
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —É—Å–ø–µ—à–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                            with open(f'immobiliare_success_{i}.html', 'w', encoding='utf-8') as f:
                                f.write(content)
                            logger.info(f"   üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ immobiliare_success_{i}.html")
                        else:
                            logger.warning(f"   ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ —É—Å–ø–µ—Ö–∞")
                            logger.debug(f"   –ü–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤: {content[:300]}")
                    else:
                        logger.error(f"   ‚ùå HTTP –æ—à–∏–±–∫–∞: {response.status}")
                        content = await response.text()
                        logger.debug(f"   –û—Ç–≤–µ—Ç: {content[:200]}")
                        
        except Exception as e:
            logger.error(f"   ‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
        if i < len(param_sets):
            await asyncio.sleep(5)
    
    logger.info(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ê IMMOBILIARE:")
    logger.info(f"   ‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫: {successful_attempts}/{len(param_sets)}")
    
    return successful_attempts > 0


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üöÄ –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –¢–ï–°–¢ SCRAPERAPI")
    
    # –¢–µ—Å—Ç 1: –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ ScraperAPI
    basic_result = await test_scraperapi_basic()
    
    if not basic_result:
        logger.error("‚ùå –ë–∞–∑–æ–≤—ã–π —Ç–µ—Å—Ç ScraperAPI –ø—Ä–æ–≤–∞–ª–µ–Ω! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–ª—é—á API.")
        sys.exit(1)
    
    # –¢–µ—Å—Ç 2: –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Immobiliare
    immobiliare_result = await test_immobiliare_alternatives()
    
    if immobiliare_result:
        logger.info("üéâ –ù–∞–π–¥–µ–Ω—ã —Ä–∞–±–æ—á–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Immobiliare.it!")
        sys.exit(0)
    else:
        logger.error("‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –¥–æ—Å—Ç—É–ø–∞ –∫ Immobiliare.it –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å.")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main()) 