"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
"""
import logging
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session

from src.parsers import CasaScraper, SubitoScraper, IdealistaScraper, ImmobiliareScraper
from src.crud.crud_listing import listing as crud_listing
from src.schemas.listing import ListingCreate
from src.db.models import Listing

logger = logging.getLogger(__name__)


class ScrapingService:
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—Å–µ 4 –∏—Å—Ç–æ—á–Ω–∏–∫–∞: Casa.it, Subito, Idealista, Immobiliare
    """
    
    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ —Å–∫—Ä–∞–ø–µ—Ä—ã
        self.casa_scraper = CasaScraper(max_concurrent=5, enable_geocoding=False)
        self.subito_scraper = SubitoScraper(enable_geocoding=False, fetch_coords=False)
        self.idealista_scraper = IdealistaScraper(max_concurrent=5, enable_geocoding=False)
        self.immobiliare_scraper = ImmobiliareScraper(enable_geocoding=False)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.default_max_pages = 5
        
    def get_available_sources(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        return ['casa_it', 'subito', 'idealista', 'immobiliare']
    
    async def scrape_casa_async(
        self,
        filters: Dict[str, Any],
        max_pages: int = None
    ) -> List[Dict[str, Any]]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ Casa.it"""
        if max_pages is None:
            max_pages = self.default_max_pages
            
        try:
            logger.info(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ Casa.it –Ω–∞ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
            listings = await self.casa_scraper.scrape_multiple_pages(max_pages=max_pages)
            logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ Casa.it")
            return listings
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Casa.it: {e}")
            return []
    
    async def scrape_subito_async(
        self,
        filters: Dict[str, Any],
        max_pages: int = None
    ) -> List[Dict[str, Any]]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ Subito"""
        if max_pages is None:
            max_pages = self.default_max_pages
            
        try:
            logger.info(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ Subito.it –Ω–∞ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
            listings = await self.subito_scraper.scrape_multiple_pages(max_pages=max_pages)
            logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ Subito.it")
            return listings
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Subito.it: {e}")
            return []
    
    async def scrape_idealista_async(
        self,
        filters: Dict[str, Any],
        max_pages: int = None
    ) -> List[Dict[str, Any]]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ Idealista"""
        if max_pages is None:
            max_pages = self.default_max_pages
            
        try:
            logger.info(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ Idealista.it –Ω–∞ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
            listings = await self.idealista_scraper.scrape_multiple_pages(max_pages=max_pages)
            logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ Idealista.it")
            return listings
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Idealista.it: {e}")
            return []
    
    async def scrape_immobiliare_async(
        self,
        filters: Dict[str, Any],
        max_pages: int = None
    ) -> List[Dict[str, Any]]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ Immobiliare.it"""
        if max_pages is None:
            max_pages = self.default_max_pages
            
        try:
            logger.info(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ Immobiliare.it –Ω–∞ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
            listings = await self.immobiliare_scraper.scrape_multiple_pages(max_pages=max_pages)
            logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ Immobiliare.it")
            return listings
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Immobiliare.it: {e}")
            return []
    
    async def scrape_all_sources(
        self,
        filters: Dict[str, Any],
        max_pages: int = None
    ) -> List[Dict[str, Any]]:
        """
        –û–°–ù–û–í–ù–û–ô –ú–ï–¢–û–î: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        
        Args:
            filters: –§–∏–ª—å—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫
            
        Returns:
            List[Dict]: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        """
        if max_pages is None:
            max_pages = self.default_max_pages
            
        logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (max_pages={max_pages})")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(
            self.scrape_casa_async(filters, max_pages),
            self.scrape_subito_async(filters, max_pages),
            self.scrape_idealista_async(filters, max_pages),
            self.scrape_immobiliare_async(filters, max_pages),
            return_exceptions=True
        )
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_listings = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {result}")
            elif isinstance(result, list):
                all_listings.extend(result)
        
        logger.info(f"üìä –í—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(all_listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        return all_listings
    
    def save_listings_to_db(
        self,
        listings: List[Dict[str, Any]],
        db: Session
    ) -> Dict[str, int]:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π
        
        Args:
            listings: –°–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            db: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            
        Returns:
            Dict[str, int]: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        if not listings:
            return {"created": 0, "updated": 0, "errors": 0, "skipped_duplicates": 0}
            
        stats = {
            "created": 0, 
            "updated": 0, 
            "errors": 0, 
            "skipped_duplicates": 0,
            "by_source": {}
        }
        
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        for listing_data in listings:
            source = listing_data.get('source', 'unknown')
            if source not in stats["by_source"]:
                stats["by_source"][source] = {
                    "total": 0, "created": 0, "updated": 0, "errors": 0, "skipped": 0
                }
            stats["by_source"][source]["total"] += 1
        
        for listing_data in listings:
            try:
                external_id = listing_data.get('external_id')
                source = listing_data.get('source', 'unknown')
                url = listing_data.get('url', '')
                
                if not external_id:
                    logger.warning("‚ö†Ô∏è –û–±—ä—è–≤–ª–µ–Ω–∏–µ –±–µ–∑ external_id, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    stats["errors"] += 1
                    stats["by_source"][source]["errors"] += 1
                    continue
                
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ external_id + source (–æ—Å–Ω–æ–≤–Ω–æ–π —Å–ø–æ—Å–æ–±)
                existing = crud_listing.get_by_external_id(
                    db=db,
                    external_id=external_id,
                    source=source
                )
                
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ external_id, –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ URL
                if not existing and url:
                    existing = crud_listing.get_by_url(db=db, url=url)
                    
                    if existing and existing.source != source:
                        # –û–±—ä—è–≤–ª–µ–Ω–∏–µ —Å —Ç–∞–∫–∏–º URL —É–∂–µ –µ—Å—Ç—å, –Ω–æ –∏–∑ –¥—Ä—É–≥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                        # –û–±–Ω–æ–≤–ª—è–µ–º external_id –∏ source –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏–∏
                        logger.info(f"üîó –ù–∞–π–¥–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ —Å URL {url[:50]}... –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {existing.source}, –æ–±–Ω–æ–≤–ª—è–µ–º –Ω–∞ {source}")
                        listing_update = ListingCreate(**listing_data)
                        crud_listing.update(db=db, db_obj=existing, obj_in=listing_update)
                        stats["updated"] += 1
                        stats["by_source"][source]["updated"] += 1
                        continue
                    elif existing:
                        # –¢–æ –∂–µ —Å–∞–º–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        logger.debug(f"üîÑ –û–±—ä—è–≤–ª–µ–Ω–∏–µ —Å URL —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {url[:50]}...")
                        stats["skipped_duplicates"] += 1
                        stats["by_source"][source]["skipped"] += 1
                        continue
                
                if existing:
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
                    listing_update = ListingCreate(**listing_data)
                    crud_listing.update(db=db, db_obj=existing, obj_in=listing_update)
                    stats["updated"] += 1
                    stats["by_source"][source]["updated"] += 1
                    logger.debug(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ {external_id}")
                else:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
                    listing_create = ListingCreate(**listing_data)
                    crud_listing.create(db=db, obj_in=listing_create)
                    stats["created"] += 1
                    stats["by_source"][source]["created"] += 1
                    logger.debug(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ {external_id}")
                    
            except Exception as e:
                error_msg = str(e)
                source = listing_data.get('source', 'unknown')
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ URL
                if "duplicate key value violates unique constraint" in error_msg and "url" in error_msg:
                    logger.warning(f"‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç URL –¥–ª—è {external_id}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    stats["skipped_duplicates"] += 1
                    stats["by_source"][source]["skipped"] += 1
                    
                    # –ù–ï –¥–µ–ª–∞–µ–º rollback –¥–ª—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ URL - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
                    continue
                else:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—ä—è–≤–ª–µ–Ω–∏—è {external_id}: {e}")
                    
                    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π rollback —Å–µ—Å—Å–∏–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
                    try:
                        db.rollback()
                        logger.debug("üîÑ –°–µ—Å—Å–∏—è –ë–î –æ—Ç–∫–∞—Ç–∞–Ω–∞")
                    except Exception as rollback_error:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ rollback: {rollback_error}")
                    
                    # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                    if "value too long" in error_msg:
                        logger.error(f"üîç –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –æ–±—ä—è–≤–ª–µ–Ω–∏–∏ {external_id}")
                        for field, value in listing_data.items():
                            if isinstance(value, str) and len(value) > 100:
                                logger.error(f"   üìè –ü–æ–ª–µ '{field}': {len(value)} —Å–∏–º–≤–æ–ª–æ–≤ (–ø–µ—Ä–≤—ã–µ 100: {value[:100]}...)")
                    
                    stats["errors"] += 1
                    stats["by_source"][source]["errors"] += 1
                    continue
        
        # –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.info(f"üíæ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è:")
        logger.info(f"   üìä –û–±—â–∞—è: {stats['created']} –Ω–æ–≤—ã—Ö, {stats['updated']} –æ–±–Ω–æ–≤–ª–µ–Ω–æ, {stats['skipped_duplicates']} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, {stats['errors']} –æ—à–∏–±–æ–∫")
        
        for source, source_stats in stats["by_source"].items():
            logger.info(f"   üìå {source}: {source_stats['created']} –Ω–æ–≤—ã—Ö, {source_stats['updated']} –æ–±–Ω–æ–≤–ª–µ–Ω–æ, {source_stats['skipped']} –ø—Ä–æ–ø—É—â–µ–Ω–æ, {source_stats['errors']} –æ—à–∏–±–æ–∫ –∏–∑ {source_stats['total']} –≤—Å–µ–≥–æ")
        
        return stats
    
    def save_listing(self, db: Session, listing_data: Dict[str, Any]) -> str:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–¥–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            db: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            listing_data: –î–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            
        Returns:
            str: "created", "updated" –∏–ª–∏ "error"
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
            external_id = listing_data.get('external_id')
            source = listing_data.get('source', 'immobiliare')
            
            if not external_id:
                logger.warning("‚ö†Ô∏è –û–±—ä—è–≤–ª–µ–Ω–∏–µ –±–µ–∑ external_id, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return "error"
            
            existing = crud_listing.get_by_external_id(
                db=db,
                external_id=external_id,
                source=source
            )
            
            if existing:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
                listing_update = ListingCreate(**listing_data)
                crud_listing.update(db=db, db_obj=existing, obj_in=listing_update)
                logger.debug(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ {external_id}")
                return "updated"
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
                listing_create = ListingCreate(**listing_data)
                crud_listing.create(db=db, obj_in=listing_create)
                logger.debug(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ {external_id}")
                return "created"
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—ä—è–≤–ª–µ–Ω–∏—è {external_id}: {e}")
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π rollback —Å–µ—Å—Å–∏–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
            try:
                db.rollback()
                logger.debug("üîÑ –°–µ—Å—Å–∏—è –ë–î –æ—Ç–∫–∞—Ç–∞–Ω–∞")
            except Exception as rollback_error:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ rollback: {rollback_error}")
            
            return "error"
    
    async def scrape_and_save(
        self,
        filters: Dict[str, Any],
        db: Session,
        max_pages: int = None
    ) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: –ø–∞—Ä—Å–∏–Ω–≥ + —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        
        Args:
            filters: –§–∏–ª—å—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            db: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø–µ—Ä–∞—Ü–∏–∏
        """
        start_time = datetime.now()
        
        try:
            # –®–∞–≥ 1: –ü–∞—Ä—Å–∏–Ω–≥
            listings = await self.scrape_all_sources(filters, max_pages)
            
            if not listings:
                return {
                    "success": False,
                    "message": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è",
                    "scraped_count": 0,
                    "saved_count": 0,
                    "elapsed_time": (datetime.now() - start_time).total_seconds()
                }
            
            # –®–∞–≥ 2: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
            saved_stats = self.save_listings_to_db(listings, db)
            
            elapsed_time = (datetime.now() - start_time).total_seconds()
            
            return {
                "success": True,
                "message": f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π",
                "scraped_count": len(listings),
                "saved_count": saved_stats["created"],
                "updated_count": saved_stats["updated"],
                "error_count": saved_stats["errors"],
                "sources": ["casa_it", "subito", "idealista", "immobiliare"],
                "elapsed_time": elapsed_time
            }
            
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ scrape_and_save: {e}")
            return {
                "success": False,
                "message": f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}",
                "scraped_count": 0,
                "saved_count": 0,
                "elapsed_time": (datetime.now() - start_time).total_seconds()
            }

    # –ú–µ—Ç–æ–¥—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ —Å—Ç–∞—Ä—ã–º API
    def scrape_single_source(
        self,
        source: str,
        filters: Dict[str, Any],
        max_pages: int = None,
        use_scraperapi: bool = None
    ) -> List[Dict[str, Any]]:
        """
        –£–°–¢–ê–†–ï–í–®–ò–ô –ú–ï–¢–û–î: –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        """
        logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç–∞—Ä–µ–≤—à–∏–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ scrape_single_source")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –≤ –Ω–æ–≤–æ–º event loop
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            if source == 'immobiliare':
                result = loop.run_until_complete(
                    self.scrape_immobiliare_async(filters, max_pages)
                )
            else:
                logger.error(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {source}")
                result = []
                
            loop.close()
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±–µ—Ä—Ç–∫–µ: {e}")
            return [] 

    def get_database_statistics(self, db: Session) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –∏ –æ–±—â–∏—Ö –º–µ—Ç—Ä–∏–∫
        """
        try:
            from src.db.models import Listing
            from sqlalchemy import func, desc
            from datetime import datetime, timedelta
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_listings = db.query(func.count(Listing.id)).scalar()
            active_listings = db.query(func.count(Listing.id)).filter(Listing.is_active == True).scalar()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            source_stats = db.query(
                Listing.source,
                func.count(Listing.id).label('total'),
                func.count(func.nullif(Listing.is_active, False)).label('active')
            ).group_by(Listing.source).all()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
            since_24h = datetime.utcnow() - timedelta(hours=24)
            recent_stats = db.query(
                Listing.source,
                func.count(Listing.id).label('recent_24h')
            ).filter(
                Listing.created_at >= since_24h
            ).group_by(Listing.source).all()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é
            since_week = datetime.utcnow() - timedelta(days=7)
            week_stats = db.query(
                Listing.source,
                func.count(Listing.id).label('recent_week')
            ).filter(
                Listing.created_at >= since_week
            ).group_by(Listing.source).all()
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                "total_listings": total_listings,
                "active_listings": active_listings,
                "inactive_listings": total_listings - active_listings,
                "by_source": {},
                "recent_24h": {},
                "recent_week": {}
            }
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            for source, total, active in source_stats:
                result["by_source"][source] = {
                    "total": total,
                    "active": active,
                    "inactive": total - active
                }
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω–µ–¥–∞–≤–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            for source, count in recent_stats:
                result["recent_24h"][source] = count
                
            for source, count in week_stats:
                result["recent_week"][source] = count
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ë–î: {e}")
            return {
                "error": str(e),
                "total_listings": 0,
                "active_listings": 0,
                "by_source": {}
            } 