"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
"""
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session

from src.parsers import ImmobiliareParser
from src.crud.crud_listing import listing as crud_listing
from src.schemas.listing import ListingCreate
from src.db.models import Listing

logger = logging.getLogger(__name__)


class ScrapingService:
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –≤—Å–µ–º–∏ –ø–∞—Ä—Å–µ—Ä–∞–º–∏ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    """
    
    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã
        self.parsers = {
            'immobiliare': ImmobiliareParser(),
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –ø–∞—Ä—Å–µ—Ä—ã:
            # 'idealista': IdealistaParser(),
            # 'subito': SubitoParser(),
        }
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.default_max_pages = 5
        self.default_use_scraperapi = True
        
    def get_available_parsers(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤"""
        return list(self.parsers.keys())
    
    def scrape_single_source(
        self,
        source: str,
        filters: Dict[str, Any],
        max_pages: int = None,
        use_scraperapi: bool = None
    ) -> List[Dict[str, Any]]:
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        
        Args:
            source: –ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (immobiliare, idealista, etc.)
            filters: –§–∏–ª—å—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            use_scraperapi: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ScraperAPI
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        """
        if source not in self.parsers:
            raise ValueError(f"–ü–∞—Ä—Å–µ—Ä '{source}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {list(self.parsers.keys())}")
        
        parser = self.parsers[source]
        max_pages = max_pages or self.default_max_pages
        use_scraperapi = use_scraperapi if use_scraperapi is not None else self.default_use_scraperapi
        
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ '{source}' —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏: {filters}")
        
        try:
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
            if hasattr(parser, 'validate_filters'):
                cleaned_filters = parser.validate_filters(filters)
                logger.info(f"‚úÖ –§–∏–ª—å—Ç—Ä—ã –æ—á–∏—â–µ–Ω—ã: {cleaned_filters}")
            else:
                cleaned_filters = filters
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥
            listings = parser.scrape_listings(
                filters=cleaned_filters,
                max_pages=max_pages,
                use_scraperapi=use_scraperapi
            )
            
            logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è '{source}': –Ω–∞–π–¥–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            return listings
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ '{source}': {e}")
            raise
    
    def scrape_all_sources(
        self,
        filters: Dict[str, Any],
        sources: List[str] = None,
        max_pages: int = None,
        use_scraperapi: bool = None
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –≤—Å–µ—Ö –∏–ª–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        
        Args:
            filters: –§–∏–ª—å—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            sources: –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (None = –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ)
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            use_scraperapi: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ScraperAPI
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –∏—Å—Ç–æ—á–Ω–∏–∫—É
        """
        if sources is None:
            sources = list(self.parsers.keys())
        
        results = {}
        
        for source in sources:
            if source not in self.parsers:
                logger.warning(f"‚ö†Ô∏è –ü–∞—Ä—Å–µ—Ä '{source}' –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue
            
            try:
                listings = self.scrape_single_source(
                    source=source,
                    filters=filters,
                    max_pages=max_pages,
                    use_scraperapi=use_scraperapi
                )
                results[source] = listings
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ '{source}': {e}")
                results[source] = []
        
        total_listings = sum(len(listings) for listings in results.values())
        logger.info(f"üéâ –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω: {total_listings} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        
        return results
    
    def save_listings_to_db(
        self,
        listings: List[Dict[str, Any]],
        db: Session,
        update_existing: bool = True
    ) -> Dict[str, int]:
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏—è –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            listings: –°–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            db: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            update_existing: –û–±–Ω–æ–≤–ª—è—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (created, updated, skipped)
        """
        stats = {
            'created': 0,
            'updated': 0,
            'skipped': 0,
            'errors': 0
        }
        
        logger.info(f"üíæ –ù–∞—á–∏–Ω–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤ –ë–î")
        
        for listing_data in listings:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
                existing_listing = crud_listing.get_by_external_id(
                    db=db,
                    source=listing_data['source'],
                    external_id=listing_data['external_id']
                )
                
                if existing_listing:
                    if update_existing:
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
                        updated_data = listing_data.copy()
                        updated_data['scraped_at'] = datetime.utcnow()
                        
                        updated_listing = crud_listing.update(
                            db=db,
                            db_obj=existing_listing,
                            obj_in=updated_data
                        )
                        stats['updated'] += 1
                        logger.debug(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: {updated_listing.external_id}")
                    else:
                        stats['skipped'] += 1
                        logger.debug(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: {listing_data['external_id']}")
                else:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
                    listing_data['scraped_at'] = datetime.utcnow()
                    
                    # –°–æ–∑–¥–∞–µ–º —Å—Ö–µ–º—É –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                    listing_create = ListingCreate(**listing_data)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
                    new_listing = crud_listing.create(db=db, obj_in=listing_create)
                    stats['created'] += 1
                    logger.debug(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –Ω–æ–≤–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: {new_listing.external_id}")
                
            except Exception as e:
                stats['errors'] += 1
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è {listing_data.get('external_id', 'unknown')}: {e}")
                continue
        
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: —Å–æ–∑–¥–∞–Ω–æ {stats['created']}, –æ–±–Ω–æ–≤–ª–µ–Ω–æ {stats['updated']}, –ø—Ä–æ–ø—É—â–µ–Ω–æ {stats['skipped']}, –æ—à–∏–±–æ–∫ {stats['errors']}")
        return stats
    
    def scrape_and_save(
        self,
        filters: Dict[str, Any],
        db: Session,
        sources: List[str] = None,
        max_pages: int = None,
        use_scraperapi: bool = None,
        update_existing: bool = True
    ) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: –ø–∞—Ä—Å–∏–Ω–≥ + —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        
        Args:
            filters: –§–∏–ª—å—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            db: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            sources: –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (None = –≤—Å–µ)
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            use_scraperapi: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ScraperAPI
            update_existing: –û–±–Ω–æ–≤–ª—è—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            
        Returns:
            –ü–æ–ª–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏
        """
        start_time = datetime.utcnow()
        
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        logger.info(f"üìã –§–∏–ª—å—Ç—Ä—ã: {filters}")
        logger.info(f"üåê –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {sources or '–≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ'}")
        
        try:
            # –≠—Ç–∞–ø 1: –ü–∞—Ä—Å–∏–Ω–≥
            scraping_results = self.scrape_all_sources(
                filters=filters,
                sources=sources,
                max_pages=max_pages,
                use_scraperapi=use_scraperapi
            )
            
            # –≠—Ç–∞–ø 2: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            all_listings = []
            for source, listings in scraping_results.items():
                all_listings.extend(listings)
            
            # –≠—Ç–∞–ø 3: –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
            unique_listings = self._deduplicate_listings(all_listings)
            
            # –≠—Ç–∞–ø 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
            save_stats = self.save_listings_to_db(
                listings=unique_listings,
                db=db,
                update_existing=update_existing
            )
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            end_time = datetime.utcnow()
            duration = (end_time - start_time).total_seconds()
            
            final_stats = {
                'duration_seconds': duration,
                'sources_processed': len(scraping_results),
                'total_scraped': len(all_listings),
                'unique_listings': len(unique_listings),
                'database_stats': save_stats,
                'scraping_results': {
                    source: len(listings) 
                    for source, listings in scraping_results.items()
                }
            }
            
            logger.info(f"üéâ –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {duration:.1f} —Å–µ–∫—É–Ω–¥")
            logger.info(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {final_stats}")
            
            return final_stats
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–æ–ª–Ω–æ–º —Ü–∏–∫–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            raise
    
    def _deduplicate_listings(self, listings: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        
        Args:
            listings: –°–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            
        Returns:
            –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        """
        seen = set()
        unique_listings = []
        
        for listing in listings:
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏ external_id
            key = f"{listing['source']}_{listing['external_id']}"
            
            if key not in seen:
                seen.add(key)
                unique_listings.append(listing)
            else:
                logger.debug(f"üîÑ –£–¥–∞–ª–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: {key}")
        
        duplicates_count = len(listings) - len(unique_listings)
        if duplicates_count > 0:
            logger.info(f"üßπ –£–¥–∞–ª–µ–Ω–æ {duplicates_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –æ—Å—Ç–∞–ª–æ—Å—å {len(unique_listings)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        
        return unique_listings
    
    def get_parser_info(self, source: str) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞—Ä—Å–µ—Ä–µ
        
        Args:
            source: –ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            
        Returns:
            –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞—Ä—Å–µ—Ä–µ
        """
        if source not in self.parsers:
            return None
        
        parser = self.parsers[source]
        
        info = {
            'name': parser.name,
            'base_url': parser.base_url,
            'has_scraperapi': bool(parser.scraperapi_key),
        }
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if hasattr(parser, 'get_supported_cities'):
            info['supported_cities'] = parser.get_supported_cities()
        
        return info
    
    def get_all_parsers_info(self) -> Dict[str, Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–∞—Ö"""
        return {
            source: self.get_parser_info(source)
            for source in self.parsers.keys()
        }
    
    def test_parser(
        self,
        source: str,
        test_filters: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä—Å–µ—Ä —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        
        Args:
            source: –ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            test_filters: –¢–µ—Å—Ç–æ–≤—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é Roma)
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        if source not in self.parsers:
            return {'error': f"–ü–∞—Ä—Å–µ—Ä '{source}' –Ω–µ –Ω–∞–π–¥–µ–Ω"}
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if test_filters is None:
            test_filters = {'city': 'roma'}
        
        try:
            start_time = datetime.utcnow()
            
            listings = self.scrape_single_source(
                source=source,
                filters=test_filters,
                max_pages=1,  # –¢–æ–ª—å–∫–æ –æ–¥–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–ª—è —Ç–µ—Å—Ç–∞
                use_scraperapi=False  # –ë–µ–∑ ScraperAPI –¥–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã
            )
            
            end_time = datetime.utcnow()
            duration = (end_time - start_time).total_seconds()
            
            return {
                'success': True,
                'source': source,
                'filters_used': test_filters,
                'listings_found': len(listings),
                'duration_seconds': duration,
                'sample_listing': listings[0] if listings else None
            }
            
        except Exception as e:
            return {
                'success': False,
                'source': source,
                'error': str(e)
            } 