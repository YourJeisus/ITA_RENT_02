"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
"""
import logging
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session

from src.parsers.immobiliare_scraper import ImmobiliareScraper
from src.crud.crud_listing import listing as crud_listing
from src.schemas.listing import ListingCreate
from src.db.models import Listing

logger = logging.getLogger(__name__)


class ScrapingService:
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ V2
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–µ—Ä –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    
    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–µ—Ä
        self.immobiliare_scraper = ImmobiliareScraper(enable_geocoding=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.default_max_pages = 5
        
    def get_available_sources(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        return ['immobiliare']
    
    async def scrape_immobiliare_async(
        self,
        filters: Dict[str, Any],
        max_pages: int = None
    ) -> List[Dict[str, Any]]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ Immobiliare.it
        
        Args:
            filters: –§–∏–ª—å—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ (–ø–æ–∫–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è, –ø–∞—Ä—Å–∏–º –†–∏–º)
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        """
        if max_pages is None:
            max_pages = self.default_max_pages
            
        try:
            logger.info(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ Immobiliare.it –Ω–∞ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –Ω–æ–≤—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–µ—Ä
            listings = await self.immobiliare_scraper.scrape_multiple_pages(max_pages=max_pages)
            
            logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ Immobiliare.it")
            return listings
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Immobiliare.it: {e}")
            return []
    
    async def scrape_all_sources(
        self,
        filters: Dict[str, Any],
        max_pages: int = None
    ) -> List[Dict[str, Any]]:
        """
        –û–°–ù–û–í–ù–û–ô –ú–ï–¢–û–î: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        
        Args:
            filters: –§–∏–ª—å—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫
            
        Returns:
            List[Dict]: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        """
        if max_pages is None:
            max_pages = self.default_max_pages
            
        logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (max_pages={max_pages})")
        
        all_listings = []
        
        # –ü–æ–∫–∞ —É –Ω–∞—Å —Ç–æ–ª—å–∫–æ Immobiliare.it, –Ω–æ –º–æ–∂–Ω–æ –ª–µ–≥–∫–æ –¥–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        try:
            immobiliare_listings = await self.scrape_immobiliare_async(filters, max_pages)
            all_listings.extend(immobiliare_listings)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Immobiliare.it: {e}")
        
        # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:
        # try:
        #     idealista_listings = await self.scrape_idealista_async(filters, max_pages)
        #     all_listings.extend(idealista_listings)
        # except Exception as e:
        #     logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Idealista.it: {e}")
        
        logger.info(f"üìä –í—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(all_listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        return all_listings
    
    def save_listings_to_db(
        self,
        listings: List[Dict[str, Any]],
        db: Session
    ) -> Dict[str, int]:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π
        
        Args:
            listings: –°–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            db: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            
        Returns:
            Dict[str, int]: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        if not listings:
            return {"created": 0, "updated": 0, "errors": 0}
            
        stats = {"created": 0, "updated": 0, "errors": 0}
        
        logger.debug(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
        
        for listing_data in listings:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
                external_id = listing_data.get('external_id')
                source = listing_data.get('source', 'immobiliare')
                
                if not external_id:
                    logger.warning("‚ö†Ô∏è –û–±—ä—è–≤–ª–µ–Ω–∏–µ –±–µ–∑ external_id, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    stats["errors"] += 1
                    continue
                
                existing = crud_listing.get_by_external_id(
                    db=db,
                    external_id=external_id,
                    source=source
                )
                
                if existing:
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
                    listing_update = ListingCreate(**listing_data)
                    crud_listing.update(db=db, db_obj=existing, obj_in=listing_update)
                    stats["updated"] += 1
                    logger.debug(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ {external_id}")
                else:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
                    listing_create = ListingCreate(**listing_data)
                    crud_listing.create(db=db, obj_in=listing_create)
                    stats["created"] += 1
                    logger.debug(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ {external_id}")
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π rollback —Å–µ—Å—Å–∏–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
                try:
                    db.rollback()
                    logger.debug("üîÑ –°–µ—Å—Å–∏—è –ë–î –æ—Ç–∫–∞—Ç–∞–Ω–∞")
                except Exception as rollback_error:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ rollback: {rollback_error}")
                
                # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                if "value too long" in str(e):
                    logger.error(f"üîç –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –æ–±—ä—è–≤–ª–µ–Ω–∏–∏ {external_id}")
                    for field, value in listing_data.items():
                        if isinstance(value, str) and len(value) > 100:
                            logger.error(f"   üìè –ü–æ–ª–µ '{field}': {len(value)} —Å–∏–º–≤–æ–ª–æ–≤ (–ø–µ—Ä–≤—ã–µ 100: {value[:100]}...)")
                
                logger.debug(f"–î–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {listing_data}")
                stats["errors"] += 1
                continue
        
        logger.debug(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {stats['created']} –Ω–æ–≤—ã—Ö, {stats['updated']} –æ–±–Ω–æ–≤–ª–µ–Ω–æ, {stats['errors']} –æ—à–∏–±–æ–∫")
        
        return stats
    
    async def scrape_and_save(
        self,
        filters: Dict[str, Any],
        db: Session,
        max_pages: int = None
    ) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: –ø–∞—Ä—Å–∏–Ω–≥ + —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        
        Args:
            filters: –§–∏–ª—å—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            db: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø–µ—Ä–∞—Ü–∏–∏
        """
        start_time = datetime.now()
        
        try:
            # –®–∞–≥ 1: –ü–∞—Ä—Å–∏–Ω–≥
            listings = await self.scrape_all_sources(filters, max_pages)
            
            if not listings:
                return {
                    "success": False,
                    "message": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è",
                    "scraped_count": 0,
                    "saved_count": 0,
                    "elapsed_time": (datetime.now() - start_time).total_seconds()
                }
            
            # –®–∞–≥ 2: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
            saved_stats = self.save_listings_to_db(listings, db)
            
            elapsed_time = (datetime.now() - start_time).total_seconds()
            
            return {
                "success": True,
                "message": f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π",
                "scraped_count": len(listings),
                "saved_count": saved_stats["created"],
                "updated_count": saved_stats["updated"],
                "error_count": saved_stats["errors"],
                "sources": ["immobiliare"],
                "elapsed_time": elapsed_time
            }
            
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ scrape_and_save: {e}")
            return {
                "success": False,
                "message": f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}",
                "scraped_count": 0,
                "saved_count": 0,
                "elapsed_time": (datetime.now() - start_time).total_seconds()
            }

    # –ú–µ—Ç–æ–¥—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ —Å—Ç–∞—Ä—ã–º API
    def scrape_single_source(
        self,
        source: str,
        filters: Dict[str, Any],
        max_pages: int = None,
        use_scraperapi: bool = None
    ) -> List[Dict[str, Any]]:
        """
        –£–°–¢–ê–†–ï–í–®–ò–ô –ú–ï–¢–û–î: –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        """
        logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç–∞—Ä–µ–≤—à–∏–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ scrape_single_source")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –≤ –Ω–æ–≤–æ–º event loop
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            if source == 'immobiliare':
                result = loop.run_until_complete(
                    self.scrape_immobiliare_async(filters, max_pages)
                )
            else:
                logger.error(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {source}")
                result = []
                
            loop.close()
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±–µ—Ä—Ç–∫–µ: {e}")
            return [] 