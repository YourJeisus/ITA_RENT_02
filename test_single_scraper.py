#!/usr/bin/env python3
"""
üß™ –¢–ï–°–¢ –û–¢–î–ï–õ–¨–ù–´–• –°–ö–†–ê–ü–ï–†–û–í

–ü–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É –∫–∞–∂–¥–æ–≥–æ —Å–∫—Ä–∞–ø–µ—Ä–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
–¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python test_single_scraper.py idealista
    python test_single_scraper.py immobiliare  
    python test_single_scraper.py subito
    python test_single_scraper.py all
"""
import sys
import asyncio
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent))

from src.parsers.idealista_scraper import IdealistaScraper
from src.parsers.immobiliare_scraper import ImmobiliareScraper
from src.parsers.subito_scraper import SubitoScraper
from src.services.scraping_service import ScrapingService
from src.db.database import SessionLocal
import logging
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


async def test_idealista():
    """–¢–µ—Å—Ç Idealista scraper"""
    print("\n" + "="*50)
    print("üè† –¢–ï–°–¢ IDEALISTA SCRAPER")
    print("="*50)
    
    try:
        scraper = IdealistaScraper(enable_geocoding=False)  # –û—Ç–∫–ª—é—á–∞–µ–º –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã
        
        print("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ Idealista (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)...")
        start_time = datetime.now()
        
        listings = await scraper.scrape_multiple_pages(max_pages=2)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ Idealista –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {duration:.1f}—Å")
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
        
        if listings:
            print(f"\nüìã –ü—Ä–∏–º–µ—Ä—ã –æ–±—ä—è–≤–ª–µ–Ω–∏–π:")
            for i, listing in enumerate(listings[:3], 1):
                title = listing.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]
                price = listing.get('price', '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞')
                city = listing.get('city', '–ì–æ—Ä–æ–¥ –Ω–µ —É–∫–∞–∑–∞–Ω')
                external_id = listing.get('external_id', 'ID –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç')
                
                print(f"   {i}. {title}...")
                print(f"      üí∞ {price} | üìç {city} | ID: {external_id}")
        
        return len(listings)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è Idealista: {e}")
        return 0


async def test_immobiliare():
    """–¢–µ—Å—Ç Immobiliare scraper"""
    print("\n" + "="*50)
    print("üè† –¢–ï–°–¢ IMMOBILIARE SCRAPER")
    print("="*50)
    
    try:
        scraper = ImmobiliareScraper(enable_geocoding=False)
        
        print("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ Immobiliare (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)...")
        start_time = datetime.now()
        
        listings = await scraper.scrape_multiple_pages(max_pages=2)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ Immobiliare –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {duration:.1f}—Å")
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
        
        if listings:
            print(f"\nüìã –ü—Ä–∏–º–µ—Ä—ã –æ–±—ä—è–≤–ª–µ–Ω–∏–π:")
            for i, listing in enumerate(listings[:3], 1):
                title = listing.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]
                price = listing.get('price', '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞')
                city = listing.get('city', '–ì–æ—Ä–æ–¥ –Ω–µ —É–∫–∞–∑–∞–Ω')
                external_id = listing.get('external_id', 'ID –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç')
                
                print(f"   {i}. {title}...")
                print(f"      üí∞ {price} | üìç {city} | ID: {external_id}")
        
        return len(listings)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è Immobiliare: {e}")
        return 0


async def test_subito():
    """–¢–µ—Å—Ç Subito scraper"""
    print("\n" + "="*50)
    print("üè† –¢–ï–°–¢ SUBITO SCRAPER")
    print("="*50)
    
    try:
        scraper = SubitoScraper(enable_geocoding=False)
        
        print("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ Subito (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)...")
        start_time = datetime.now()
        
        listings = await scraper.scrape_multiple_pages(max_pages=2)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ Subito –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {duration:.1f}—Å")
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
        
        if listings:
            print(f"\nüìã –ü—Ä–∏–º–µ—Ä—ã –æ–±—ä—è–≤–ª–µ–Ω–∏–π:")
            for i, listing in enumerate(listings[:3], 1):
                title = listing.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]
                price = listing.get('price', '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞')
                city = listing.get('city', '–ì–æ—Ä–æ–¥ –Ω–µ —É–∫–∞–∑–∞–Ω')
                external_id = listing.get('external_id', 'ID –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç')
                
                print(f"   {i}. {title}...")
                print(f"      üí∞ {price} | üìç {city} | ID: {external_id}")
        
        return len(listings)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è Subito: {e}")
        return 0


async def test_all_scrapers():
    """–¢–µ—Å—Ç –≤—Å–µ—Ö —Å–∫—Ä–∞–ø–µ—Ä–æ–≤"""
    print("\n" + "="*50)
    print("üöÄ –¢–ï–°–¢ –í–°–ï–• –°–ö–†–ê–ü–ï–†–û–í")
    print("="*50)
    
    results = {}
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Å–∫—Ä–∞–ø–µ—Ä
    results['idealista'] = await test_idealista()
    results['immobiliare'] = await test_immobiliare()
    results['subito'] = await test_subito()
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "="*50)
    print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("="*50)
    
    total_listings = sum(results.values())
    
    for source, count in results.items():
        status = "‚úÖ" if count > 0 else "‚ùå"
        print(f"   {status} {source.upper()}: {count} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
    
    print(f"\nüéØ –û–±—â–∏–π –∏—Ç–æ–≥: {total_listings} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
    
    if total_listings == 0:
        print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ù–∏ –æ–¥–∏–Ω —Å–∫—Ä–∞–ø–µ—Ä –Ω–µ –≤–µ—Ä–Ω—É–ª –æ–±—ä—è–≤–ª–µ–Ω–∏—è!")
        print("   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
        print("   ‚Ä¢ SCRAPERAPI_KEY –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        print("   ‚Ä¢ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
        print("   ‚Ä¢ –†–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–∞–π—Ç–æ–≤")
    else:
        print("‚úÖ –°–∏—Å—Ç–µ–º–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")


async def save_test_results(all_listings: list):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –ë–î"""
    if not all_listings:
        print("‚ùå –ù–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return
    
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(all_listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤ –ë–î...")
    
    db = SessionLocal()
    try:
        scraping_service = ScrapingService()
        saved_stats = scraping_service.save_listings_to_db(all_listings, db)
        
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
        print(f"   ‚Ä¢ –ù–æ–≤—ã—Ö: {saved_stats['created']}")
        print(f"   ‚Ä¢ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {saved_stats['updated']}")
        print(f"   ‚Ä¢ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø—Ä–æ–ø—É—â–µ–Ω–æ: {saved_stats.get('skipped_duplicates', 0)}")
        print(f"   ‚Ä¢ –û—à–∏–±–æ–∫: {saved_stats['errors']}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
    finally:
        db.close()


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    if len(sys.argv) < 2:
        print("‚ùå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python test_single_scraper.py <scraper>")
        print("   –î–æ—Å—Ç—É–ø–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã: idealista, immobiliare, subito, all")
        return
    
    scraper_name = sys.argv[1].lower()
    
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –°–ö–†–ê–ü–ï–†–û–í ITA_RENT_BOT")
    print("=" * 50)
    print(f"‚è∞ –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    try:
        from src.core.config import settings
        if not settings.SCRAPERAPI_KEY:
            print("‚ùå –û–®–ò–ë–ö–ê: SCRAPERAPI_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω!")
            return
        print(f"‚úÖ SCRAPERAPI_KEY –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫: {e}")
        return
    
    all_listings = []
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –Ω—É–∂–Ω—ã–π —Ç–µ—Å—Ç
    if scraper_name == "idealista":
        count = await test_idealista()
    elif scraper_name == "immobiliare":
        count = await test_immobiliare()
    elif scraper_name == "subito":
        count = await test_subito()
    elif scraper_name == "all":
        await test_all_scrapers()
    else:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∫—Ä–∞–ø–µ—Ä: {scraper_name}")
        print("   –î–æ—Å—Ç—É–ø–Ω—ã–µ: idealista, immobiliare, subito, all")
        return
    
    print("\n" + "="*50)
    print("‚úÖ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print("="*50)


if __name__ == "__main__":
    asyncio.run(main()) 